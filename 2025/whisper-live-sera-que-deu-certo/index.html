<!doctype html>
<html lang="pt-BR">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />

    <title>Tentei usar o whisper live via microfone - Ot√°vio Miranda</title>
    <meta
      name="description"
      content="Criei essa p√°gina para documentar minhas tentativas de usar o whisper live via microfone. J√° quero deixar claro que isso n√£o funcionou conforme eu esperava e cabem mais testes."
    />

    <link rel="stylesheet" href="../../css/markdown.css" />

    <link rel="icon" type="image/webp" href="../../imgs/favicon-1.webp" />

    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/github-markdown-css/5.5.1/github-markdown-light.min.css"
    />

    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css"
    />

    <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
  </head>
  <body>
    <nav class="breadcrumb-nav">
      <a href="/">&larr; ir para in√≠cio</a>
    </nav>

    <article class="markdown-body" id="content"></article>

    <script type="text/markdown" id="markdown-source">




# (üö´ FAILED) `whisper` live via microfone

Criei esse arquivo para ir documentando minhas tentativas de montar um
`whisper` live pegando o √°udio direto do meu microfone. J√° quero **deixar
claro que isso n√£o funcionou** conforme eu esperava e cabem mais testes.

Ao meu ver, precisar√≠amos trabalhar diretamente com o modelo do `whisper`
sem usar o c√≥digo oficial da OpenAI, porque a janela de 30 segundos pode
ter me atrapalhado. Pode at√© ser necess√°rio refinar o modelo para
trabalhos `live`. Isso √© apenas um pensamento, visto que n√£o pude ir al√©m
do que descrevo √† seguir porque preciso ver se esse conte√∫do ser√° atrativo
para meu p√∫blico.

---

## (üö´ FAILED) Captura do microfone via `ffmpeg`

Minha primeira tentativa ser√° capturar o √°udio do computador usando o
`ffmpeg`. Para constar, estou usando o ChatGPT e o Gemini para me ajudar
com as partes que eu tenho pouco conhecimento relacionado a √°udio. Al√©m
disso, estou usando o seguinte hardware (n√£o sei se ser√° relevante, mas
cabe informar):

- MacBook Pro 2021
- Chip Apple M1 Max
- Mem√≥ria 32 GB RAM
- Disco 1 TB
- Sistema Operacional Sequoia 15.5

---

### Listando os dispositivos de √°udio

**Observa√ß√£o:** s√≥ estou testando no macOS.

```sh
# macOS
ffmpeg -hide_banner -f avfoundation -list_devices true -i ""

# Linux
ffmpeg -hide_banner -f alsa -list_devices true -i ""
# ou (talvez mais confi√°vel)
arecord -l
# ou
arecord --list-devices

# Windows
ffmpeg -hide_banner -list_devices true -f dshow -i dummy
```

Para mim, a sa√≠da foi mais ou menos como mostro abaixo. S√≥ ajustei um
pouco a sa√≠da e removi alguns dispositivos irrelevantes, como fones,
smartphones, etc. Tamb√©m removi o trechinho inicial do ffmpeg.

```txt
‚ûú  Desktop
ffmpeg -hide_banner -f avfoundation -list_devices true -i "" 2>&1

2025-06-18 09:15:21.567 ffmpeg[37312:1839628] WARNING:
Add NSCameraUseContinuityCameraDeviceType to your Info.plist to use AVCaptureDeviceTypeContinuityCamera.

2025-06-18 09:15:21.708 ffmpeg[37312:1839628] WARNING:
AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras.

Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.

[AVFoundation indev @ 0x153706960] AVFoundation video devices:
[AVFoundation indev @ 0x153706960] [0] C√¢mera FaceTime HD
[AVFoundation indev @ 0x153706960] [5] Capture screen 0
[AVFoundation indev @ 0x153706960] [6] Capture screen 1
[AVFoundation indev @ 0x153706960] AVFoundation audio devices:
[AVFoundation indev @ 0x153706960] [0] MOTIV Mix Virtual
[AVFoundation indev @ 0x153706960] [1] JBL TUNE125BT FOREBA
[AVFoundation indev @ 0x153706960] [2] Microfone (MacBook Pro)

[in#0 @ 0x600001f5c600] Error opening input: Input/output error

Error opening input file .
Error opening input files: Input/output error
```

A parte importante √© sabermos duas coisas: `[ID] NOME DO DISPOSITIVO`. Por
exemplo, quero usar o **Microfone (MacBook Pro)**, isso significa que o
dispositivo de √°udio que quero usar √© o `2`.

---

### (üö´ FAILED) Testando a captura de √°udio em um `.wav`

S√≥ para garantir que tudo est√° funcionando perfeitamente, vamos fazer uma
captura de √°udio para um arquivo `.wav`. Ainda n√£o √© a vers√£o final do
nosso comando, mas s√≥ queremos ver se est√° tudo funcionando.

```sh
# Lembra do meu [2] Microfone (MacBook Pro) -> ID 2
# No comando abaixo ":2" √© o dispositivo que estou usando
# Sa√≠da: out.wav de 10 segundos com a captura do dispositivo escolhido
ffmpeg -f avfoundation -i ":2" -t 10 -ac 1 -ar 16000 out.wav
```

Ou seja, para capturar apenas o **\[2] Microfone (MacBook Pro)**, basta
passar `:2` no argumento `-i`.

Se fosse necess√°rio capturar **v√≠deo e √°udio simultaneamente**, o formato
seria `V:A`, onde `V` √© o ID do v√≠deo e `A` o do √°udio.

Por exemplo:

```sh
# Captura da C√¢mera FaceTime HD (ID 0) e Microfone (MacBook Pro) (ID 2)
# Sa√≠da: out.mp4 de 10 segundos com a captura de v√≠deo e √°udio de 0 e 2
ffmpeg -f avfoundation -framerate 30 -i "0:2" -t 10 out.mp4
```

### (üö´ FAILED) O que houve com o `ffmpeg`?

No meu caso o `ffmpeg` n√£o funcionou conforme eu gostaria. O que aconteceu
aqui foi que o √°udio ficou picotando de modo muito estranho. A partir
daqui, fiquei praticamente o dia inteiro tentando testar coisas como:
trocar o sample rate do √°udio, o codec, o microfone, a fonte (testei
v√°rias fontes, at√© mesmo o √°udio direto do computador). Nada!

Cheguei na conclus√£o que o `avfoundation` no meu macOS n√£o est√°
funcionando perfeitamente. Exemplos de testes que fiz:

```sh
ffmpeg \
    -thread_queue_size 512 \
    -loglevel debug \
    -f avfoundation \
    -i ":2" \
    -c:a copy \
    -t 10 \
    out.wav \
    -y
```

Esse `copy` do codec de √°udio foi um dos √∫ltimos testes que fiz (depois de
trocar v√°rias outras coisas) e isso revelou que o ffmpeg detecta meu √°udio
com essas caracter√≠sticas:

```
Stream #0:0, 1, 1/1000000: Audio: pcm_f32le, 48000 Hz, mono, flt, 1536 kb/s
```

Eu j√° havia testado pcm_f32le e 48000 Hz, antes, ent√£o sem novidades aqui.

Enfim, deixo isso documentado aqui, porque se isso ocorrer com voc√™, n√£o
vi nenhuma solu√ß√£o poss√≠vel. A solu√ß√£o para este problema de √°udio tremido
ou picotando foi **usar o `sox`**.

---

## (‚úÖ SUCCESS) `sox` para capturar o √°udio

Na primeira tentativa com `sox` j√° obtive sucesso com **zero esfor√ßo**.

```sh
sox \
    -b 32 \
    -e float \
    -r 16000 \
    -c 1 \
    -d \
    --buffer $((16000*4*10)) \
    out.wav \
    trim 0 10 \
    fade t 1 -0 1
```

Este comendo foi gerado pelo Gemini, apenas solicitei um comando que
gravasse um √°udio de 10s com fade-in e fade-out (apenas para testes).

Depois testei um monte de coisas. O comando que vou usar com o `whisper`
ser√° o que mostro abaixo (üö´ n√£o execute isso, a sa√≠da est√° indo para o
`stdout`, n√£o para um arquivo).

```sh
# üö´ n√£o execute isso ainda
sox -t \
    coreaudio "Microfone (MacBook Pro)" \
    -b 16 \
    -e signed-integer \
    -r 16000 -c 1 \
    -t raw\
    -
```

Eu n√£o entendo muito sobre o `sox`, mas usei o `ffmpeg` (descrito acima)
para listar o nome dos dispositivos. Ao inv√©s de trabalhar com o ID (como
no `ffmpeg`), usamos o nome aqui.

---

## (ü§î SUCCESS?) `whisper` live - ou quase!

Aqui √© onde eu penso que posso ter que refinar o modelo ou usar o modelo
"cru", sem o c√≥digo da OpenAI (do `whisper`).

Fiz um c√≥digo extremamente desleixado apenas para **ver se iria
funcionar**. Se tudo desse certo, meu plano era refinar bastante tudo o
que voc√™ ver√° a seguir. Considere um "MVP" `0.0.0alpha`.

```python
# pyright: basic
import asyncio

import numpy as np
import whisper
from scipy.io import wavfile

MODEL = whisper.load_model("turbo")


async def recorder(queue, counter, buffer, chunk_size, sr=16000, duration=5):
    print("üó£Ô∏è Recording started")

    proc = await asyncio.create_subprocess_exec(
        "sox",
        "-t",
        "coreaudio",
        "Microfone (MacBook Pro)",
        "-b",
        "16",
        "-e",
        "signed-integer",
        "-r",
        "16000",
        "-c",
        "1",
        "-t",
        "raw",
        "-",  # raw para facilitar o corte
        stdout=asyncio.subprocess.PIPE,
        stderr=asyncio.subprocess.DEVNULL,
    )

    while True:
        data = await proc.stdout.read(chunk_size)
        if not data:
            break
        buffer += data

        chunk_byte_size = sr * 2 * duration

        if len(buffer) >= chunk_byte_size:
            audio_chunk = buffer[:chunk_byte_size]
            buffer = buffer[chunk_byte_size:]

            audio_np = np.frombuffer(audio_chunk, np.int16).astype(np.float32) / 32768.0
            await queue.put(audio_np)

            wavfile.write(
                f"media/debug_{counter}.wav",
                16000,
                audio_np,
            )

            print(f"üó£Ô∏è Chunk {counter} ends \n")
            counter += 1


async def transcriber(queue):
    prefix = ""
    while True:
        print("üí¨ Whisper started")

        audio = await queue.get()
        audio = whisper.pad_or_trim(audio)

        mel = whisper.log_mel_spectrogram(audio, n_mels=MODEL.dims.n_mels).to(
            MODEL.device
        )

        options = whisper.DecodingOptions(
            language="en",
            temperature=0,
            beam_size=1,
            patience=0,
            without_timestamps=True,
            prompt=None,
            prefix=None,
        )
        result = whisper.decode(
            MODEL,
            mel,
            options,
        )

        prefix = result.text
        print("üí¨", result.text, "\n")
        queue.task_done()


async def main():
    from pathlib import Path
    from shutil import rmtree

    media = Path("media")
    rmtree(media)
    media.mkdir(exist_ok=True)

    queue = asyncio.Queue()
    counter = 0
    buffer = b""
    chunk_size = 4096

    await asyncio.gather(
        recorder(queue, counter, buffer, chunk_size, 16000, 30), transcriber(queue)
    )


if __name__ == "__main__":
    asyncio.run(main())
```

Nesse c√≥digo eu uso `asyncio` para n√£o parar para esperar o `whisper`
transcrever o √°udio. Al√©m disso, tamb√©m uso `scipy.io.wavfile` para salvar
o √°udio em `chunks` da mesma maneira que o `whisper` recebe para debug.
Depois eu fa√ßo a jun√ß√£o desses arquivos e ou√ßo como ficou.

A fun√ß√£o `recorder` inicia o `sox` com o meu microfone e um loop que
analisa a quantidade de bytes (`chunk_byte_size`) que gerariam
aproximadamente o tempo informado em `duration`.

```python
# Sample rate (16000) * Sample Width (2) * duration (5)
chunk_byte_size = sr * 2 * duration
```

Quando o tamanho do `buffer` bate com o tamanho do `chunk_byte_size`,
capturamos o `buffer` at√© aquele ponto. Esses s√£o os bytes brutos do
trecho de √°udio. Tamb√©m mudamos o `buffer` para come√ßar de onde parou e
pegar qualquer byte que vier no final para uso posterior.

```python
while True:
    # captura o que vem do sox
    data = await proc.stdout.read(chunk_size)
    if not data:
        break
    # Joga no buffer
    buffer += data

    # calcula quantos bytes daria a duration
    chunk_byte_size = sr * 2 * duration

    # chegou na duration
    if len(buffer) >= chunk_byte_size:
        # pegamos o trecho de √°udio
        audio_chunk = buffer[:chunk_byte_size]
        # mudamos o buffer pra pegar algo que venha depois
        buffer = buffer[chunk_byte_size:]
```

Agora √© s√≥ converter o buffer do trecho de √°udio para um formato que o
`whisper` vai entender e jogar isso na queue para o `whisper` pegar
depois:

```python
audio_np = np.frombuffer(audio_chunk, np.int16).astype(np.float32) / 32768.0
await queue.put(audio_np)
```

Como eu queria ver exatamente o que o `whisper` tamb√©m "estava vendo",
salvei o √°udio usando:

```python
wavfile.write(
    f"media/debug_{counter}.wav",
    16000,
    audio_np,
)
```

Tem um contador a√≠ tamb√©m s√≥ pra eu criar arquivos: `dubug_0.wav`,
`dubug_1.wav`, `dubug_2.wav`, etc.

Depois eu junto os √°udios com o ffmpeg:

```sh
ffmpeg -f concat -safe 0 -i input.txt -c copy out.wav
```

O arquivo `input.txt` fica assim:

```
file 'dubug_0.wav'
file 'dubug_1.wav'
file 'dubug_2.wav'
```

At√© aqui t√° tudo indo lindamente. Enquanto estava testando essas coisas,
para mim tudo iria fluir perfeitamente. Mas o resultado foi bem mais ou
menos.

---

### (ü§î SUCCESS?) os problemas!

Essa √© a parte que entra o `whisper`. Aqui eu testei um monte de coisas
para tentar ser o mais r√°pido poss√≠vel e tamb√©m ter precis√£o na
transcri√ß√£o.

D√° uma conferida nos coment√°rios que adicionei no c√≥digo. Depois vou
explicar outras coisas a seguir.

```python
async def transcriber(queue):
    prefix = "" # USEI ISSO DE DUAS FORMAS DIFERENTES, SEM SUCESSO
    while True:
        print("üí¨ Whisper started")

        # Vamos pegar o √°udio bruto da queue
        audio = await queue.get()

        # Isso √© um saco, porque independente da dura√ß√£o do √°udio,
        # isso sempre vai gerar aproximadamente 30s.
        # Se o √°udio for menor do que 30s, ele preenche o resto com zeros at√©
        # chegar em 30s. Isso gera sil√™ncio no final do √°udio.
        # Se o √°udio for maior do que 30s, ele faz o trim (corta) para 30s.
        # IMPORTANTE: voc√™ n√£o conseguir√° enviar um √°udio de dura√ß√£o diferente
        # de 30s usando o c√≥digo da OpenAI porque o whisper foi treinado com 30s.
        audio = whisper.pad_or_trim(audio)

        # J√° expliquei isso em outro v√≠deo: o whisper n√£o ouve o √°udio, ele
        # ve uma representa√ß√£o gr√°fica do √°udio. D√° uma olhada sobre:
        # Espectrograma Mel logar√≠tmico (log-Mel spectrogram).
        # √â facinho de entender ü´†üò≥üòµ‚Äçüí´
        mel = whisper.log_mel_spectrogram(audio, n_mels=MODEL.dims.n_mels).to(
            MODEL.device
        )

        options = whisper.DecodingOptions(
            # Esse foi meu √∫ltimo teste: en (english)
            # 90% dos meus testes foram feitos com pt (portuguese) usando
            # v√≠deos do YouTube com pessoas falando claramente, bom √°udio,
            # sem g√≠rias ou termos dif√≠ceis.
            language="en",

            # Para deixar o modelo mais r√°pido, tentei usar o modo "greedy"
            # tamb√©m j√° expliquei isso no outro v√≠deo
            temperature=0,
            beam_size=1,
            patience=0, # isso n√£o faz nada, s√≥ desespero mesmo

            # tentei desativar os timestamps para deixar o modelo livre apenas
            # para transcrever o que ouvia
            without_timestamps=True,

            # Aquele `prefix` l√° em cima, eu usei nessas duas op√ß√µes. Isso
            # faz o model entrar em loop v√°rias vezes. Por isso desistir de usar.
            prompt=None,
            prefix=None,
        )

        # Abaixo √© s√≥ resultado
        result = whisper.decode(
            MODEL,
            mel,
            options,
        )

        prefix = result.text
        print("üí¨", result.text, "\n")
        queue.task_done()
```

Eu tentei detalhar bastante nos coment√°rios do c√≥digo acima. Mas vamos
olhar outras coisas relevantes.

N√£o consegui usar nenhum modelo acima do `medium`. Isso porque o tempo de
transcri√ß√£o do `whisper` ficava maior do que a janela de dura√ß√£o
escolhida. Por exemplo, com modelo `turbo` e uma janela de 2 segundos, o
whisper levava uns 10 segundos ou mais para transcrever. Ou seja, n√£o dava
tempo e nem tinha como ser "tempo real". Pode ser meu hardware tamb√©m.

Os modelos que funcionaram melhor para "tempo real" foram: `tiny` e `base`
(ambos imprecisos). Se fosse escolher, ficaria com o `base`. O `tiny`
entra em loop com uma facilidade extrema.

Sobre os tempos, testei com uma varia√ß√£o enorme de tempos:

- ‚ö†Ô∏è 1s, 2s, 3s, 4s: sem chance! O modelo quase n√£o acerta e em pouco
  tempo entra em loop
- ‚ö†Ô∏è 5s, 6s, 7s, 8s, 9s: at√© funciona, mas o modelo continua entrando em
  loop em alguns momentos
- ‚úÖ 10s at√© 30s: foi a melhor janela de tempo que encontrei.

Percebeu que eu queria algo em tempo real e as coisas s√≥ funcionaram bem a
partir dos 10s? Por isso que eu disse l√° em cima que talvez seja
necess√°rio refinar o modelo e usar c√≥digo pr√≥prio. Todos os meus testes
foram feitos em cima do c√≥digo do `whisper`.

---

## Conclus√£o geral

Dessa maneira que detalhei acima, ao meu ver, n√£o tem como fazer o
`whisper` funcionar de maneira precisa em tempo real. Pelo menos foi o que
conclu√≠ disso tudo.

Foi uma excelente experi√™ncia. Aprendi muitas coisas que n√£o sabia,
principalmente sobre √°udio, e tamb√©m me diverti bastante brincando com
esse modelo. Deixo documentado tudo que fiz, talvez te ajude a ir mais
fundo no modelo e c√≥digo do `whisper` para chegar a um objetivo melhor que
o meu. Outra op√ß√£o seria usar whisper.cpp (implementa√ß√£o C++ do Whisper)
ou faster-whisper (uma vers√£o otimizada em Python baseada no CTranslate2),
o que eu n√£o fiz.

Valeu!!!












    </script>

    <script>
      const markdownContent =
        document.getElementById('markdown-source').textContent;
      const contentDiv = document.getElementById('content');
      contentDiv.innerHTML = marked.parse(markdownContent);
    </script>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>
      // Ativa o realce de sintaxe em todos os blocos de c√≥digo
      hljs.highlightAll();
    </script>
  </body>
</html>
