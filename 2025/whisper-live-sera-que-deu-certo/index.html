<!doctype html>
<html lang="pt-BR">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />

    <title>Tentei usar o whisper live via microfone - Ot√°vio Miranda</title>
    <meta
      name="description"
      content="Criei essa p√°gina para documentar minhas tentativas de usar o whisper live via microfone. J√° quero deixar claro que isso n√£o funcionou conforme eu esperava e cabem mais testes."
    />

    <link rel="stylesheet" href="../../css/markdown.css" />

    <link rel="icon" type="image/webp" href="../../imgs/favicon-1.webp" />

    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/github-markdown-css/5.5.1/github-markdown-light.min.css"
    />

    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css"
    />

    <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
  </head>
  <body>
      <nav class="breadcrumb-nav">
        <a href="/">&larr; ir para in√≠cio</a>


        <form id="main-search" action="https://www.google.com/search" method="get" target="_blank">
          <input type="hidden" name="q" value="site:otaviomiranda.com.br ">
          <input type="search" name="q" placeholder="Buscar no site via Google">
          <button type="submit">Buscar</button>
        </form>

      </nav>

    <article class="markdown-body" id="content"></article>

    <script type="text/markdown" id="markdown-source">




# (üö´ FAILED) `whisper` em tempo real via microfone

Criei esse arquivo pra ir anotando minhas tentativas de rodar o `whisper` em
tempo real, puxando o √°udio direto do microfone. J√° vou **deixar claro que isso
n√£o funcionou** do jeito que eu esperava, e ainda tem coisa pra testar.

Na minha cabe√ßa, talvez o certo fosse trabalhar direto com o modelo do
`whisper`, sem usar o c√≥digo oficial da OpenAI. Aquela janela fixa de 30
segundos pode ter atrapalhado real. Pode at√© ser que precise refinar o modelo pra
rodar liso num esquema _live_.

Mas enfim... isso √© s√≥ uma ideia. N√£o fui muito al√©m do que t√° descrito aqui
porque quero ver primeiro se esse tipo de conte√∫do realmente chama aten√ß√£o do
pessoal pra eu focar mais tempo pra gerar conte√∫do (v√≠deos, posts, aulas, etc).

---

## (üö´ FAILED) Captura do microfone via `ffmpeg`

Minha primeira tentativa foi capturar o √°udio do computador usando o `ffmpeg`.

Pra deixar claro: t√¥ usando o ChatGPT e o Gemini pra me ajudar nas partes que eu
manjo menos, principalmente as coisas mais t√©cnicas de √°udio. Al√©m disso, t√¥
rodando tudo com esse setup aqui (n√£o sei se faz diferen√ßa, mas vai que...):

- MacBook Pro 2021
- Chip Apple M1 Max
- 32 GB de RAM
- SSD de 1 TB
- macOS Sequoia 15.5

---

### Listando os dispositivos de √°udio

**Obs:** t√¥ testando s√≥ no macOS por enquanto.

```sh
# macOS
ffmpeg -hide_banner -f avfoundation -list_devices true -i ""

# Linux
ffmpeg -hide_banner -f alsa -list_devices true -i ""
# ou (√†s vezes mais confi√°vel)
arecord -l
# ou
arecord --list-devices

# Windows
ffmpeg -hide_banner -list_devices true -f dshow -i dummy
```

No meu caso, a sa√≠da foi mais ou menos como a que mostro logo abaixo. S√≥ dei uma
enxugada, tirando os dispositivos que n√£o interessavam (tipo fone, celular,
etc). Tamb√©m cortei aquele trecho inicial que o ffmpeg cospe sempre.

```txt
‚ûú  Desktop
ffmpeg -hide_banner -f avfoundation -list_devices true -i "" 2>&1

2025-06-18 09:15:21.567 ffmpeg[37312:1839628] WARNING:
Add NSCameraUseContinuityCameraDeviceType to your Info.plist to use AVCaptureDeviceTypeContinuityCamera.

2025-06-18 09:15:21.708 ffmpeg[37312:1839628] WARNING:
AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras.

Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.

[AVFoundation indev @ 0x153706960] AVFoundation video devices:
[AVFoundation indev @ 0x153706960] [0] C√¢mera FaceTime HD
[AVFoundation indev @ 0x153706960] [5] Capture screen 0
[AVFoundation indev @ 0x153706960] [6] Capture screen 1
[AVFoundation indev @ 0x153706960] AVFoundation audio devices:
[AVFoundation indev @ 0x153706960] [0] MOTIV Mix Virtual
[AVFoundation indev @ 0x153706960] [1] JBL TUNE125BT FOREBA
[AVFoundation indev @ 0x153706960] [2] Microfone (MacBook Pro)

[in#0 @ 0x600001f5c600] Error opening input: Input/output error

Error opening input file .
Error opening input files: Input/output error
```

A parte importante dessa sa√≠da √© saber duas coisas: o `[ID]` e o nome do
dispositivo.

Por exemplo, se eu quiser usar o **Microfone (MacBook Pro)**, isso quer dizer
que o ID do dispositivo de √°udio que eu vou usar √© o `2`.

---

### (üö´ FAILED) Testando a captura de √°udio em um `.wav`

Antes de tudo, bora garantir que t√° tudo funcionando bonitinho. A ideia aqui √©
s√≥ gravar um `.wav` com o √°udio capturado direto do microfone. Ainda n√£o √© o
comando final, √© s√≥ pra testar mesmo.

```sh
# Lembra do meu [2] Microfone (MacBook Pro) -> ID 2
# No comando abaixo, o ":2" representa o dispositivo que eu t√¥ usando
# Resultado: grava um arquivo out.wav com 10 segundos de √°udio
ffmpeg -f avfoundation -i ":2" -t 10 -ac 1 -ar 16000 out.wav
```

Ou seja, pra capturar s√≥ o **\[2] Microfone (MacBook Pro)**, √© s√≥ passar `:2` no
argumento `-i`.

Se tu quiser capturar **v√≠deo e √°udio ao mesmo tempo**, o formato vira `V:A`,
onde `V` √© o ID do v√≠deo e `A` o do √°udio.

Tipo assim:

```sh
# Captura da C√¢mera FaceTime HD (ID 0) + Microfone (MacBook Pro) (ID 2)
# Grava um out.mp4 com 10 segundos de v√≠deo e √°udio dos dispositivos 0 e 2
ffmpeg -f avfoundation -framerate 30 -i "0:2" -t 10 out.mp4
```

---

### (üö´ FAILED) O que rolou com o `ffmpeg`?

No meu caso, o `ffmpeg` n√£o funcionou do jeito que eu queria. O √°udio ficou todo
picotado, com umas falhas bem esquisitas. Da√≠ pra frente, passei praticamente o
dia inteiro testando v√°rias coisas: mudei o _sample rate_, tentei outros
_codecs_, troquei de microfone, testei outras fontes (at√© tentei capturar o som
direto do sistema). Nada deu certo.

Minha conclus√£o: o `avfoundation` no meu macOS n√£o t√° se comportando direito.

Testei comandos como esse aqui:

```sh
ffmpeg \
    -thread_queue_size 512 \
    -loglevel debug \
    -f avfoundation \
    -i ":2" \
    -c:a copy \
    -t 10 \
    out.wav \
    -y
```

Esse `-c:a copy` (pra manter o codec de √°udio original) foi uma das √∫ltimas
tentativas, depois de trocar um monte de outras coisas. E com isso o ffmpeg
revelou que o √°udio tava vindo com essas specs:

```
Stream #0:0, 1, 1/1000000: Audio: pcm_f32le, 48000 Hz, mono, flt, 1536 kb/s
```

Eu j√° tinha testado `pcm_f32le` com `48000 Hz` antes, ent√£o... nenhuma surpresa.

Enfim, deixo isso aqui documentado porque, se acontecer contigo tamb√©m, j√° te
poupo umas horas de frustra√ß√£o. A √∫nica coisa que resolveu o problema de √°udio
tremido/picotado foi **usar o `sox`**.

---

## (‚úÖ SUCCESS) `sox` pra capturar o √°udio

Na primeira tentativa com `sox`, j√° rolou de boa, **zero esfor√ßo**.

```sh
sox \
    -b 32 \
    -e float \
    -r 16000 \
    -c 1 \
    -d \
    --buffer $((16000*4*10)) \
    out.wav \
    trim 0 10 \
    fade t 1 -0 1
```

Esse comando foi gerado pelo Gemini ou ChatGPT (n√£o lembro). S√≥ pedi um exemplo
que gravasse 10 segundos de √°udio com fade-in e fade-out, s√≥ pra teste mesmo.

Depois fui testando outras paradas, mas o comando que pretendo usar com o
`whisper` √© esse aqui:

```sh
# üö´ n√£o executa ainda
sox -t \
    coreaudio "Microfone (MacBook Pro)" \
    -b 16 \
    -e signed-integer \
    -r 16000 -c 1 \
    -t raw \
    -
```

Repara que a sa√≠da t√° indo pro `stdout` (por isso tem esse `-` no fim), n√£o pra
um arquivo. Ou seja, isso aqui √© s√≥ a base pra integrar com outro processo, tipo
jogar direto no `whisper`.

Eu n√£o entendo muito do `sox` ainda (primeiro uso), mas usei o `ffmpeg` (l√° em cima) pra listar
os dispositivos. E aqui √© diferente: em vez de usar o ID como no `ffmpeg`, tu
passa o nome do dispositivo, tipo, meu caso: `"Microfone (MacBook Pro)"`.

---

## (ü§î SUCCESS?) `whisper` live, ou quase

Aqui √© onde come√ßo a achar que talvez eu precise refinar o modelo ou at√© usar
ele "cru", sem passar pelo c√≥digo oficial da OpenAI (`whisper`).

Fiz um c√≥digo bem porquinho, s√≥ pra **ver se ia funcionar**. A ideia era: se
desse certo, a√≠ sim eu refinava tudo com calma.

Considera isso aqui como um _MVP_ na vers√£o `0.0.0alpha` mesmo.

```python
# pyright: basic
import asyncio

import numpy as np
import whisper
from scipy.io import wavfile

MODEL = whisper.load_model("turbo")


async def recorder(queue, counter, buffer, chunk_size, sr=16000, duration=5):
    print("üó£Ô∏è Recording started")

    proc = await asyncio.create_subprocess_exec(
        "sox",
        "-t",
        "coreaudio",
        "Microfone (MacBook Pro)",
        "-b",
        "16",
        "-e",
        "signed-integer",
        "-r",
        "16000",
        "-c",
        "1",
        "-t",
        "raw",
        "-",  # raw para facilitar o corte
        stdout=asyncio.subprocess.PIPE,
        stderr=asyncio.subprocess.DEVNULL,
    )

    while True:
        data = await proc.stdout.read(chunk_size)
        if not data:
            break
        buffer += data

        chunk_byte_size = sr * 2 * duration

        if len(buffer) >= chunk_byte_size:
            audio_chunk = buffer[:chunk_byte_size]
            buffer = buffer[chunk_byte_size:]

            audio_np = np.frombuffer(audio_chunk, np.int16).astype(np.float32) / 32768.0
            await queue.put(audio_np)

            wavfile.write(
                f"media/debug_{counter}.wav",
                16000,
                audio_np,
            )

            print(f"üó£Ô∏è Chunk {counter} ends \n")
            counter += 1


async def transcriber(queue):
    prefix = ""
    while True:
        print("üí¨ Whisper started")

        audio = await queue.get()
        audio = whisper.pad_or_trim(audio)

        mel = whisper.log_mel_spectrogram(audio, n_mels=MODEL.dims.n_mels).to(
            MODEL.device
        )

        options = whisper.DecodingOptions(
            language="en",
            temperature=0,
            beam_size=1,
            patience=0,
            without_timestamps=True,
            prompt=None,
            prefix=None,
        )
        result = whisper.decode(
            MODEL,
            mel,
            options,
        )

        prefix = result.text
        print("üí¨", result.text, "\n")
        queue.task_done()


async def main():
    from pathlib import Path
    from shutil import rmtree

    media = Path("media")
    rmtree(media)
    media.mkdir(exist_ok=True)

    queue = asyncio.Queue()
    counter = 0
    buffer = b""
    chunk_size = 4096

    await asyncio.gather(
        recorder(queue, counter, buffer, chunk_size, 16000, 30), transcriber(queue)
    )


if __name__ == "__main__":
    asyncio.run(main())
```

Nesse c√≥digo eu uso `asyncio` justamente pra n√£o travar tudo enquanto o
`whisper` t√° transcrevendo. A ideia √© deixar o processo rodando em paralelo:
enquanto uma parte grava, a outra j√° vai transcrevendo em tempo real.

Tamb√©m uso o `scipy.io.wavfile` pra salvar os √°udios em _chunks_, do mesmo
jeitinho que o `whisper` recebe. Isso ajuda no debug: depois eu junto esses
arquivos e escuto como ficou o √°udio real que foi processado.

A fun√ß√£o `recorder` √© quem inicia o `sox` com o microfone que escolhi e roda um
loop que fica monitorando a quantidade de bytes acumulados. Essa contagem √©
baseada no tamanho que cada trecho de √°udio teria, considerando a dura√ß√£o que
defini.

```python
# Sample rate (16000) * Sample Width (2 bytes) * dura√ß√£o (ex: 5s)
chunk_byte_size = sr * 2 * duration
```

Quando o tamanho do `buffer` atinge esse `chunk_byte_size`, a gente corta
exatamente at√© ali. Esses s√£o os bytes crus do trecho de √°udio que vai pra
transcri√ß√£o. Em seguida, o `buffer` √© atualizado pra remover esse peda√ßo
processado e guardar qualquer sobra, que vai ser usada no pr√≥ximo ciclo.

Esse loop aqui embaixo √© o cora√ß√£o da grava√ß√£o. Ele fica rodando enquanto o
`sox` t√° mandando dados pela `stdout`.

```python
while True:
    # captura o que vem do sox
    data = await proc.stdout.read(chunk_size)
    if not data:
        break
    # joga no buffer
    buffer += data

    # calcula quantos bytes correspondem √† duration configurada
    chunk_byte_size = sr * 2 * duration

    # se o buffer j√° tiver o tamanho certo pra um trecho de √°udio completo
    if len(buffer) >= chunk_byte_size:
        # pegamos s√≥ o peda√ßo que interessa
        audio_chunk = buffer[:chunk_byte_size]
        # e atualizamos o buffer, removendo o que j√° usamos
        buffer = buffer[chunk_byte_size:]
```

A√≠, com esse `audio_chunk` em m√£os, √© s√≥ converter pra um formato que o
`whisper` entenda e jogar isso na `queue`, que o `transcriber` vai processar
depois:

```python
audio_np = np.frombuffer(audio_chunk, np.int16).astype(np.float32) / 32768.0
await queue.put(audio_np)
```

Esse `.frombuffer(...).astype(...) / 32768.0` basicamente transforma os bytes
crus em um array de `float32` com valores normalizados entre -1 e 1, que √© o
formato que o `whisper` espera.

Como eu queria ver exatamente o que o `whisper` tamb√©m "tava enxergando", salvei
cada trecho de √°udio assim:

```python
wavfile.write(
    f"media/debug_{counter}.wav",
    16000,
    audio_np,
)
```

Usei esse `counter` s√≥ pra gerar arquivos separados, tipo: `debug_0.wav`,
`debug_1.wav`, `debug_2.wav`, e por a√≠ vai.

Depois juntei tudo com o `ffmpeg`:

```sh
ffmpeg -f concat -safe 0 -i input.txt -c copy out.wav
```

E o arquivo `input.txt` fica assim:

```
file 'debug_0.wav'
file 'debug_1.wav'
file 'debug_2.wav'
```

At√© esse ponto, tava tudo fluindo lindamente. Eu j√° tava achando que ia dar bom.
Mas... o resultado final foi bem mais ou menos.

---

### (ü§î SUCCESS?) os problemas!

√â aqui que entra o `whisper` de verdade. Testei um monte de coisas pra tentar
deixar tudo o mais r√°pido poss√≠vel, e, claro, sem perder precis√£o na
transcri√ß√£o.

D√° uma olhada nos coment√°rios que deixei no c√≥digo. Logo mais abaixo eu explico
outras coisas que descobri no caminho.

Essa √© a fun√ß√£o `transcriber`, que fica rodando enquanto tem coisa na fila
(`queue`). Comentei bastante no c√≥digo, mas aqui vai uma vis√£o geral com uns
complementos:

```python
async def transcriber(queue):
    prefix = ""  # USEI ISSO DE DUAS FORMAS DIFERENTES, SEM SUCESSO
    while True:
        print("üí¨ Whisper started")

        # Pega o √°udio cru da fila
        audio = await queue.get()

        # Isso aqui √© um p√© no saco: n√£o importa o tamanho real do √°udio,
        # o whisper sempre espera cerca de 30s.
        #
        # Se for menor, ele preenche o resto com zeros (sil√™ncio).
        # Se for maior, ele corta o que passa dos 30s.
        #
        # IMPORTANTE: n√£o tem como passar √°udios de dura√ß√µes diferentes
        # usando o c√≥digo da OpenAI, porque o modelo foi treinado assim.
        audio = whisper.pad_or_trim(audio)

        # O whisper n√£o "ouve" o som, ele transforma em imagem.
        # √â um gr√°fico do som chamado espectrograma (Mel logar√≠tmico).
        # J√° expliquei isso num v√≠deo, √© bem de boa de entender.
        mel = whisper.log_mel_spectrogram(audio, n_mels=MODEL.dims.n_mels).to(
            MODEL.device
        )

        options = whisper.DecodingOptions(
            # √öltimo teste: ingl√™s (en)
            # Mas 90% dos testes foram em portugu√™s (pt), com √°udios limpos,
            # de v√≠deos do YouTube com gente falando claro, sem g√≠ria nem ru√≠do.
            language="en",

            # Tentei deixar mais r√°pido usando o modo "greedy"
            # Tamb√©m expliquei isso no outro v√≠deo
            temperature=0,
            beam_size=1,
            patience=0,  # isso aqui nem faz nada, foi mais no desespero

            # Desativei os timestamps pra ver se o modelo ficava mais leve,
            # focando s√≥ na transcri√ß√£o mesmo
            without_timestamps=True,

            # Aquele prefixo l√° de cima... tentei usar nessas duas op√ß√µes aqui
            # (prompt e prefix). O modelo entrou em loop e fiquei bolado.
            # Desisti de usar.
            prompt=None,
            prefix=None,
        )

        # E aqui √© onde rola a transcri√ß√£o de fato
        result = whisper.decode(MODEL, mel, options)

        prefix = result.text
        print("üí¨", result.text, "\n")
        queue.task_done()
```

Tentei detalhar bastante nos coment√°rios do c√≥digo acima, mas ainda tem umas
coisas importantes que vale comentar.

Primeiro: **n√£o consegui usar nenhum modelo acima do `medium`**. O motivo √©
simples, o tempo de transcri√ß√£o ficava maior do que a janela de tempo que eu
escolhi.

Tipo assim: com o modelo `turbo` e uma janela de 2 segundos, o `whisper` levava
uns 10 segundos (ou mais!) pra cuspir a transcri√ß√£o. Ou seja, **n√£o dava
tempo**. N√£o tinha como rodar em tempo real desse jeito. Pode ser limita√ß√£o do
meu hardware tamb√©m? Pode. Mas o fato √© que n√£o rolou.

Os modelos que **se sa√≠ram melhor** nesse esquema foram o `tiny` e o `base`.
Ambos ainda s√£o imprecisos, mas se eu tivesse que escolher um, ficaria com o
`base`. O `tiny`, cara... entra em loop com uma facilidade absurda, qualquer
coisa repete, embanana, e vira um samba do sussurrador doido.

Sobre os tempos, testei com v√°rias janelas diferentes:

- ‚ö†Ô∏è **1s, 2s, 3s, 4s:** sem chance. O modelo n√£o entende nada, e em poucos
  ciclos j√° come√ßa a repetir as coisas.
- ‚ö†Ô∏è **5s a 9s:** at√© vai, mas ainda rola loop e perda de qualidade em alguns
  momentos.
- ‚úÖ **10s a 30s:** aqui a coisa come√ßou a funcionar de verdade. Foi a faixa que
  deu o melhor resultado.

Percebeu o paradoxo? Eu queria uma parada em _tempo real_, mas s√≥ come√ßou a
ficar us√°vel a partir dos 10 segundos de √°udio por vez. Foi a√≠ que me bateu a
real: talvez s√≥ role se eu refinar o modelo e usar um c√≥digo customizado, sem
depender da implementa√ß√£o padr√£o do `whisper`.

Todos esses testes foram feitos usando exatamente o c√≥digo oficial da OpenAI.
Nada modificado no core do modelo.

---

## Conclus√£o geral

Desse jeito que mostrei at√© aqui, pra mim **n√£o rola usar o `whisper` com
precis√£o em tempo real**. Pelo menos foi essa a conclus√£o que cheguei depois de
tudo isso.

Mas √≥: foi uma experi√™ncia massa. Aprendi muita coisa que eu nem fazia ideia,
principalmente sobre √°udio, e me diverti bastante fu√ßando esse modelo.

T√¥ deixando tudo documentado aqui porque pode ser que te ajude a ir mais fundo
no modelo, ou at√© mexer no c√≥digo do `whisper` pra tentar um resultado melhor
que o meu.

Outra op√ß√£o que eu _n√£o testei_, mas pode valer muito a pena, seria usar o
[`whisper.cpp`](https://github.com/ggerganov/whisper.cpp) (vers√£o C++) ou o
[`faster-whisper`](https://github.com/guillaumekln/faster-whisper), que √© uma
implementa√ß√£o otimizada em Python com base no CTranslate2.

Valeu demais se tu leu at√© aqui! Tomara que eu tenha te ajudado ou feito voc√™
perder menos tempo se fosse testar tudo isso que testei.

**Obs:** escrevi esse texto com `markdown`, se quiser baixar:

- [Baixar esse texto em `markdown`](TEXT.md)

Bye üëã!!!

---












    </script>

    <script>
      const markdownContent =
        document.getElementById('markdown-source').textContent;
      const contentDiv = document.getElementById('content');
      contentDiv.innerHTML = marked.parse(markdownContent);
    </script>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>
      // Ativa o realce de sintaxe em todos os blocos de c√≥digo
      hljs.highlightAll();
    </script>
  </body>
</html>
