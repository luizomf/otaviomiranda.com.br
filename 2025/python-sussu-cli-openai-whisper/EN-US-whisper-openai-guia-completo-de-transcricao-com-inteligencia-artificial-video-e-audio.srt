1
00:00:00,000 --> 00:00:04,520
Hey everyone! Turn on the subtitles for this
video, so you can see what I'm saying in real time.

2
00:00:04,520 --> 00:00:08,200
It's obvious, right?  It's subtitles, but these
subtitles are more

3
00:00:08,200 --> 00:00:12,440
special because they were generated by this
guy here, which is OpenAI's Whisper, which is

4
00:00:12,440 --> 00:00:17,020
an artificial intelligence model from OpenAI
that will basically do

5
00:00:17,020 --> 00:00:20,240
speech recognition and, through this speech
recognition, it can do

6
00:00:20,240 --> 00:00:24,000
several things; for example, transcribe this
video, which is the subtitle

7
00:00:24,000 --> 00:00:27,400
you're seeing there, or even do the translation.
I'll see later if I do the

8
00:00:27,400 --> 00:00:32,260
translation of this transcription, so that I
have two transcriptions: I have one

9
00:00:32,260 --> 00:00:35,560
subtitle in Portuguese and a subtitle of what
I'm saying, which is more common for us

10
00:00:35,560 --> 00:00:40,680
See; subtitles in a different language, it'll
be a subtitle for English, for example.

11
00:00:40,900 --> 00:00:45,060
We'll talk about that later. So, what we're
going to talk about in this video, just so

12
00:00:45,060 --> 00:00:47,920
you understand, in case you want to, like:
"I want to watch this video or I want to watch the

13
00:00:47,920 --> 00:00:50,840
other one", because I'm going to split this
into two parts. First, Whisper, it

14
00:00:50,840 --> 00:00:54,700
works as a command-line tool. So, I'm going
to focus on the command-line tool, because if we

15
00:00:54,700 --> 00:00:57,820
look here, just by looking at the size
of this thing I wrote here, you'll see the amount

16
00:00:57,820 --> 00:01:02,020
of information we have here just with the
command-line tool. And then, later I'll show you

17
00:01:02,020 --> 00:01:05,660
in the next video how to do the code
implementation using the

18
00:01:05,660 --> 00:01:09,220
Whisper.

19
00:01:09,220 --> 00:01:12,440
Whisper. And you can embed this
into your code and put it, I don't know, in your

20
00:01:12,440 --> 00:01:17,200
Django, wherever you want to work
with Whisper.  You just use the part

21
00:01:17,200 --> 00:01:21,120
from the other video. So, like I said,
we're basically going to focus on the command

22
00:01:21,120 --> 00:01:24,680
line of Whisper here. Another
interesting fact for you: it's because I've

23
00:01:24,680 --> 00:01:30,880
been using Whisper for about six months now,
and I've noticed its strengths and weaknesses,

24
00:01:30,880 --> 00:01:34,800
which I'll tell you about in this video, but

25
00:01:34,800 --> 00:01:38,400
when I went to write this, I got incredibly
curious to know if OpenAI

26
00:01:38,400 --> 00:01:42,560
used this internally. So, the only thing I
thought was to ask ChatGPT

27
00:01:42,560 --> 00:01:46,760
if it could tell me. I asked ChatGPT this: "Does

28
00:01:46,760 --> 00:01:51,200
"Does OpenAI use Whisper internally in any
feature that people consume?". Out of

29
00:01:51,200 --> 00:01:55,160
curiosity, they answered me — I don't know
if it's true, but they said yes — OpenAI

30
00:01:55,160 --> 00:01:59,600
uses Whisper as a basis for audio
transcription and understanding features in

31
00:01:59,600 --> 00:02:04,420
products like ChatGPT with voice input. It's
also used in APIs that

32
00:02:04,420 --> 00:02:10,220
involve speech recognition, like OpenAI's
Speech-to-Text. So, you see

33
00:02:10,220 --> 00:02:15,320
that it's a free, open-source artificial
intelligence model, and it's

34
00:02:15,320 --> 00:02:19,720
used by a huge and very famous company.
That's why I think

35
00:02:19,720 --> 00:02:23,200
it's so good. But what we're going to talk
about in this video is

36
00:02:23,200 --> 00:02:27,920
basically the command-line part of Whisper,
which we'll start now. I'll

37
00:02:27,920 --> 00:02:31,000
Let's use the clip I've been talking about so far,
which for me is about

38
00:02:31,000 --> 00:02:35,220
3 minutes long.  This clip I'm talking about, I'll
cut and use in our

39
00:02:35,220 --> 00:02:39,300
examples.  Alright? So, if you remember what I said
from the beginning of the

40
00:02:39,300 --> 00:02:42,760
video, you can see it in the subtitles, because
I'm not going to replay the

41
00:02:42,760 --> 00:02:45,860
video.  So you can go back to the beginning, see
what I said and see if it's

42
00:02:45,860 --> 00:02:50,100
matching what I'm saying here in the video
transcript. Alright? So let's

43
00:02:50,100 --> 00:02:54,500
get started with this. So, there you go, folks. I've
already done the introduction and I've

44
00:02:54,500 --> 00:02:58,800
put that introduction into another Python script
that automatically cuts the

45
00:02:58,800 --> 00:03:03,540
silence from the video for me. This cut removed
more than 30

46
00:03:03,540 --> 00:03:07,220
seconds of audio. Maybe I was silent for too long
at the beginning of the video

47
00:03:07,220 --> 00:03:10,480
or at the end of the video, something like that.
He already cut this audio, it went to

48
00:03:10,480 --> 00:03:14,320
about 2 minutes and 30 seconds.
So, that's what we're gonna use. I put

49
00:03:14,320 --> 00:03:19,540
the name one-auto.mp4, to say, this is the
first part of the video and it was also

50
00:03:19,540 --> 00:03:23,720
automatically cut for me. Besides that, we
need to understand how we're gonna

51
00:03:23,720 --> 00:03:28,300
run Whisper here. What did I do? I put this
code in a repository that I'll call

52
00:03:28,300 --> 00:03:32,940
`sussu`, `sussu` (rro), in this case. So, our
command there will be `sussu`. And there's

53
00:03:32,940 --> 00:03:37,240
also an official Whisper repository, in case
you want to take a look. I even left a

54
00:03:37,240 --> 00:03:41,040
thing open here too, which was this here,
so you can understand that video

55
00:03:41,040 --> 00:03:45,600
transcription can be used for several

56
00:03:45,600 --> 00:03:50,460
smart and interesting things, too. For example, I use the

57
00:03:50,460 --> 00:03:54,840
video transcriptions to pass this transcription into another API, for

58
00:03:54,840 --> 00:03:59,800
example, Gemini or ChatGPT, GPT-4, or any GPT you want

59
00:03:59,800 --> 00:04:04,100
to use. But I pass this transcription into an AI that generates

60
00:04:04,100 --> 00:04:09,140
summaries of sections for me. So, I send a chunk of the captions, a section of the

61
00:04:09,140 --> 00:04:13,560
captions, into any AI API and give it a

62
00:04:13,560 --> 00:04:17,240
prompt telling it what I want it to do. In this case, it's the video about

63
00:04:17,240 --> 00:04:20,920
argparse that we talked about in Python. It's an hour and a half long. So, I cut it

64
00:04:20,920 --> 00:04:25,340
into seven-minute chunks, pass it to Gemini, and then ask it,

65
00:04:25,420 --> 00:04:29,040
through a prompt, to summarize what I said in that section. And

66
00:04:29,040 --> 00:04:32,060
He goes through it, mentioning
the tools I used, and it's super

67
00:04:32,060 --> 00:04:35,700
helpful. Look, here he's saying
that the main goal of this section is

68
00:04:35,700 --> 00:04:40,000
to introduce a tutorial on how to use
Python's argparse to create command-line

69
00:04:40,000 --> 00:04:44,620
tools. The presenter proposes a practical
challenge: building the CLI interface

70
00:04:44,620 --> 00:04:48,620
for an existing project, allowing external
communication with the program. If you

71
00:04:48,620 --> 00:04:52,720
watch this video, you'll see that's
exactly what I'm talking about in that

72
00:04:52,720 --> 00:04:55,780
video. And then he shows which tools I

73
00:04:55,780 --> 00:04:59,140
used, details the tools, shows the steps
I followed up to this

74
00:04:59,140 --> 00:05:03,020
point, and then gives some tips down
here and so on. And

75
00:05:03,020 --> 00:05:06,640
It does this through snippets, using a for loop, I'm

76
00:05:06,640 --> 00:05:10,760
sending the subtitle pieces to it, and it generates this.  In the end,

77
00:05:10,920 --> 00:05:14,320
you can ask it to do whatever you want. For example, I've already used Gemini

78
00:05:14,320 --> 00:05:19,200
to translate to another language. For instance, Whisper does this, but I

79
00:05:19,200 --> 00:05:24,340
will explain why I used Gemini here. But I've already used it to

80
00:05:24,340 --> 00:05:29,560
translate these subtitles of mine to English or another language, Spanish, and

81
00:05:29,560 --> 00:05:33,720
then it generates several languages from that same transcription for you. I've also

82
00:05:33,720 --> 00:05:37,840
used Gemini to create a general summary of what I said in the video in the form of an

83
00:05:37,840 --> 00:05:42,700
article. It's really good too. And I've also used it to write, and this I

84
00:05:42,700 --> 00:05:46,920
I still use it today to write the video
description and chapters here,

85
00:05:47,000 --> 00:05:51,080
this part here. The final result, content
for YouTube. Chapters for the

86
00:05:51,080 --> 00:05:54,240
video, I asked it for that, and it generates
the chapters.  Okay, then I asked it to

87
00:05:54,240 --> 00:06:00,040
do some SEO optimization for me here, and
it did the video description there. I

88
00:06:00,040 --> 00:06:03,420
forgot to add the title here, but it also
does the title, adds keywords and

89
00:06:03,420 --> 00:06:06,320
so on. So I'm showing you this just so you
can see that

90
00:06:06,320 --> 00:06:11,260
transcribing a video is super powerful
because it gives you the text of what you

91
00:06:11,260 --> 00:06:16,020
said. And from that text you can do
whatever you want with that text there

92
00:06:16,020 --> 00:06:20,860
for your video. Generate a post, translate,
generate a post in another language, in short, whatever

93
00:06:20,860 --> 00:06:24,440
whatever you want. Whatever you can
imagine, you can do with the text

94
00:06:24,440 --> 00:06:28,580
of your video. Alright, that said, the
Whisper here, just so you understand

95
00:06:28,580 --> 00:06:32,500
better, it uses this format here. Why am
I going to show you this? Because

96
00:06:32,500 --> 00:06:35,380
this is super nerdy, but I'm going to
show you this just so you understand

97
00:06:35,380 --> 00:06:39,000
how it works. And I'm also going to give
you tips here, the strengths and

98
00:06:39,000 --> 00:06:41,980
weaknesses of it, I've been using it for
quite a while now, so

99
00:06:41,980 --> 00:06:45,720
I can tell you these things. But
initially what does it do? When you

100
00:06:45,720 --> 00:06:51,060
pass an audio, we talk about audio, but
since Whisper uses FFmpeg, which is

101
00:06:51,060 --> 00:06:55,420
a... We'll get to that, but Whisper uses
FFmpeg. This means that you

102
00:06:55,420 --> 00:07:00,140
You can directly pass a video to it, and
the Whisper code that OpenAI wrote

103
00:07:00,140 --> 00:07:06,940
already converts that video to audio
and that audio into a log-mel spectrogram.

104
00:07:07,240 --> 00:07:12,540
Tough word, but basically, it's this:
Whisper doesn't hear audio, it sees

105
00:07:12,540 --> 00:07:16,920
an image representing that audio, a
graph here, this spectrogram here, which

106
00:07:16,920 --> 00:07:22,500
uses the Mel logarithmic scale, it's a
super nerdy thing here, that for you to

107
00:07:22,500 --> 00:07:26,960
understand you'd have to be an audio
engineer, mathematician, and nerd all at

108
00:07:26,960 --> 00:07:30,880
the same time, so it's kind of complicated.
But Whisper doesn't hear audio, it sees the

109
00:07:30,880 --> 00:07:36,020
image of what you said, and that image is
separated into 30-second chunks. So

110
00:07:36,020 --> 00:07:40,680
it takes your original text, your original
audio, breaks those parts into parts of

111
00:07:40,680 --> 00:07:44,840
30 seconds, run these parts through Whisper,
and it'll do everything it

112
00:07:44,840 --> 00:07:49,800
needs to do there.  There's also the
complete workflow here if you want to see it, because

113
00:07:49,800 --> 00:07:54,240
Whisper doesn't just do audio transcription
and translation, it also does

114
00:07:54,240 --> 00:08:00,500
the following: it can do multilingual speech
recognition, that is, multi-

115
00:08:00,500 --> 00:08:05,340
lingual; it can do that, it can do speech
translation, but

116
00:08:05,340 --> 00:08:09,440
that speech translation is only to English.
So you speak something in

117
00:08:09,440 --> 00:08:14,100
any language, ask it to do translation
instead of transcription; this

118
00:08:14,100 --> 00:08:18,420
translation will go from your language to
English only. If you want to do it in another

119
00:08:18,420 --> 00:08:22,340
language, that's why I used Gemini; if you
want to do it in another language, you

120
00:08:22,340 --> 00:08:27,760
You can also use NLLB, which stands for "No
Languages Left Behind," if I'm not

121
00:08:27,760 --> 00:08:33,460
mistaken, which translates between languages
for 200 languages, in this case. I've already done this

122
00:08:33,460 --> 00:08:41,180
using this, which is the NLLB,
which is this. I already did this

123
00:08:41,180 --> 00:08:44,720
a while ago; if you want, I can bring a
tutorial. It uses Whisper and NLLB to

124
00:08:44,720 --> 00:08:48,740
do exactly that: it transcribes the audio,
in this case, and also translates it

125
00:08:48,740 --> 00:08:53,040
to any language you want. But you can also use, if

126
00:08:53,040 --> 00:08:56,400
you want something more robust, you'll
have to pay a little bit for it, but

127
00:08:56,400 --> 00:08:59,440
you can use Gemini, as was the case with
what I showed you earlier.

128
00:08:59,440 --> 00:09:04,620
Okay, but going back to Whisper, it does
the recognition, the translation, in this case,

129
00:09:04,960 --> 00:09:09,920
it can also identify what language
the audio is spoken in;

130
00:09:09,980 --> 00:09:13,160
this is also cool, if you want to
create a program just to do

131
00:09:13,160 --> 00:09:16,420
language recognition. You can do this,

132
00:09:16,720 --> 00:09:19,680
you can also do — I had thought about
this, but I haven't done it yet —,

133
00:09:19,960 --> 00:09:24,660
but you can also do it, if several
people are speaking in a video and

134
00:09:24,660 --> 00:09:28,700
every hour of the video they're speaking
in different languages, you

135
00:09:28,700 --> 00:09:32,460
could have Whisper do this recognition
and switch the language for it

136
00:09:32,460 --> 00:09:35,540
to start translating into another
language. It doesn't do this automatically; you'd have to

137
00:09:35,540 --> 00:09:40,000
write code for that. And it can also —
this is a part that I

138
00:09:40,000 --> 00:09:43,840
I'm focusing on this part here —, which is
this: it can do what's

139
00:09:43,840 --> 00:09:49,700
called Voice Activity Detection or VAD,
this voice activity detection,

140
00:09:49,820 --> 00:09:54,220
it can, for example, tell where a person
is speaking or where a person is not;

141
00:09:54,640 --> 00:09:57,880
so this would be very useful for me to use
something I use here

142
00:09:57,880 --> 00:10:01,820
that isn't very perfect. I already did this
in the video I recorded for the introduction, which

143
00:10:01,820 --> 00:10:06,060
is basically cutting out silences from the
video; if I'm quiet, I want it to cut

144
00:10:06,060 --> 00:10:09,560
that section. So, if I do this, you
probably didn't see this, because I

145
00:10:09,560 --> 00:10:13,500
was silent for about 5 seconds, but the cut
happened here in this

146
00:10:13,500 --> 00:10:18,620
video, and this was done automatically
here with the auto-editor.  Then I'll

147
00:10:18,620 --> 00:10:22,020
I'll show this too, if you want a tutorial
on it. But using

148
00:10:22,020 --> 00:10:25,420
artificial intelligence, it would be much
more accurate, because what I use

149
00:10:25,520 --> 00:10:31,060
basically uses the sound waves there, it's
not artificial intelligence, but it would

150
00:10:31,060 --> 00:10:34,620
be possible to do this with Whisper, if you
wanted. And one important thing about

151
00:10:34,620 --> 00:10:38,240
Whisper. You can read about it in its
documentation, but I've already written this

152
00:10:38,240 --> 00:10:44,160
here for you. Whisper uses FFmpeg under the
hood, so I left

153
00:10:44,160 --> 00:10:47,240
this text here; I'll leave this whole text
in the video description for you,

154
00:10:47,300 --> 00:10:50,380
so you can read exactly what I'm showing
you here, and then you'll have

155
00:10:50,380 --> 00:10:56,040
all those commands, but it uses FFmpeg, so
you should install it first

156
00:10:56,040 --> 00:11:01,260
so you can continue using Whisper. I've left here the

157
00:11:01,260 --> 00:11:05,940
commands for Debian, Arch Linux, MacOS,
Windows, in several ways

158
00:11:05,940 --> 00:11:10,500
different. You can use these commands here
to install FFmpeg. And

159
00:11:10,500 --> 00:11:15,380
I also created this repository here, the
`sussu` (whisper), that I created here,

160
00:11:15,700 --> 00:11:19,740
which is basically using `uv` to upload
this repository. So the only

161
00:11:19,740 --> 00:11:24,060
thing you need to do is `git clone`, clone
this repository, enter the folder

162
00:11:24,060 --> 00:11:29,040
of what was cloned and run `uv sync`. Of
course, you'll need `uv` to manage the

163
00:11:29,040 --> 00:11:33,160
packages. I've already left a tutorial here
that I've already recorded explaining how to use

164
00:11:33,160 --> 00:11:37,120
`uv`. So, that's it, after you've done
that, another important thing I

165
00:11:37,120 --> 00:11:42,700
I just saw, `uv` will do this for you,
download and install Python 3

166
00:11:42,700 --> 00:11:46,240
.11, this is super important due to
compatibility with Whisper, if you

167
00:11:46,240 --> 00:11:49,880
have a very high version of Python, it won't
install, and it will create your

168
00:11:49,880 --> 00:11:53,380
virtual environment, install the necessary
packages, build both Whisper

169
00:11:53,380 --> 00:11:58,740
and `sussu`. After you do this, `uv sync`,
you'll already have the project

170
00:11:58,740 --> 00:12:03,220
ready, that we can come here and run it for
the first time. So here I

171
00:12:03,220 --> 00:12:06,920
already have that project ready, I'll even
unpin this, and I'll

172
00:12:06,920 --> 00:12:12,020
use it here; I already used `uv sync` here,
I already did this in this project, and then

173
00:12:12,020 --> 00:12:15,680
it already synchronized everything there, and
then now we can run it for the first time. For the

174
00:12:15,680 --> 00:12:18,680
When running it for the first time, we
can run it, or in this case, I'll

175
00:12:18,680 --> 00:12:23,220
exit the terminal just so you can see;
in this case, my editor already activates the

176
00:12:23,220 --> 00:12:27,620
virtual environment automatically. See?
If yours doesn't do that, you can

177
00:12:27,620 --> 00:12:31,580
or manually activate the virtual environment.
I've already talked about this, I won't

178
00:12:31,580 --> 00:12:35,980
focus on that, but you can either activate your
virtual environment manually, or you can

179
00:12:35,980 --> 00:12:41,280
use `uv run` and run Whisper. In this
case, I've already put this here to

180
00:12:41,280 --> 00:12:45,000
Or you can use it like this: `uv run whisper`, or if you activated the

181
00:12:45,000 --> 00:12:49,580
virtual environment, directly with the Whisper command here.
So, Whisper, and then you'll see why

182
00:12:49,580 --> 00:12:54,000
this video might get pretty long,
right? Look at the number of options you

183
00:12:54,000 --> 00:12:57,720
it's here in Whisper. So you have the
audio, the audio that's speaking, it speaks in

184
00:12:57,720 --> 00:13:01,180
`audio file` here, but it can also be video.
I'll show you everything in video here,

185
00:13:01,620 --> 00:13:05,200
but you have this `help`, you have the `model`,
you have all these options here that I

186
00:13:05,200 --> 00:13:08,860
talked about in this article, I even
told you about some that I

187
00:13:08,860 --> 00:13:08,860
think you can use, but you have to
use the running mode, which is the running

188
00:13:08,860 --> 00:13:08,860
mode, which is the running mode, which
is the running mode, which is the running

189
00:13:08,860 --> 00:13:09,220
mode, which is the running mode, which
is the running mode, which is the running

190
00:13:09,220 --> 00:13:11,380
mode. Look, I've never even used this one
and I don't even know what it's for,

191
00:13:11,480 --> 00:13:14,680
so, I said what it's for, but I said, I
didn't use this. But let's

192
00:13:14,680 --> 00:13:18,460
So, what are the arguments I use most
often in Whisper here? First, the

193
00:13:18,460 --> 00:13:22,100
audio.  As I mentioned, this audio can
also be a video. So, if I

194
00:13:22,100 --> 00:13:26,600
type `whisper`, the path to my file here,
for example, `whisper`, I'll grab

195
00:13:26,600 --> 00:13:30,880
the audio, the first video I recorded here,
which I called `one-auto.mp4`. If I

196
00:13:30,880 --> 00:13:33,900
send this to it, I'm not sending much
information, I'm just telling it

197
00:13:33,900 --> 00:13:37,800
this: look, it's `whisper`, here's the
audio. The audio, actually, that I'm

198
00:13:37,800 --> 00:13:42,500
sending is an MP4 video of the intro.
FFmpeg will handle loading this

199
00:13:42,500 --> 00:13:47,500
audio, it will extract the audio from
the video and then there's code here in
Whisper,

200
00:13:47,600 --> 00:13:52,460
that you can see in the documentation,
that it uses this, because

201
00:13:52,460 --> 00:13:56,760
For example, `load_audio` will load the audio,
for instance, and then it will do this,

202
00:13:56,860 --> 00:14:01,280
this function will use this function here,
and this function will convert this audio into

203
00:14:01,280 --> 00:14:06,600
a 30-second clip.  Then, after that, it will
generate that log-mel spectrogram

204
00:14:06,600 --> 00:14:11,880
it will generate that spectrogram and pass it
into the

205
00:14:11,880 --> 00:14:16,220
`model` here. Then the `model` will detect the
language, it detected the language for it here,

206
00:14:16,340 --> 00:14:20,400
it will decode the options you passed to it here,
and then it will

207
00:14:20,400 --> 00:14:26,220
perform the decoding using the `model`, which is
the model you chose, the graph there,

208
00:14:26,280 --> 00:14:29,800
that graph I showed you, because Whisper doesn't
hear, it sees the audio and the

209
00:14:29,800 --> 00:14:33,720
options, and then it gives you a text result here.
We'll also use

210
00:14:33,720 --> 00:14:37,140
this way, which is much more practical,
basically, you're telling it to

211
00:14:37,140 --> 00:14:40,980
use this specific model here,
do the transcription and get the text. We

212
00:14:40,980 --> 00:14:45,200
will do this in the next video, but
when you run this command I was

213
00:14:45,200 --> 00:14:48,320
showing you, that's basically what it'll
do, it'll do just that, only

214
00:14:48,320 --> 00:14:52,780
it uses the function called `transcribe`
here, so if we

215
00:14:52,780 --> 00:14:57,320
look at the Whisper code itself, in the lib
here, for example, in site-packages, you

216
00:14:57,320 --> 00:15:02,300
will see that it calls this, this
`transcribe` function here. So this function,

217
00:15:02,420 --> 00:15:06,640
oops, this `transcribe` function, let me
see if I can find it here, this function here

218
00:15:06,640 --> 00:15:09,960
`transcribe`, it's basically what I'm
asking it to do there when I pass

219
00:15:09,960 --> 00:15:13,420
like this, and here you have several options,
those options over there, in fact there are

220
00:15:13,420 --> 00:15:17,480
more options there than here, because you
can also pass the decode options

221
00:15:17,480 --> 00:15:22,500
in here, and then it goes in here, and then
it does all this code here, all this,

222
00:15:22,580 --> 00:15:27,020
to do, it will slide through your audio, so
it grabs the first 30

223
00:15:27,020 --> 00:15:31,120
seconds, then slides to the next 30 seconds
and keeps doing that until it finishes the

224
00:15:31,120 --> 00:15:35,200
audio. In fact, in the way we used it, it
uses the previous 30 seconds

225
00:15:35,200 --> 00:15:39,960
to generate the next caption, so it has a
broader context. I'll show you

226
00:15:39,960 --> 00:15:42,480
this here, but you can see that the code is
huge, right? And here's

227
00:15:42,480 --> 00:15:45,840
that CLI you saw, that uses `argparse`,
which we've already talked about on this channel,

228
00:15:46,280 --> 00:15:50,060
Basically, all the options you saw in that help
menu are here,

229
00:15:50,300 --> 00:15:53,260
pure `argparse` right there. So I'm just showing
you what's going on under the hood

230
00:15:53,260 --> 00:15:58,780
here. So, if you hit enter here, it'll use the
`turbo` model. Keep

231
00:15:58,780 --> 00:16:02,740
that in mind, I'll show you the models later, but
if I hit enter

232
00:16:02,740 --> 00:16:06,100
here, what will it do? If it's your first time
doing this, it

233
00:16:06,100 --> 00:16:09,940
will download the `turbo` model first.  It's also
saying here that my

234
00:16:09,940 --> 00:16:13,920
CPU, I'm using a Mac, doesn't support FP16, that
it's

235
00:16:13,920 --> 00:16:19,500
using FP32. That's an option too. It already
detected Portuguese as the

236
00:16:19,500 --> 00:16:22,560
audio I was speaking in that introduction, and
now it's going to

237
00:16:22,560 --> 00:16:25,420
Let's start the translation.  Let's
wait, I'll let it process here.

238
00:16:25,420 --> 00:16:30,300
It's translating and I'll show you the
final result. Look, it's already generating

239
00:16:30,300 --> 00:16:34,540
the subtitles here and I can already
spot a problem; it's generating

240
00:16:34,540 --> 00:16:39,340
huge subtitles, and that would cause a
problem in the end,

241
00:16:39,400 --> 00:16:43,460
because what would happen? I'd have
subtitles that would take up too much space

242
00:16:43,460 --> 00:16:46,560
on screen. We can configure this, but I
want to show you what it

243
00:16:46,560 --> 00:16:51,140
generated. See if you remember if I said
this.  Hey everyone, turn on the subtitles

244
00:16:51,140 --> 00:16:56,040
for this video so you can see in real time
what I'm saying. Obviously, this is

245
00:16:56,040 --> 00:17:00,660
a subtitle, but this subtitle is special
because it was generated by this

246
00:17:00,660 --> 00:17:05,020
Here's the deal, this is OpenAI's Whisper,
an AI model from

247
00:17:05,020 --> 00:17:10,080
OpenAI that basically does speech
recognition, and through that

248
00:17:10,080 --> 00:17:14,380
speech recognition, it can do a lot of
things, for example, transcribe

249
00:17:14,380 --> 00:17:18,460
this video, which is the caption you're
seeing, or even do the

250
00:17:18,460 --> 00:17:22,300
translation. I'll see later if I do the
translation of this transcription. Look how

251
00:17:22,300 --> 00:17:25,800
crazy, I remember well, I think I said
those words precisely. So

252
00:17:25,800 --> 00:17:30,300
it's doing this transcription of the video,
so far I haven't seen any

253
00:17:30,300 --> 00:17:34,400
mistakes. I know where it makes a lot of
mistakes, I'll tell you about it here. But running

254
00:17:34,400 --> 00:17:38,560
the command, just like that, you'll get
a caption like this if it detects it. It's worth

255
00:17:38,560 --> 00:17:42,360
I want to highlight something important here: my
audio is coming from this microphone, which is a

256
00:17:42,360 --> 00:17:48,380
super high-end mic, it's really good. I route
my audio, I'll show you here in OBS, and

257
00:17:48,380 --> 00:17:53,380
it has a noise filter. So this audio you're
hearing is super clean.  It might be that

258
00:17:53,380 --> 00:17:57,720
if your audio isn't as clean, you'll need to
use some of the

259
00:17:57,720 --> 00:18:02,040
options I'm going to show you here. So let's
go, we saw Whisper, just the

260
00:18:02,040 --> 00:18:06,460
process, and then it generated the captions.
Let me also show you where those

261
00:18:06,460 --> 00:18:11,200
captions ended up. So these captions ended up
in the root of my

262
00:18:11,200 --> 00:18:14,240
project here, because I didn't specify where I
wanted it to generate the captions; it

263
00:18:14,240 --> 00:18:18,460
used the same name as the file I sent it,
which was oneauto.json, and

264
00:18:18,460 --> 00:18:22,940
So it generated all the formats it can.  It generated SRT, TSV,

265
00:18:23,100 --> 00:18:27,960
TXT, and VTT. So if you look at the TXT, it's
just the text I said without any

266
00:18:27,960 --> 00:18:32,900
timestamps. If you look at the SRT, for example,
it's the text I spoke in the

267
00:18:32,900 --> 00:18:37,480
exact second I started and finished speaking.
Started speaking and

268
00:18:37,480 --> 00:18:41,140
finished speaking, and so on. And then it has
other formats there, like TSV

269
00:18:41,140 --> 00:18:44,640
as well. I don't know this format, but
apparently it's also from where I started

270
00:18:44,640 --> 00:18:47,920
and where I finished speaking the sentence. It
generated the TXT, which I already showed you, and the

271
00:18:47,920 --> 00:18:52,740
VTT, which looks a lot like SRT, but
basically it's a WebVTT here, which also does

272
00:18:52,740 --> 00:18:56,800
this transcription. It's a subtitle there. Since
I didn't specify, it generated all

273
00:18:56,800 --> 00:19:00,800
These formats here, but we can
provide some information. Now, the

274
00:19:00,800 --> 00:19:04,880
gold standard for me, in my
conception, is JSON, okay? Because JSON,

275
00:19:04,880 --> 00:19:08,480
what's it gonna do? I'm gonna format this
JSON here, so here in the auto-editor, which is Ctrl

276
00:19:08,480 --> 00:19:12,540
Shift I, it formats this text for me.
I'll also hit Alt Z to wrap the

277
00:19:12,540 --> 00:19:16,100
line. And then it gets here, first, the
text, and this is the whole text you

278
00:19:16,100 --> 00:19:20,840
mentioned. This \u is Unicode, it's just a
character that represents an accent, or

279
00:19:20,840 --> 00:19:25,100
some character not in the ASCII table.
But it generates our text there,

280
00:19:25,160 --> 00:19:28,780
nicely, and then it grabs this part
here, this I find super interesting, which

281
00:19:28,780 --> 00:19:33,760
is the segments. The segments, it will have
an ID, which is the sequential ID, so it's like

282
00:19:33,760 --> 00:19:37,660
SRT 0, 1, 2, 3, and so on.  If I'm not
mistaken, SRT starts at 1. But

283
00:19:37,660 --> 00:19:41,220
it will have an ID, a position, when you
started and when you finished

284
00:19:41,220 --> 00:19:46,780
this speech here, pretty; it has the text
of this segment and it has the tokens of this

285
00:19:46,780 --> 00:19:50,480
segment here too. Tokens are important if
you want to suppress

286
00:19:50,480 --> 00:19:54,020
a token; there's that option too. Also the
temperature used, I

287
00:19:54,020 --> 00:19:57,220
will talk about all these things here for
you, but I just want to show you that you

288
00:19:57,220 --> 00:20:02,320
have these options. And these options, as I
told you, are gold here, because these

289
00:20:02,320 --> 00:20:07,220
options give you the possibility of knowing
how the model is detecting your

290
00:20:07,220 --> 00:20:12,020
audio. So we saw where these options came
from. Going back here, what else

291
00:20:12,020 --> 00:20:16,020
What do I have here? I have the following, I'll
look through my text because it's more

292
00:20:16,020 --> 00:20:21,560
organized. I can choose my model here. And
this will now involve

293
00:20:21,560 --> 00:20:26,600
part of your computer, because unfortunately
there's no way for us to run the

294
00:20:26,600 --> 00:20:31,160
Whisper on just any computer. If you don't
have a computer that can run

295
00:20:31,160 --> 00:20:35,700
Whisper, you can use the OpenAI API, which
has an API, if I'm not mistaken,

296
00:20:35,700 --> 00:20:41,600
it uses Whisper and also has versions with
GPT-4, something like that. But you can

297
00:20:41,600 --> 00:20:45,620
check the API, you can use practically the
same things

298
00:20:45,620 --> 00:20:50,340
that I'm going to tell you here. And then I
have the model, the model it uses by default

299
00:20:50,340 --> 00:20:55,680
is the turbo model. The turbo model is
excellent, super fast, but it requires

300
00:20:55,680 --> 00:21:01,540
It requires 6GB of VRAM to run. So, 6GB of
VRAM isn't something just any computer will have.

301
00:21:01,540 --> 00:21:05,420
This here. What is VRAM? Basically, in short,
VRAM is the memory

302
00:21:05,420 --> 00:21:09,460
of your GPU. So basically, you would need a
video card that

303
00:21:09,460 --> 00:21:13,900
has at least 6GB of VRAM to use this turbo
model. Of course, you can

304
00:21:13,900 --> 00:21:18,200
use smaller models, which are faster, but
also less

305
00:21:18,200 --> 00:21:24,880
accurate. So, as I usually use, here I'm on a
Mac M1 Max and this Mac has 32

306
00:21:24,880 --> 00:21:29,340
GB of memory. The Mac has a scheme, check if
your computer has this

307
00:21:29,340 --> 00:21:33,620
too, some computers have this, which is a
scheme that shares the memory

308
00:21:33,620 --> 00:21:38,520
total, which is 32GB, it shares that total
memory with the GPU as well, the Mac

309
00:21:38,520 --> 00:21:43,380
here. So the CPU and GPU use the same
memory block. So whatever I have

310
00:21:43,380 --> 00:21:46,620
available, it will use. This allows me,
for example, to use large models

311
00:21:46,620 --> 00:21:51,240
without any problem, requiring more or
less 10 GB of VRAM, but it runs, it gets

312
00:21:51,240 --> 00:21:57,140
slow, but it runs. So you can use the
tiny, base, small, medium, large, and

313
00:21:57,140 --> 00:22:01,720
turbo models. I'm going over this quickly,
but if you come here, I wrote a lot of

314
00:22:01,720 --> 00:22:04,860
information, a lot of insight here about
things I've been seeing while I've been

315
00:22:04,860 --> 00:22:10,200
using Whisper. So I highly recommend that
you read this and

316
00:22:10,200 --> 00:22:13,400
also watch the video, because I wrote this,
I'm recording the video quickly

317
00:22:13,400 --> 00:22:16,820
for you, to help you out.  And like the
video, comment down below, please!

318
00:22:17,080 --> 00:22:19,860
You guys don't say anything, you don't
engage with this kind of video, so I

319
00:22:19,860 --> 00:22:24,220
have to stop and start making videos
about, I don't know, taking a Nutella bath in the

320
00:22:24,220 --> 00:22:27,560
tub. So it doesn't work. So comment
on the video, say: hi, ok,

321
00:22:27,820 --> 00:22:32,600
everything's fine, just hi, everything's fine,
that's already good, it's already a comment. But getting back here, the tiny

322
00:22:32,600 --> 00:22:38,380
one here requires about 1 GB of VRAM,
so this is already easier to

323
00:22:38,380 --> 00:22:43,660
use. It's not very accurate, but it works
fine. The Base also requires

324
00:22:43,660 --> 00:22:47,700
about 1 GB, okay? So, between the two,
maybe using Base would be better,

325
00:22:47,880 --> 00:22:54,020
because Base, all those that have ".en" at
the end, they work both in English only,

326
00:22:54,240 --> 00:22:58,940
and in English and other languages.  The
others here that don't have ".en" at the end,

327
00:22:59,060 --> 00:23:02,420
They're just multilingual, alright? That's
basically the difference. So, that's why

328
00:23:02,420 --> 00:23:06,760
some have ".en", which means it works
specifically only in English, and

329
00:23:06,760 --> 00:23:10,860
multilingual, but some don't have that here.
So there's Tiny, Base, Small, and

330
00:23:10,860 --> 00:23:14,220
then you'll see the amount of VRAM you need
here. You can test this

331
00:23:14,220 --> 00:23:17,620
as well to see if it runs on your computer.
I usually use this a lot here, when I

332
00:23:17,620 --> 00:23:21,920
have time, because this takes forever, I
usually use the large-v2,

333
00:23:22,120 --> 00:23:26,960
which for me was the best one here, because
I use a lot of technical terms. If I

334
00:23:26,960 --> 00:23:31,440
say any technical term here, for example, I'm
going to say UV sync. If I say

335
00:23:31,440 --> 00:23:34,760
that, if in this subtitle I've already
corrected it, forgive me, but usually

336
00:23:34,760 --> 00:23:39,440
When I say that, it doesn't understand.
When I say "UV add rembg," it won't

337
00:23:39,440 --> 00:23:42,360
understand this phrase I just said.  So, what's
gonna happen is that I'll

338
00:23:42,360 --> 00:23:46,680
have to correct these captions, and that's
where Gemini comes in. I'll give it these captions,

339
00:23:47,000 --> 00:23:50,820
tell it that this was transcribed by... let me
show you here, so you can

340
00:23:50,820 --> 00:23:54,100
understand what I'm talking about. I give this
to Gemini and ask it to

341
00:23:54,100 --> 00:23:58,640
generate... there are several things, there's
a summary, translation, SEO, several things, but

342
00:23:58,640 --> 00:24:03,100
let's use this.  I ask Gemini to correct my
captions with this. So

343
00:24:03,100 --> 00:24:06,880
it's a code here, a script I threw together
quickly, but I pass chunks

344
00:24:06,880 --> 00:24:10,460
of the captions, sections of the captions to it
and tell it, look, you're a

345
00:24:10,460 --> 00:24:14,660
Technical subtitle reviewer in SRT format,
SubRip. The subtitles were

346
00:24:14,660 --> 00:24:19,640
automatically transcribed by an AI,
Whisper, and may contain errors in terms

347
00:24:19,640 --> 00:24:22,880
of programming. I put some code here for
him, these terms here have a lot of

348
00:24:22,880 --> 00:24:26,760
errors, so I say rembg, UV, and so on, I
tell him

349
00:24:26,760 --> 00:24:30,900
to. His job is to correct the words that
are wrong based on the context,

350
00:24:31,040 --> 00:24:35,540
correct the punctuation and add capital
letters at the beginning of sentences, maintain the

351
00:24:35,540 --> 00:24:39,580
original structure of the subtitle to avoid
misaligning the video, respect the blocks,

352
00:24:39,580 --> 00:24:43,540
timestamps and line breaks and so on. I
explain, show an example and

353
00:24:43,540 --> 00:24:46,900
he does all that and he says here, for
example, if I have... this is real,

354
00:24:46,980 --> 00:24:51,400
Okay? If I have a caption that was generated
yes, by Whisper, let me open the

355
00:24:51,400 --> 00:24:57,360
pyproject.toml, did you see what it said there?
To configure Python, this already

356
00:24:57,360 --> 00:25:01,760
happened too, I want you to return to me,
let me open the pyproject.toml

357
00:25:01,760 --> 00:25:06,420
to configure Python. And that's it,
all right? And then I pass this inside the

358
00:25:06,420 --> 00:25:10,880
Gemini, I generate this caption, I generated
this caption, I finish there, putting this

359
00:25:10,880 --> 00:25:14,340
caption on YouTube, making summaries,
making text articles and so on.

360
00:25:14,340 --> 00:25:17,600
So this is super interesting. So I, as I told
you, I usually

361
00:25:17,600 --> 00:25:21,880
use the Large model, but that's up to you
to check on your system

362
00:25:21,880 --> 00:25:26,260
here, in this case. To use the model, you just
have to say "model" and then the model

363
00:25:26,260 --> 00:25:30,300
whatever you want. For example, let's say I
want to use, in this case, the model

364
00:25:30,300 --> 00:25:34,320
Tiny. I've already downloaded all the models here,
because I tested all of this for you,

365
00:25:34,640 --> 00:25:40,400
but then the Tiny model here, it's pretty
fast to do this translation, but it

366
00:25:40,400 --> 00:25:43,580
isn't as accurate as the Turbo. So you see
it already started there, it already

367
00:25:43,580 --> 00:25:46,580
detected the language much faster than... I
don't know if I left that in the

368
00:25:46,580 --> 00:25:50,520
video, but it's much faster than the others.
And if you run it on Tiny and

369
00:25:50,520 --> 00:25:54,400
see that it's not generating that much error,
I'm not seeing much error here, if you see

370
00:25:54,400 --> 00:25:57,820
that it's not generating that much error, you
can use Tiny, because it's way faster

371
00:25:57,820 --> 00:26:02,380
and consumes much less of your computer's
resources. Now let me show you

372
00:26:02,380 --> 00:26:06,980
This is the same code I ran with the
Large model, just so you can see the difference. It

373
00:26:06,980 --> 00:26:12,400
will take forever to show you the
first one. So Large-v2, which is what

374
00:26:12,400 --> 00:26:16,040
I like to use when I have
time, keep an eye on it and I'll be

375
00:26:16,040 --> 00:26:20,200
opening my system right now, just
so you can see how many resources it's

376
00:26:20,200 --> 00:26:23,740
consuming from my system. Let me
reduce the screen size for you to see.

377
00:26:23,840 --> 00:26:27,200
Now that it's starting, see? I'm
talking to you here

378
00:26:27,200 --> 00:26:33,640
so I don't cut this video.
Look, it's consuming about 6 GB

379
00:26:33,640 --> 00:26:37,620
or so from my computer, but
I've already noticed that these 6 GB it's

380
00:26:37,620 --> 00:26:42,280
consuming from my computer, it will
increase as the video gets longer

381
00:26:42,280 --> 00:26:46,560
it's longer. So it'll reach those
10 MB or so at some point in the

382
00:26:46,560 --> 00:26:51,560
video. See, it hasn't even started
processing the first language yet. So I'm not

383
00:26:51,560 --> 00:26:55,220
gonna let it do that, I just wanted you
to see how many resources this

384
00:26:55,220 --> 00:26:59,220
model uses on your computer.
Hit Ctrl+C to stop it. If I go back to

385
00:26:59,220 --> 00:27:02,740
the Tiny one, let's see how much it uses
here. Back to Tiny, look, the

386
00:27:02,740 --> 00:27:08,100
Python here is using 388 MB and it's
already doing the whole job there. That's

387
00:27:08,100 --> 00:27:11,080
as I told you, because my video is 3
minutes long, if I'm not mistaken, so

388
00:27:11,080 --> 00:27:15,600
it's a shorter video, but it can keep going
up until it reaches 1 GB of memory, depending on the

389
00:27:15,600 --> 00:27:19,740
model you use. So that's it about the model.
Let me close this and the

390
00:27:19,740 --> 00:27:23,380
You can check out other options here.
Going back, I have the device. That's

391
00:27:23,380 --> 00:27:28,040
here, if you have an Nvidia card
with CUDA drivers and a compatible version

392
00:27:28,040 --> 00:27:33,240
with PyTorch, you can also use the
CUDA device. In my case, I don't

393
00:27:33,240 --> 00:27:36,680
have a video card, because as I told
you, I'm on a Mac, so I just

394
00:27:36,680 --> 00:27:40,560
had to use this. I'll keep Tiny here
for now, but I'll put the

395
00:27:40,560 --> 00:27:45,080
device here and, in my case, this is
already the default, so I don't need to add it, but

396
00:27:45,080 --> 00:27:49,940
in my case I put device CPU here and then
you would put CUDA if you had the

397
00:27:49,940 --> 00:27:56,080
Nvidia card. The output_dir is important
to avoid overloading my code too much, which

398
00:27:56,080 --> 00:28:00,020
it dumped everything here, so I'm going to put
everything here, I'm going to delete all this and I'm going to

399
00:28:00,020 --> 00:28:05,960
always save it to the "transcriptions" folder,
for example, so "-o" or "--output_dir",

400
00:28:05,960 --> 00:28:09,720
let me see, that's right, "-o" or
"--output_dir", and you can also choose the

401
00:28:09,720 --> 00:28:13,380
format, which defaults to "all", it already
includes all the formats it can handle. So I

402
00:28:13,380 --> 00:28:18,760
will put a "-o", and I'll put
"transcriptions" here, and it will create

403
00:28:18,760 --> 00:28:21,420
the folder if it doesn't exist. Let me
cancel this and do the

404
00:28:21,420 --> 00:28:24,620
following. Another option there, that you see
whenever it gives a warning at the

405
00:28:24,620 --> 00:28:29,660
beginning, saying this, two warnings:
first, this computer, which is a

406
00:28:29,660 --> 00:28:35,520
Mac, doesn't support this FP16, so it will
use FP32, which is float 32,

407
00:28:35,660 --> 00:28:40,280
float 32. FP16 is usually faster,
so that's why it tries first

408
00:28:40,280 --> 00:28:44,180
it uses FP16; if it doesn't support it, it switches
to 32, which is the case on this computer

409
00:28:44,180 --> 00:28:47,960
here. And also this one here, it's saying: "Look,
I'm detecting the language,

410
00:28:48,080 --> 00:28:52,580
but if you want, use "--language", to
specify the correct language so

411
00:28:52,580 --> 00:28:55,300
you don't get this warning.
So basically here, I'll say the

412
00:28:55,300 --> 00:29:02,660
following: --device CPU, transcriptions and
then --FP16 false, on my computer,

413
00:29:02,820 --> 00:29:06,900
in this case here, just so it doesn't give
that warning, "--language", here I'll

414
00:29:06,900 --> 00:29:11,900
put PT, for PT-BR. "Language" is the language
spoken in the video. So, if we look

415
00:29:11,900 --> 00:29:15,860
here, I want all formats.
So, if you want, just put "-f",

416
00:29:15,860 --> 00:29:18,860
any of these formats you want to use here.
And here I left an example

417
00:29:18,860 --> 00:29:23,260
for you. Another thing is: it does two things, it does this, and this I'm

418
00:29:23,260 --> 00:29:28,220
talking about here is the Whisper code. The
model does more than that, it does

419
00:29:28,220 --> 00:29:31,180
those things that are in the model, okay?
So we're talking about two separate things

420
00:29:31,180 --> 00:29:34,720
here. I'm talking about the code we're
using, the command we're using.

421
00:29:34,720 --> 00:29:38,220
This command line, it receives two options:
either transcribe or

422
00:29:38,220 --> 00:29:42,120
translate. Both do transcription; the only
difference is that transcribe

423
00:29:42,120 --> 00:29:46,540
transcribes to the language spoken in the
video, and translate will do that, but it

424
00:29:46,540 --> 00:29:50,900
will only generate English. So it will see
what you're saying in the video and

425
00:29:50,900 --> 00:29:55,040
will translate that to English. As I told
you, you can use the transcription

426
00:29:55,040 --> 00:29:59,000
the video's language to translate using

427
00:29:59,000 --> 00:30:03,400
any Gemini or GPT API.  So here, or any
other, I'm just

428
00:30:03,400 --> 00:30:07,740
guessing the ones I've already used, but
there are others. So here we can put the

429
00:30:07,740 --> 00:30:12,060
task there, so --task, this is already
standard, I'm putting this here just so you

430
00:30:12,060 --> 00:30:16,380
know. So transcribe here, or translate,
this is the standard transcribe.

431
00:30:16,740 --> 00:30:19,780
Another thing we can use, which we already
saw there, is the `language`, right?

432
00:30:19,880 --> 00:30:23,780
The `language`, what language is spoken in
the video? Sometimes you are, for example,

433
00:30:24,140 --> 00:30:29,080
a while ago I did one of an anime here,
okay? And then I needed to use the

434
00:30:29,080 --> 00:30:32,820
language spoken in the video, I don't even
remember, I don't know if it was Japanese,
Korean, I don't know what

435
00:30:32,820 --> 00:30:36,300
which is, but I had to use a language
from here, but it's hard to know what it is,

436
00:30:36,300 --> 00:30:40,260
I don't know what these letters are. So
you can take the long form here of the

437
00:30:40,260 --> 00:30:43,620
`name`, okay? So you can take, for
example, here, there's a proper name, in

438
00:30:43,620 --> 00:30:47,380
at some point you'll see, Portuguese.
You can use both ways, both with the

439
00:30:47,380 --> 00:30:50,880
short form, which is this, which I'm
using there, or the long form, `language`

440
00:30:50,880 --> 00:30:55,260
Portuguese. Okay. If you want to see, you
can go to the Whisper code, `tokenizer`

441
00:30:55,700 --> 00:31:00,840
`languages`, and then you'll see its
`languages`, just go to the Python `lib`, there

442
00:31:00,840 --> 00:31:04,420
where you're downloading your packages in
your virtual environment, and in the `tokenizer`

443
00:31:04,420 --> 00:31:08,420
you have `languages` here, which is this.
And then you have the short and long forms

444
00:31:08,420 --> 00:31:12,540
it's long there, which is the format it uses
there. So, they're all here, you can

445
00:31:12,540 --> 00:31:15,980
check them out here too.  Alright, having said that,
we've already seen this, this,

446
00:31:16,200 --> 00:31:20,160
`--temperature`, okay? So let's look at these
parts here, because these parts start to

447
00:31:20,160 --> 00:31:25,800
get into more obscure areas, because here you'll be messing with the `model`, you

448
00:31:25,800 --> 00:31:29,900
will specify how you want it to
work, okay? But let's do this,

449
00:31:30,300 --> 00:31:34,220
let's just run this command
here, I'll wait for it to finish, I won't

450
00:31:34,220 --> 00:31:36,980
make you wait, obviously, but let's
run this command, just so it gets

451
00:31:36,980 --> 00:31:42,100
saved there, and then we'll continue talking
about the temperature part, `--beam_size`,

452
00:31:42,220 --> 00:31:46,340
these things that are also interesting.
So folks, the model's temperature

453
00:31:46,340 --> 00:31:50,640
Here. When you talk about temperature, in
any AI model

454
00:31:50,640 --> 00:31:53,400
you're going to use, at least most of
the ones I've used have

455
00:31:53,400 --> 00:31:57,820
temperature. You're basically talking
about the model's creativity.  Like, you

456
00:31:57,820 --> 00:32:03,000
will give it more freedom to be more
creative, try more options, or you'll

457
00:32:03,000 --> 00:32:07,760
tell it to be strict and try only that
specific option. This

458
00:32:07,760 --> 00:32:10,820
connects to other parts here, in these
lower parts, which I'll

459
00:32:10,820 --> 00:32:15,500
explain nicely. But always think about
this, the model's creativity. The lower

460
00:32:15,500 --> 00:32:19,520
the number, the number here in Whisper
goes from 0 to 1, and you can use a

461
00:32:19,520 --> 00:32:25,980
`float` as well, so 0.something, up to 1.
In this case, in Whisper, 0 is like 0

462
00:32:25,980 --> 00:32:30,160
creativity, that's my go-to. I try, because of how my

463
00:32:30,160 --> 00:32:33,800
audio is recorded here, for example, the
microphone is very clean, I don't use much

464
00:32:33,800 --> 00:32:38,700
slang, I use a lot of technical terms, okay?
But even with temperature 0 or

465
00:32:38,700 --> 00:32:43,300
temperature 1, in my case, it doesn't change much,
it will try another word, in a

466
00:32:43,300 --> 00:32:46,780
different way there, but it doesn't change much.
So I always leave the temperature as

467
00:32:46,780 --> 00:32:51,100
0, but that will depend, as I'm telling you, on the
quality of your audio, on the

468
00:32:51,100 --> 00:32:54,440
thing the person is saying within
that audio, which model you're

469
00:32:54,440 --> 00:32:58,440
using, is it Tiny, is it Large, is it
Turbo, so it depends on several factors

470
00:32:58,440 --> 00:33:02,280
here. It's up to you to test and see what
comes out for you. As I'm telling you,

471
00:33:02,280 --> 00:33:07,460
Most of the time, I use temperature 0.  But temperature 0 activates

472
00:33:07,460 --> 00:33:12,060
certain options, and if you start increasing the temperature, giving more

473
00:33:12,060 --> 00:33:15,740
freedom to the model to choose certain things, you can also use

474
00:33:15,740 --> 00:33:19,520
other options. It's almost like an IF statement; I even created a

475
00:33:19,520 --> 00:33:23,040
table below for you to understand this. But let's say I have

476
00:33:23,040 --> 00:33:27,200
temperature 0. Temperature 0 means 0 creativity, but I can still

477
00:33:27,200 --> 00:33:32,500
configure --beam_size and --patience.  Basically, --beam_size will be the number

478
00:33:32,500 --> 00:33:37,520
of hypotheses you keep in parallel. So the model will do the following:

479
00:33:37,600 --> 00:33:41,280
you said a certain word, and the model isn't so sure about a certain

480
00:33:41,280 --> 00:33:46,460
Given a word, it will give several possibilities
for it to choose from. And then, based

481
00:33:46,460 --> 00:33:50,640
on those possibilities, it will choose the one
that has the highest probability of being what

482
00:33:50,640 --> 00:33:54,460
you said in the video. So this is basically
giving more hypotheses to the

483
00:33:54,460 --> 00:33:59,040
model, so it can select one. And patience is
the patience it has

484
00:33:59,040 --> 00:34:04,860
to choose the words. So patience is the
tolerance factor that makes

485
00:34:04,860 --> 00:34:09,200
the model continue exploring new hypotheses
after finding an acceptable one.

486
00:34:09,660 --> 00:34:13,860
So this and this, both --beam size and
--patience, are basically

487
00:34:13,860 --> 00:34:19,100
used only with temperature 0. If the
temperature is greater than 0, in this case

488
00:34:19,100 --> 00:34:25,700
you need to use --best_of here. And another
thing is,

489
00:34:25,700 --> 00:34:30,100
--patience here basically multiplies the
--beam_size. So if I have, I'm

490
00:34:30,100 --> 00:34:35,700
speaking as a layman here.  The
--beam_size is 2, for example. If you set

491
00:34:35,700 --> 00:34:41,300
--temperature to 0, --beam_size to 2 and --patience to 2,
I'm basically multiplying the --beam_size.

492
00:34:41,400 --> 00:34:46,160
So it's like I'm telling it, choose
--beam_size 2, two hypotheses

493
00:34:46,160 --> 00:34:50,720
of what you said, and do that twice.
--patience. That's basically what

494
00:34:50,720 --> 00:34:54,440
I understood about this, in this case. But
another thing you can do

495
00:34:54,440 --> 00:35:00,220
is greedy mode. Greedy mode means it finds
only one hypothesis and uses that hypothesis,

496
00:35:00,220 --> 00:35:03,580
which makes the code a little faster.
Not a little, much faster. Because

497
00:35:03,580 --> 00:35:09,080
the more hypotheses you give the model
and the more patience, the slower it will be

498
00:35:09,080 --> 00:35:12,820
to choose a specific option. So if
I want, for example, --temperature 0 and

499
00:35:12,820 --> 00:35:18,180
--beam_size 1, this, patience doesn't
work here. The --patience won't

500
00:35:18,180 --> 00:35:23,420
work. If I set --temperature 0 and
--beam_size greater than 1, then patience

501
00:35:23,420 --> 00:35:27,200
kicks in.  Understand the logic? So
that's basically it. This with

502
00:35:27,200 --> 00:35:30,860
--temperature 0. If I have --temperature
greater than 0, that is, more

503
00:35:30,860 --> 00:35:35,160
creativity, then it works differently.
Here with --beam_size it

504
00:35:35,160 --> 00:35:40,380
uses hypotheses. In --best_of, it will
choose samples. It will take several

505
00:35:40,380 --> 00:35:44,200
different samples of what it has and then
it will choose among the specific

506
00:35:44,200 --> 00:35:47,920
samples there. You can test both options.
As I told you, I tested

507
00:35:47,920 --> 00:35:52,520
All of this before mentioning this
to you, and in my case, I didn't see much

508
00:35:52,520 --> 00:35:56,720
difference. I researched all about
this, and what happens is that this will

509
00:35:56,720 --> 00:36:00,640
kick in when you have very bad audio or
when the person is speaking

510
00:36:00,640 --> 00:36:04,800
a lot of slang or a lot of technical terms in
that particular video, then this will kick in

511
00:36:04,800 --> 00:36:09,180
here. And a quick tip here for you is: when
the temperature

512
00:36:09,180 --> 00:36:13,840
is greater than 0, it will use samples,
sampling. So here in this

513
00:36:13,840 --> 00:36:18,480
case, only the --best_of option works, and
the default is 5 samples that it uses.

514
00:36:18,600 --> 00:36:22,560
When you set the temperature above 0, it
automatically does this, but if

515
00:36:22,560 --> 00:36:25,780
you want to change it for more or less
samples, you can use this option.

516
00:36:26,160 --> 00:36:30,740
When the temperature is 0, it uses Beam
Search, and in this case, which are

517
00:36:30,740 --> 00:36:36,660
the hypotheses, it will by default use 5
Beam Search, 5 hypotheses, and the

518
00:36:36,660 --> 00:36:42,740
default --patience is 1. So these will
only be 5 hypotheses. If I increase

519
00:36:42,740 --> 00:36:45,980
the --patience here, as I told you before,
I'm basically going to multiply my

520
00:36:45,980 --> 00:36:50,180
number of hypotheses. Again, that's what I
understood from this. What does it do?

521
00:36:50,440 --> 00:36:54,080
Because I was also looking at the code,
how the code was written, so I

522
00:36:54,080 --> 00:36:58,480
went deep into this. And here in this case
this part is ignored, the Best

523
00:36:58,480 --> 00:37:03,580
Of is ignored. Whenever the temperature is
0, the --best_of is ignored. The temperature

524
00:37:03,580 --> 00:37:09,520
equal to 0 and --beam_size equal to 1, this
is called greedy. It will basically find

525
00:37:09,520 --> 00:37:13,340
Let's try the first option and use it, don't
overthink it, it's usually faster.

526
00:37:13,340 --> 00:37:18,640
It's pretty quick here. The default is temperature 0 and
--beam_size equals 5, alright? That's the

527
00:37:18,640 --> 00:37:22,640
default. If we change that, we can even see if it'll be faster. So

528
00:37:22,640 --> 00:37:29,740
if I set, for example, temperature 0 and
--beam_size equals 1, which will be the

529
00:37:29,740 --> 00:37:34,500
greedy, the Tiny model should be much faster
here than the way I was using it

530
00:37:34,500 --> 00:37:37,200
before. You see it's already spitting out
almost the entire caption,

531
00:37:37,260 --> 00:37:41,220
it'll reach the end in, I don't know, 10
seconds. That's because I used the Tiny

532
00:37:41,220 --> 00:37:45,820
model, but there's also a higher chance of it
making mistakes. A quick tip for you:

533
00:37:46,200 --> 00:37:50,880
When you use the default options, the way
Whisper comes, it already works, for

534
00:37:50,880 --> 00:37:54,900
For me, at least, it works perfectly.
You don't need to change much about how

535
00:37:54,900 --> 00:37:58,800
the model works. I explain everything here
for you, I even show you in practice, the

536
00:37:58,800 --> 00:38:02,800
model will respond the way it was trained,
regardless of your tweaking in the

537
00:38:02,800 --> 00:38:08,040
configs. So, changing the temperature, beam
size, --patience and so on, maybe, depending

538
00:38:08,040 --> 00:38:12,080
on your audio, might be a waste of time.
When would I change this,

539
00:38:12,160 --> 00:38:15,860
if I were to change it?  Many times, this
also depends on the audio, this has happened

540
00:38:15,860 --> 00:38:19,700
to me a few times, the model might get
stuck in a loop. Like, it picks up a

541
00:38:19,700 --> 00:38:23,680
word, the last word it detected there, like
"1", it says "1", then it keeps

542
00:38:23,680 --> 00:38:27,100
saying "1", "1", "1", until those 30
seconds are up. So, it gets stuck in a loop.

543
00:38:27,500 --> 00:38:31,840
In that case, then I start tweaking the
--beam_size, --patience, and so on, and

544
00:38:31,840 --> 00:38:35,940
there are other options down there that can
also change how the model works

545
00:38:35,940 --> 00:38:40,200
there. Another thing that's already there, by
default, is --temperature Increment on

546
00:38:40,200 --> 00:38:44,740
Fallback. If it discards a certain option, it
will try again,

547
00:38:45,060 --> 00:38:48,880
basically, with this option here. By
default, this option raises the

548
00:38:48,880 --> 00:38:53,320
temperature to 0.2, and then, anyway, it will
try to generate some text

549
00:38:53,320 --> 00:38:56,500
there for you. If it can detect anything, it
will generate some text

550
00:38:56,500 --> 00:39:00,440
there for you, or if it can't, it will loop,
then you start to

551
00:39:00,440 --> 00:39:04,320
use the options. This is super interesting,
which is the following: Max Line

552
00:39:04,320 --> 00:39:09,700
Width is basically the line width.
So, here, you're noticing that it's

553
00:39:09,700 --> 00:39:14,140
generating huge captions.  This doesn't
always happen, it's good to test this,

554
00:39:14,140 --> 00:39:18,660
because what happens? Sometimes, it
generates captions

555
00:39:18,660 --> 00:39:23,120
in the perfect size for you. But if you
let the model generate the captions,

556
00:39:23,120 --> 00:39:27,100
what can happen is exactly this. Because,
for me, these captions it's

557
00:39:27,100 --> 00:39:30,240
generating here, let me see how many
characters are in this line, it's

558
00:39:30,240 --> 00:39:36,580
generating 102 characters in one line,
it's hard to read. So, I usually generate

559
00:39:36,580 --> 00:39:41,060
the caption with a width of 42, more or
less, or you can also choose the words.

560
00:39:41,380 --> 00:39:45,480
But let's look at the options here. By
default, in the hardcoded code it's 1000

561
00:39:45,480 --> 00:39:49,000
size, that's why it allows the model to
generate this. It's in the Subtitles

562
00:39:49,000 --> 00:39:54,500
Writer, in their class. But here, I told
you 42, I put 45 here, I

563
00:39:54,500 --> 00:39:58,780
use 42, I did. Later, if I remember, I'll
correct this. But this will

564
00:39:58,780 --> 00:40:03,480
correct it, but this requires you to
enable --words_timestamp. Because what

565
00:40:03,480 --> 00:40:09,180
happens? Then it will start tagging by
word. So, it starts tagging the

566
00:40:09,180 --> 00:40:13,720
timestamp of where you spoke and
finished. In this case, it tags by

567
00:40:13,720 --> 00:40:17,240
word specifically. So, each word has its
start, end, start,

568
00:40:17,420 --> 00:40:21,500
end, start and end. This allows you to
make almost a karaoke there. Because what

569
00:40:21,500 --> 00:40:24,720
will happen? You can enable an option
that is --highlight_words, I don't know if

570
00:40:24,720 --> 00:40:28,460
I added this option here, but it underlines
everything you're saying,

571
00:40:28,560 --> 00:40:31,820
the exact moment you're saying it.  Only
thing is, this takes a little longer.

572
00:40:32,040 --> 00:40:35,180
But let's test it. What do we need to do?
First, I need to set

573
00:40:35,180 --> 00:40:40,240
`words_timestamps` to `True`. Let's add it
here to the command. I'll add `words_`

574
00:40:40,240 --> 00:40:45,240
`timestamps` as `True`. I'll add this
option `--max_line_width`, alright. And then I'll

575
00:40:45,240 --> 00:40:49,400
add another option here to explain to you.
This `--max_line_width` I'll set to

576
00:40:49,400 --> 00:40:55,360
42, as I told you, and the `--max_line_count` I
want it to generate a maximum of two

577
00:40:55,360 --> 00:41:00,600
lines. This `max` is confusing, because in my
opinion it's not maximum, it's `line_`

578
00:41:00,600 --> 00:41:04,940
`count`. Why does this happen? When you
enable this option, what will happen is

579
00:41:04,940 --> 00:41:09,440
so now my captions will always
have two lines. And that doesn't come from the `model`

580
00:41:09,440 --> 00:41:13,400
here, you'll see that, if I'm not
mistaken, when it generates, here when it

581
00:41:13,400 --> 00:41:18,160
generates, that's not where it works.  That's
in the final caption. So here it keeps

582
00:41:18,160 --> 00:41:21,780
generating the same way, but in the final
caption, look how it turned out. It generates

583
00:41:21,780 --> 00:41:26,960
up to 42 characters, it gave 41 on this line and
two lines. But notice something, because

584
00:41:26,960 --> 00:41:31,700
what was I saying about `max` potentially
misleading us?  `max` isn't `max`,

585
00:41:31,840 --> 00:41:35,500
it's literally two lines. Because look, all
the captions now have two

586
00:41:35,500 --> 00:41:40,180
lines. Two lines, two lines and so on and so
forth. So that's what

587
00:41:40,180 --> 00:41:44,740
this does. Instead of using `line_width` here, you can
also use

588
00:41:44,740 --> 00:41:49,820
You can also use the number of `words` per
line, `--max_words_per_line`. This option

589
00:41:49,820 --> 00:41:54,100
lets you specify how many words you want per
line, but this

590
00:41:54,100 --> 00:42:00,940
option is overridden if you use `--max_line_width`.
You use either `--max_words_per_line`

591
00:42:00,940 --> 00:42:05,980
or `--max_line_width`. I don't use this
option; I use the top two

592
00:42:05,980 --> 00:42:09,460
I mentioned. And this `--highlight_words`
is super interesting because what

593
00:42:09,460 --> 00:42:12,980
happens? Let me show you; it's really cool.
All these options I'm

594
00:42:12,980 --> 00:42:16,480
talking about require `words_timestamps`,
and this tends to make the

595
00:42:16,480 --> 00:42:19,720
command a little slower.  Not just a little,
sometimes depending on the

596
00:42:19,720 --> 00:42:24,900
module it makes your command much slower.
I'll generate this and I'll show you

597
00:42:24,900 --> 00:42:29,040
Let me show you this `--highlight_words`
`True`, in this case. I don't know if I added this

598
00:42:29,040 --> 00:42:33,300
option here. `--highlight_words` is this.
This `--highlight_words` that I put here,

599
00:42:33,380 --> 00:42:36,680
I didn't set the value, later if I remember
I'll change it. The value here is to

600
00:42:36,680 --> 00:42:39,760
enable it, so you have to put `True`
in front, as I put

601
00:42:39,760 --> 00:42:43,600
here. So let me run this to see what it says.

602
00:42:44,000 --> 00:42:47,700
So it's generating the normal subtitles and I
want to show you that it will tell me this

603
00:42:47,700 --> 00:42:51,700
here at the time I'm saying the words.
So look how cool, let

604
00:42:51,700 --> 00:42:56,220
me see if I can enable this for you to see.
I'll grab our video, the

605
00:42:56,220 --> 00:43:00,280
video here, let me prepare everything here and
I'll show you. I'm putting the video

606
00:43:00,280 --> 00:43:04,860
Here, in the video I'm running, I'll go to
the subtitles, I'll add a subtitle and I'll

607
00:43:04,860 --> 00:43:09,780
go where I'm recording the transcription
and I'll put this here,

608
00:43:09,900 --> 00:43:12,740
this subtitle we generated. So notice
with me, let me try

609
00:43:12,740 --> 00:43:15,480
to make this better for you to understand.
Let me add a background.

610
00:43:15,740 --> 00:43:19,220
Notice with me that it's underlining each
word I'm

611
00:43:19,220 --> 00:43:23,040
speaking, precisely when I say the word.
Look, this is super cool.

612
00:43:23,120 --> 00:43:27,840
Let me just listen here to see if it's
working. Perfect, it's hitting

613
00:43:27,840 --> 00:43:32,120
perfectly here the word with where it's
underlining, so it makes almost a

614
00:43:32,120 --> 00:43:36,280
karaoke effect, but it's a little slower,
unfortunately. So okay,

615
00:43:36,420 --> 00:43:40,800
we generated this subtitle with all these
options here. So here I put a

616
00:43:40,800 --> 00:43:44,260
the full command for you to see, I
added this backslash here just to

617
00:43:44,260 --> 00:43:48,000
break the line, but you usually write
all of this on a single line

618
00:43:48,000 --> 00:43:52,440
in Unix, Linux, or Mac, I write the
command like this, but these

619
00:43:52,440 --> 00:43:55,860
backslashes are just showing that I
broke the line here to show you this

620
00:43:55,860 --> 00:44:00,580
full command. Oh, another interesting
thing we have here in Whisper is the

621
00:44:00,580 --> 00:44:04,920
`--initial_prompt`, which is as follows, okay? This
gave me a hard time, because I tested

622
00:44:04,920 --> 00:44:09,680
it in several ways and ended up not
seeing much use, I didn't see much

623
00:44:09,680 --> 00:44:14,480
function in this. Why? What happens?
This serves, honestly, to

624
00:44:14,480 --> 00:44:19,140
confuse the module, in my opinion,
because what is this

625
00:44:19,140 --> 00:44:24,200
Option? It's the first context of the first
30 seconds of the video. So what does it do?

626
00:44:24,400 --> 00:44:28,920
Whisper converts the first 30 seconds and
from the second it takes the

627
00:44:28,920 --> 00:44:33,220
context of the previous one, in this case.
So the first 30 seconds it's kind of

628
00:44:33,220 --> 00:44:37,500
contextless. If you want to use this
`--initial_prompt`, you can give it a hint

629
00:44:37,500 --> 00:44:40,780
to it, say look, in this I said a certain
thing. I've already tried using this

630
00:44:40,780 --> 00:44:45,220
here in every way you can imagine, saying
which words I'm

631
00:44:45,220 --> 00:44:49,740
talking about throughout the whole video,
talking about what the video is about, there are some

632
00:44:49,740 --> 00:44:55,560
discussions here in the OpenAI repository
that talk about this. I tested with

633
00:44:55,560 --> 00:44:58,720
some options here, I didn't see much
interesting functionality in this,

634
00:44:58,780 --> 00:45:03,320
at least in the command line.  Later in the
code, this might behave differently.

635
00:45:03,320 --> 00:45:07,780
here. But as I'm telling you, this works
really well in the first 30 seconds.

636
00:45:07,780 --> 00:45:11,740
If in the first 30 seconds you say a word
that it misinterprets,

637
00:45:11,740 --> 00:45:15,220
you can use the `--initial_prompt` flag
and specify what you're saying. For example,

638
00:45:15,220 --> 00:45:19,820
in the test I ran, I was talking about
REMBG, which removes the background from

639
00:45:19,820 --> 00:45:23,640
images. In that case, I took a short clip,
cut a snippet from the video, and

640
00:45:23,640 --> 00:45:28,040
said where I mentioned REMBG. So, in that
case, if I specify in the `--initial_prompt`

641
00:45:28,040 --> 00:45:31,840
that I said the word REMBG, because it
misinterprets this word, if I

642
00:45:31,840 --> 00:45:36,600
tell it that, it understands and corrects it
within the first 30 seconds. If in the second,

643
00:45:37,000 --> 00:45:41,920
within those first 30 seconds, in the next
block, I don't say the word REMBG,

644
00:45:41,980 --> 00:45:46,680
because in the first one I said REMBG,
it got it right. In the second one, if I say REMBG,

645
00:45:46,800 --> 00:45:50,460
it will pick up the context from the
previous 30 seconds and not this --initial_prompt.

646
00:45:50,900 --> 00:45:54,480
So what will happen is that, since I said
the same word twice in both blocks

647
00:45:54,480 --> 00:45:59,140
it gets it right again. If in the third
block I don't say REMBG and then in the

648
00:45:59,140 --> 00:46:02,820
fourth one I do, it lost that context,
then it will start to get it wrong again. So

649
00:46:02,820 --> 00:46:06,420
I thought this could be very useful if
you want to give the name, someone

650
00:46:06,420 --> 00:46:10,720
said your name at the beginning of the
sentence, it got it wrong, something like that, only in the

651
00:46:10,720 --> 00:46:14,340
first 30 seconds. And then it depends on
the repetition, in this case, as I was telling you

652
00:46:14,340 --> 00:46:17,260
explaining. So, --initial_prompt, I won't
even use it, but basically you would give

653
00:46:17,260 --> 00:46:20,040
it a little nudge. I even left an analogy
here for you to understand better.

654
00:46:20,540 --> 00:46:25,000
Imagine you're backstage and you're going
to give a hint to a

655
00:46:25,000 --> 00:46:28,780
singer. The singer is about to go on stage,
you say: "Hey, there are 300,000

656
00:46:28,780 --> 00:46:33,000
people waiting for you". The singer will go
on stage, see those 300,000

657
00:46:33,000 --> 00:46:36,480
people, the first 30 seconds, and then it's
the show, and then they forget, they've

658
00:46:36,480 --> 00:46:41,600
forgotten about it. So they'll keep doing
their show, but the context you gave

659
00:46:41,600 --> 00:46:45,360
was just for them to take that first step
on stage, the rest is up to them. So, that's

660
00:46:45,360 --> 00:46:51,640
why you use this. This can also cause some
problems,

661
00:46:51,760 --> 00:46:55,300
as I told you. Depending on how you write
that --initial_prompt, you

662
00:46:55,300 --> 00:46:59,780
can make the model start generating huge
captions, you can make the model

663
00:46:59,780 --> 00:47:03,720
get into a repetition loop. So, that's why
I said honestly that this

664
00:47:03,720 --> 00:47:07,740
is kind of to confuse the model and
whoever will use the model, because if you

665
00:47:07,740 --> 00:47:15,680
use this wrong, your transcription will be
weird.  That said, you can also

666
00:47:15,680 --> 00:47:19,760
do the following: if you want to test the
--initial_prompt, you can cut the video

667
00:47:19,760 --> 00:47:24,140
with FFmpeg. So FFmpeg, as you had to
install it to use Whisper,

668
00:47:24,460 --> 00:47:30,220
FFmpeg receives a command like this: `ffmpeg`
and `"-i"`, the input, and to be faster

669
00:47:30,220 --> 00:47:36,920
you copy the video codec, `-c:v`, `copy`,
copy the audio codec `-c:a`,

670
00:47:36,920 --> 00:47:41,240
`copy`, and then you can specify where you
want that video to start, the cut

671
00:47:41,240 --> 00:47:44,980
point. In this case, it's five minutes,
so it will start the video from the five

672
00:47:44,980 --> 00:47:49,120
minute mark, and where I want it to end, `-to`
in this case, which will end at 10

673
00:47:49,120 --> 00:47:53,020
minutes. What am I going to do? Imagine that
my video is 30 minutes long, I'm going to take

674
00:47:53,140 --> 00:47:57,560
from, I'm going to generate a new video that
starts at the five-minute mark of this video,

675
00:47:57,560 --> 00:48:01,800
going up to 10 minutes. In other words, I'm
going to generate a five-minute clip of this video

676
00:48:02,140 --> 00:48:05,480
using FFmpeg. And then I have a new video.
That's one way to test it. Another way to

677
00:48:05,480 --> 00:48:10,240
test is using the `clip` `timestamps` option
in Whisper itself, where you

678
00:48:10,240 --> 00:48:14,200
specify the starting and ending seconds, where
you want it to start

679
00:48:14,200 --> 00:48:18,040
transcription. And in this case, it will
leave, it will only grab the video at the

680
00:48:18,040 --> 00:48:21,620
location you mentioned, and then it will do the
transcription. I'll explain better later

681
00:48:21,620 --> 00:48:26,400
on. This --clip_timestamps. Here I've
already explained the FFmpeg command. Here

682
00:48:26,400 --> 00:48:29,880
it's also the following, this is what I
was telling you, this comes by default, but

683
00:48:29,880 --> 00:48:36,280
you can disable it. When the model uses the
first 30 seconds, from here on it

684
00:48:36,280 --> 00:48:40,600
will always, by default, use the
context of the previous 30 seconds, in the

685
00:48:40,600 --> 00:48:44,040
next context. So, from the second onwards it
always has the previous 30 seconds

686
00:48:44,040 --> 00:48:48,400
as context. This makes the model basically
have a minute of

687
00:48:48,400 --> 00:48:52,500
video context, so it doesn't get lost,
it knows what you were saying before,

688
00:48:52,680 --> 00:48:56,080
so it doesn't always start from scratch.
Because we know that the more

689
00:48:56,080 --> 00:49:00,440
context we have, the better. So, it does
this by default, grabbing the context from the

690
00:49:00,440 --> 00:49:05,580
previous block. But if you see that this is
generating an error loop, that

691
00:49:05,580 --> 00:49:09,620
problem, is generating some error that's
making it carry that error to other

692
00:49:09,620 --> 00:49:14,520
blocks, you can test, disable it. Just set
`condition_on_previous_text`

693
00:49:14,520 --> 00:49:19,840
to false. And then, in that case, it will be
conditioned to not use any context,

694
00:49:19,920 --> 00:49:24,280
it will only use the 30 seconds of audio,
which may also make your text less

695
00:49:24,280 --> 00:49:28,040
consistent, right? In this case, because it
won't remember what it said in the previous 30

696
00:49:28,040 --> 00:49:32,060
seconds, but there's this option here. Okay,
I left some recommendations

697
00:49:32,060 --> 00:49:35,860
Here's one for you, and here are some parameters
I didn't use, or barely used, but that

698
00:49:35,860 --> 00:49:40,620
are interesting for you to see. I didn't
test `length_penalty` here. It controls the

699
00:49:40,620 --> 00:49:46,520
penalization of long sequences, typical value
between 0.6 and 1. I didn't test it, just

700
00:49:46,520 --> 00:49:50,620
looked up what it does, so I won't even go
into details about it here.

701
00:49:50,940 --> 00:49:55,320
This one is interesting, I've used this one
a few times. This one, `suppress_`

702
00:49:55,320 --> 00:50:02,480
tokens, it allows you to cut out some things
that might be garbage for the

703
00:50:02,480 --> 00:50:05,660
model. Sometimes you'll say, it already does
this by default, it already comes with `-1` which

704
00:50:05,660 --> 00:50:09,860
will cut most things. Oh, hum, those little
things we say,

705
00:50:10,180 --> 00:50:14,000
noises that it might detect as noise or speech,
it already cuts those things

706
00:50:14,000 --> 00:50:19,300
automatically with `-1`. But if you want to
be specific, you can grab the

707
00:50:19,300 --> 00:50:23,560
tokens here and pass them to this `suppress_`
tokens here. Here's an example for you.

708
00:50:23,780 --> 00:50:28,800
This will cut out some useful things.
Just as an example for you, if your text

709
00:50:28,800 --> 00:50:34,140
is "Hi everyone, this is my text, with a
comma". Imagine this, "Hi everyone,

710
00:50:34,480 --> 00:50:39,900
comma, this is my text, period". If I pass
these tokens here, which are these

711
00:50:39,900 --> 00:50:44,220
ones I put down here, from this whole
sentence, your text won't have any

712
00:50:44,220 --> 00:50:47,740
of these things. So, your text will be
without commas, without "Hi", without

713
00:50:47,740 --> 00:50:51,160
"everyone", and without a period. Look, I'm
gonna grab this and show you this

714
00:50:51,160 --> 00:50:55,580
how it works. I'll come here, you see
there are several commas here and there must be

715
00:50:55,580 --> 00:50:58,960
Let me see if I can find a period somewhere.
Apparently, it

716
00:50:58,960 --> 00:51:07,920
didn't generate any periods here, but the
commas. With those `suppress_tokens`, I

717
00:51:07,920 --> 00:51:11,840
am suppressing a comma there too, which I
copied from there. So, if I run

718
00:51:11,840 --> 00:51:15,260
this command, it won't put any commas
here in the text. So, I'll run it

719
00:51:15,260 --> 00:51:19,840
just so you can see. Look, it's not putting
any commas and it already changed the

720
00:51:19,840 --> 00:51:24,560
behavior. You're seeing that it already
changed how it generates the caption width

721
00:51:24,560 --> 00:51:28,760
for some reason. You have to keep testing
to see what will happen. Each thing

722
00:51:28,760 --> 00:51:33,160
I do here changes how it works. Now it's
generating captions

723
00:51:33,160 --> 00:51:37,780
that are usable, they are much shorter, but
I was already passing the `--max_line_count` option

724
00:51:37,780 --> 00:51:42,660
There it is. And here's the underline I
showed you before, but I was already

725
00:51:42,660 --> 00:51:47,840
passing the `--max_line_count` options,
so it changed here. But it looks weird,

726
00:51:47,940 --> 00:51:52,380
you see, it should have a period
or a comma to go from here to there and

727
00:51:52,380 --> 00:51:56,240
so on. So, it's worth testing this
information. It's super interesting,

728
00:51:56,400 --> 00:52:01,140
but as I told you, test them first.
If you want to know what the tokens are

729
00:52:01,140 --> 00:52:05,860
here, I just went into Python, just to
show you. I went into regular Python

730
00:52:05,860 --> 00:52:10,060
here, and if I do a `print(1, 2, 3)`,
you've probably seen this before, obviously, right?

731
00:52:10,240 --> 00:52:13,960
But I'm just in regular Python, so what
can I do? I'm already in this project

732
00:52:13,960 --> 00:52:17,760
here with the virtual environment active. So,
if I import `from whisper.tokenizer`

733
00:52:17,760 --> 00:52:24,440
`import get_tokenizer`, it will import. Okay,
imported. If I say `tokenizer =`

734
00:52:24,440 --> 00:52:29,120
`get_tokenizer(True)`, the first
argument is `multilingual`, it's `multilingual`, I

735
00:52:29,120 --> 00:52:36,160
want it. The number of languages Whisper
supports, 99 in this case, I think it's 99,

736
00:52:36,300 --> 00:52:42,640
I know. The Portuguese language I'll be
using and the `task` which is `transcribe`, I

737
00:52:42,640 --> 00:52:46,480
can use this `tokenizer`. So now I can say,
for example, here in the option

738
00:52:46,480 --> 00:52:49,720
there, I was getting there, folks, comma,
this is my text. If you want to convert

739
00:52:49,720 --> 00:52:55,880
a text into tokens, you use `encode`, for
example, `token.encode` and then you can say

740
00:52:55,880 --> 00:53:00,860
there, "Hello world", these are the tokens the
model picks up there. In this case, `token`

741
00:53:00,860 --> 00:53:04,760
`.encode`, what did I put? `tokenizer`,
people, I put `token`. `tokenizer`

742
00:53:04,760 --> 00:53:09,680
`.encode` shows which tokens I got.  Ah, I want to know

743
00:53:09,680 --> 00:53:15,580
what's 401. `tokenizer.decode`, and then I pass a list
with the tokens I

744
00:53:15,580 --> 00:53:21,140
want. I only want 401, which is? It's "Hi", in this case,
see? "Hi". And the next one

745
00:53:21,140 --> 00:53:31,200
is 842, it will be "A", a regular "A". The other one
is 7968, which is "world". So, if I pass

746
00:53:31,200 --> 00:53:34,900
all these tokens here, it should generate, it shouldn't
generate, right? It will generate what I

747
00:53:34,900 --> 00:53:39,100
told you, which is the text there. So here, "Hi world",
was the word that this

748
00:53:39,100 --> 00:53:42,740
`tokenizer` generated. So this is interesting for you to
know which token and

749
00:53:42,740 --> 00:53:45,860
also, besides this, okay? What I'm saying is for you to
know which

750
00:53:45,860 --> 00:53:48,620
token, in case you want, look, I want to
see which token it's generating here.

751
00:53:48,780 --> 00:53:52,320
But another thing you can do
is this, go to the JSON that

752
00:53:52,320 --> 00:53:56,820
it generates and then you can take a look. I think
it's harder and less precise this way, but

753
00:53:56,820 --> 00:54:01,380
you can do it like this. If I take here
any text that it spoke there, I

754
00:54:01,380 --> 00:54:04,920
can see which tokens it generated
here. So this should match here.

755
00:54:05,120 --> 00:54:07,820
This should be a space, right? This
initial one here. Let's look here, this

756
00:54:07,820 --> 00:54:13,600
one, 50364. I think that's a space
over there. Let's see. Yeah, it really is an

757
00:54:13,600 --> 00:54:16,940
empty text, you see? But just so you
understand what I was telling you,

758
00:54:17,240 --> 00:54:20,780
you can look here at what it
generated, okay? And it doesn't match exactly, as you

759
00:54:20,780 --> 00:54:25,460
see that the word "hello", it took "OL" and "A"
separately, it doesn't match exactly here. So I

760
00:54:25,460 --> 00:54:28,880
I think there's a more precise way I showed you,
so I know what each thing is.

761
00:54:28,880 --> 00:54:34,180
For example, if I take this here, this list,
and put it there, we can see, decode this list.

762
00:54:34,180 --> 00:54:38,760
So, pasting it here will be wrong because of
the following. Let me fix it here, to avoid any

763
00:54:38,760 --> 00:54:43,440
indentation. And then we look at that.  So
looking at it now, see? It looks

764
00:54:44,600 --> 00:54:48,060
super weird. I pasted it, now it's going. So,
look, "hello", then we'll see what I'm

765
00:54:48,060 --> 00:54:53,600
talking about, okay? So it's basically this.
And then, it seems the Tiny model is

766
00:54:53,600 --> 00:54:58,240
super imprecise here, okay? So I like to use
larger models, but I'm

767
00:54:58,240 --> 00:55:02,980
using Tiny here just to show you faster what
it's doing.

768
00:55:02,980 --> 00:55:07,020
I'm using Tiny here just to show you quickly
what it's doing.

769
00:55:07,020 --> 00:55:14,060
generating. So let's look at other options.
This FP16 is float16 or float32. Float16

770
00:55:14,060 --> 00:55:18,600
tends to be faster, right? It's faster.
So, here, to prevent it from generating

771
00:55:18,600 --> 00:55:21,480
a warning, I think I already set it,
didn't I? Let me see. Yes, I did, see?

772
00:55:21,640 --> 00:55:26,000
To prevent it from generating a warning,
I set FP16 to false. But don't do that,

773
00:55:26,060 --> 00:55:30,400
if it doesn't generate a warning, it's
using FP16 there. And this is one of

774
00:55:30,400 --> 00:55:35,240
the first lines of code from the
`transcribe`. If you look in `transcribe`,

775
00:55:35,980 --> 00:55:40,900
which is this function here, it's one of
the first lines there that I was showing

776
00:55:40,900 --> 00:55:45,660
you before. What is it checking here?
Let me see up here. Here, at the

777
00:55:45,660 --> 00:55:49,800
beginning, it will check if your
`device`, in this case, if you are

778
00:55:49,800 --> 00:55:54,940
saying you want FP16 as a `tool`, or
if you don't specify, it will

779
00:55:54,940 --> 00:56:00,240
try 32. But I think that's it here.
It will check if the model `device` is

780
00:56:00,240 --> 00:56:06,120
a CPU, then it won't use FP32.  Yeah,
that's it. Then it checks if the

781
00:56:06,120 --> 00:56:10,140
device has CUDA, in this case, if it
goes in here, because if you're

782
00:56:10,140 --> 00:56:14,940
using a CPU and so on. I'm reading the
code here, more or less,

783
00:56:15,040 --> 00:56:18,800
for you, but it's from the first few
lines here, where it tells you, look, if your

784
00:56:18,800 --> 00:56:26,420
device doesn't support FP16, it will
use FP32. For the next options,

785
00:56:26,560 --> 00:56:32,040
let's look here. This is interesting
too, which is the following, `Compression`

786
00:56:32,040 --> 00:56:37,060
`Ratio Threshold`. This does the
following, it will, imagine you have

787
00:56:37,060 --> 00:56:42,140
Here's a word, a phrase of that type,
it keeps repeating the same thing here.

788
00:56:42,540 --> 00:56:47,180
When you compress this with Gzip,
which is what it uses here, what will

789
00:56:47,180 --> 00:56:50,940
happen is that the compression ratio gets
very high, because if you repeat a lot of

790
00:56:50,940 --> 00:56:54,000
things, the compression is smart enough
to throw all those things

791
00:56:54,000 --> 00:56:58,660
into one thing. And then the compression
is high, you compressed that section a lot.

792
00:56:58,860 --> 00:57:03,740
When you say something much more
diverse, with several separate words,

793
00:57:03,800 --> 00:57:08,200
which is how we usually speak, the
compression is lower. So what

794
00:57:08,200 --> 00:57:12,480
happens? It can detect if, in a certain
section, Whisper got into a

795
00:57:12,480 --> 00:57:17,820
loop, repeating a certain thing there. If
that happens to you, you can

796
00:57:17,820 --> 00:57:22,500
let's try setting the Compression Ratio to zero, and then
let's see if that improves things or

797
00:57:22,500 --> 00:57:27,640
not. The default value is 2.4. I don't remember
if that's in that JSON file, let me see

798
00:57:27,640 --> 00:57:33,660
here. That JSON file generated for us,
it has it. Look, here's my Compression

799
00:57:33,660 --> 00:57:37,980
Ratio, from that section I was looking at,
which is this little section here, this

800
00:57:37,980 --> 00:57:42,660
one up here, this Compression Ratio was
one point, that's the whole value here. That's why

801
00:57:42,660 --> 00:57:45,780
I was telling you that this JSON is
super important for us, because it

802
00:57:45,780 --> 00:57:50,280
shows how the model is performing.
So in this case, it's within the

803
00:57:50,280 --> 00:57:55,500
standard. If it went over 2.4, we'd
have a problem. This isn't something

804
00:57:55,500 --> 00:57:59,340
I change much unless I have a problem, I always keep the default. But

805
00:57:59,340 --> 00:58:03,640
as I'm telling you, the compression ratio is
exactly what you see in the JSON.  You can

806
00:58:03,640 --> 00:58:07,620
check it if your model gets stuck in a loop,
or if it's generating

807
00:58:07,620 --> 00:58:11,080
something really messed up, you can also
check the compression ratio there

808
00:58:11,080 --> 00:58:15,960
in the JSON and see if it's exceeding the
standard, which is 2.4. I don't think any of

809
00:58:15,960 --> 00:58:22,440
mine goes above 2.4. Let me look it up and
check. They're all around 1.7

810
00:58:22,440 --> 00:58:28,740
something, all of them. This one went up a
little, 1.89. It stays in that range. None

811
00:58:28,740 --> 00:58:32,520
went above 2.0 here, the compression ratio.
Because we usually talk about

812
00:58:32,520 --> 00:58:36,300
very diverse words here, so we usually won't
have a problem with that. But

813
00:58:36,300 --> 00:58:42,400
this one got pretty high, it got to 1.9. It
was higher than I've seen so far, but if you

814
00:58:42,400 --> 00:58:46,040
No problem, you can test this option here too. Let's look here

815
00:58:46,040 --> 00:58:52,960
below, this log probe threshold also has
the AVG, if you see, it's this, AVG

816
00:58:52,960 --> 00:58:57,500
log probe. And then it gives you a value here
of this log probe. Let me go back there to

817
00:58:57,500 --> 00:59:02,420
see. This is the average of the
log probability. Complex stuff,

818
00:59:02,420 --> 00:59:08,380
right? If the average of the log
probability, log probe, of the tokens

819
00:59:08,380 --> 00:59:13,440
is below the value, in this case, it
treats it as an error. I messed up. Then it

820
00:59:13,440 --> 00:59:17,440
goes to the fallback, uses a higher
temperature. But the default is minus 1. So

821
00:59:17,440 --> 00:59:22,840
you can see the AVG log probe, the average
of the log probability of the sentences

822
00:59:22,840 --> 00:59:28,420
transcribed by Whisper using that JSON
I showed you at the end. I already told you

823
00:59:28,420 --> 00:59:32,600
from this JSON, right? See here? This is
the average, the default value is -1. So

824
00:59:32,600 --> 00:59:39,980
If it falls below this value, it'll
consider that an error and then

825
00:59:39,980 --> 00:59:44,000
it will try again with a higher
temperature, which is that fallback

826
00:59:44,000 --> 00:59:48,700
temperature there. Again, I'll repeat
this for you, because I only used these

827
00:59:48,700 --> 00:59:53,860
options to make this video, to talk about
Whisper. It's not something I

828
00:59:53,860 --> 00:59:57,740
use all the time. I've said this a few
times already. So if I'm

829
00:59:57,740 --> 01:00:05,760
wrong, I researched it, asked the AI,
I read the paper here from OpenAI, this

830
01:00:05,760 --> 01:00:10,120
paper here, you can read it too, it's more
like, it explains how the model was trained, but

831
01:00:10,120 --> 01:00:14,340
I read their whole paper to explain these
things to you. So it's not a

832
01:00:14,340 --> 01:00:18,980
It's something I'm figuring out,
but I'm not 100% sure either.

833
01:00:18,980 --> 01:00:24,220
about what's going on. I don't think
even the model team is 100% sure

834
01:00:24,220 --> 01:00:28,920
how the model works, because many times
you see the researchers talking and it seems

835
01:00:28,920 --> 01:00:33,140
like they're saying the model behaved
this way, as if the model

836
01:00:33,140 --> 01:00:36,660
had a life of its own. But you can
test these options, take a look

837
01:00:36,660 --> 01:00:41,500
here, because this was the basis of a lot
of research and, as I told you, read the paper,

838
01:00:41,620 --> 01:00:45,640
read everything. This is the no
speech threshold, I don't remember having

839
01:00:45,640 --> 01:00:48,960
used this, but this is something that
interests me, which is the following: the model

840
01:00:48,960 --> 01:00:52,860
can detect periods of silence. If the
model thinks it's silence, it will

841
01:00:52,860 --> 01:00:59,480
Using this token here, in the speech,
speaking and the decoding fails. It

842
01:00:59,480 --> 01:01:03,460
discards this section as silence,
this helps cut breaths, these

843
01:01:03,460 --> 01:01:07,900
things, or even some strange noise there,
that appears, noise won't be silence,

844
01:01:08,000 --> 01:01:11,980
so it discards what I said. But this
here it will treat as silence and this is

845
01:01:11,980 --> 01:01:16,040
a part that interests me, which is
that VAD there, that I was looking at

846
01:01:16,040 --> 01:01:20,580
maybe trying to do something that cuts
the silences of the videos using AI,

847
01:01:20,700 --> 01:01:23,640
but that's something for the future,
let me know if you want me to talk about this

848
01:01:23,640 --> 01:01:28,520
here later. This part here,
gave me a headache, but wow

849
01:01:28,520 --> 01:01:31,980
man, I tested this a lot,
because what happens? I'll explain the

850
01:01:31,980 --> 01:01:37,540
That's my problem here. I mostly, 90% of the time, worked with

851
01:01:37,540 --> 01:01:41,800
my native language, Portuguese.  At times I worked with

852
01:01:41,800 --> 01:01:46,240
Japanese, Korean, English, if I'm not mistaken.  I worked with a few languages, but that

853
01:01:46,240 --> 01:01:51,980
was a specific, separate case. But what happens? None of the languages I

854
01:01:51,980 --> 01:01:57,200
tested have punctuation coming after the words, because there are languages where the

855
01:01:57,200 --> 01:02:02,880
punctuation starts at the beginning of the word. This "prepend punctuation," as I understand it,

856
01:02:02,880 --> 01:02:07,140
means this argument controls which punctuation characters

857
01:02:07,140 --> 01:02:12,600
appear before a word and should be attached to the following word instead of

858
01:02:12,600 --> 01:02:17,220
being treated as a separate token. What does this mean? Here, if I took

859
01:02:17,220 --> 01:02:21,660
Here's a segment, you see
what it does? Look, if we

860
01:02:21,660 --> 01:02:25,680
observe, it does the following: in the text
it's this, this text is this segment,

861
01:02:25,700 --> 01:02:30,480
alright? Imagine there's punctuation here
at the beginning, I don't know, I think in Spanish

862
01:02:30,480 --> 01:02:36,300
it might be like "está", in this case.  At that
beginning punctuation, it has two options,

863
01:02:36,680 --> 01:02:42,940
the model can either separate that punctuation
into a separate word here, or it can

864
01:02:42,940 --> 01:02:46,820
join that punctuation with the following word,
which would basically be, imagine

865
01:02:46,820 --> 01:02:51,460
that there's punctuation behind it, imagine here,
it can treat the upside-down question mark

866
01:02:51,460 --> 01:02:55,860
as a standalone mark, or it can attach it
to that word, that's what that does.

867
01:02:56,180 --> 01:03:00,240
So, when you say this option here,
you're basically saying, this is the default,

868
01:03:00,420 --> 01:03:05,000
So he says to include quotes, parentheses,
commas, requires `words_timestamps=true` as well,

869
01:03:05,220 --> 01:03:09,280
which is, you know, to create timestamps per
word. And then I said, in theory, if the

870
01:03:09,280 --> 01:03:13,400
model generated, for example, this snippet,
this I think is real, I think

871
01:03:13,400 --> 01:03:17,780
that word "argument", it's split into
three pieces, but if it generated "argument",

872
01:03:18,340 --> 01:03:22,160
here, separately, like this, these
tokens here, let's look at these two tokens

873
01:03:22,160 --> 01:03:25,740
just to confirm, I don't know if
I left that open, I think I didn't

874
01:03:25,740 --> 01:03:29,500
leave that open, no, it doesn't even, but,
this would be the representation of this

875
01:03:29,500 --> 01:03:34,400
here, in this case. Notice that there's a
punctuation mark here, this punctuation,

876
01:03:34,440 --> 01:03:39,860
it's separated from this "arg" here, if I
do this and don't put this

877
01:03:39,860 --> 01:03:43,640
Regarding punctuation here, what it will do is
handle both this and this

878
01:03:43,640 --> 01:03:49,420
separately. If I use the pattern this way,
it will join this with this

879
01:03:49,420 --> 01:03:53,060
word here. I left an important note for you:
I tested Whisper

880
01:03:53,060 --> 01:03:57,440
mostly with two languages, Portuguese and
English. I tested one or

881
01:03:57,440 --> 01:04:02,900
another, but very little. And what happens
here, in my use case, is that I didn't

882
01:04:02,900 --> 01:04:08,360
see any place where it generated previous
punctuation. I even looked at

883
01:04:08,360 --> 01:04:13,380
the Whisper code functions, which are the
functions that do this,

884
01:04:13,460 --> 01:04:17,560
which is in the `timing`, if I'm not mistaken,
which is here, let me see if it's this

885
01:04:17,560 --> 01:04:23,000
one, it's this one, it's this function here,
which is in the `timing`, which is this

886
01:04:23,000 --> 01:04:27,580
`merge_punctuations` here, it receives
these things, it receives a list of

887
01:04:27,580 --> 01:04:32,000
words, basically, it has word tokens
here, but it receives a list

888
01:04:32,000 --> 01:04:36,800
with the words separated, and then it
receives what you're going to do the `prepend`,
from that

889
01:04:36,800 --> 01:04:40,940
option, and the `append`, which I haven't
mentioned yet, but it's there later, I managed to see that.  And then

890
01:04:40,940 --> 01:04:45,320
what it does is go in here, see if the
word has, this I found strange, it

891
01:04:45,320 --> 01:04:50,360
checks if the previous word starts with a
space, and I noticed that the punctuation in

892
01:04:50,360 --> 01:04:53,920
Whisper doesn't have a space, so I found
that strange, but okay. So

893
01:04:53,920 --> 01:04:58,300
it checks if the previous word starts with
a space, and if the previous word without the

894
01:04:58,300 --> 01:05:02,800
space, which is the punctuation without a space,
is in what you're passing here, and then it does

895
01:05:02,800 --> 01:05:07,380
This whole operation is about joining or separating
punctuation marks. This is for `prepend`,

896
01:05:07,440 --> 01:05:11,560
I couldn't see prepend, but append,
which is the "after" part, I could see that,

897
01:05:12,100 --> 01:05:16,700
and append, let me go back here, append,
which is this part here, is the same

898
01:05:16,700 --> 01:05:20,680
thing, this argument controls which
punctuation marks appear after

899
01:05:20,680 --> 01:05:24,840
a word, that should be attached to the
previous words. It's the same thing,

900
01:05:24,940 --> 01:05:28,920
but it's the inverse. Again, I could see
this, it's doable. So, if

901
01:05:28,920 --> 01:05:33,420
you put this here, what it will do is paste
this punctuation you put

902
01:05:33,420 --> 01:05:38,220
here in the word. So, for example, if
you have two tokens generating "ok", and then there's

903
01:05:38,220 --> 01:05:43,560
a loose period, when you pass this
here, separately, the question mark will

904
01:05:43,560 --> 01:05:48,620
sticking to the word at the end, resulting
in "ok?". And I left a tip

905
01:05:48,620 --> 01:05:52,680
here, these punctuation arguments will
only make a noticeable difference if

906
01:05:52,680 --> 01:05:58,480
you, for some reason, are using the
word's timestamp specifically

907
01:05:58,480 --> 01:06:02,820
to see if the period is attached to a
word or not. For me, it didn't

908
01:06:02,820 --> 01:06:06,320
make the slightest difference, because
I don't use these features, but

909
01:06:06,320 --> 01:06:10,320
as I'm telling you, I tested each of
these options, one or another that I

910
01:06:10,320 --> 01:06:14,620
couldn't test here. Alright, other
useful things, threads. Threads is the

911
01:06:14,620 --> 01:06:18,640
number of threads you're going to use
from your computer. I tested with

912
01:06:18,640 --> 01:06:23,720
1, 4, 10, 100, and 1000. What happened
was that basically my CPU was just

913
01:06:23,720 --> 01:06:28,120
consuming more resources, but it didn't
change the transcription speed here.

914
01:06:28,360 --> 01:06:33,520
This might make a difference on other
machines, it didn't on mine. But leave it

915
01:06:33,520 --> 01:06:37,560
as default, default is 0, 0 it will use
as much as it needs and that's it. This

916
01:06:37,560 --> 01:06:41,320
is interesting, --clip_timestamps, much
more used in my conception

917
01:06:41,320 --> 01:06:46,580
for... uh, I just thought about the use
here, but let's say you want to get

918
01:06:46,580 --> 01:06:50,640
a segment of the video, or several
segments, it can be all at once. You can

919
01:06:50,640 --> 01:06:55,480
use this with seconds, with start and end.
So, for example, in this case, if

920
01:06:55,480 --> 01:06:59,840
I say 10 and 30, what it will do is
transcribe only from the 10 seconds to the

921
01:06:59,840 --> 01:07:04,820
30 seconds of that video. Nothing more
than that. But, besides that, you can also

922
01:07:04,820 --> 01:07:08,500
pass this several times. So, I thought of
something like this useful for that

923
01:07:08,500 --> 01:07:12,080
Here. First, for testing, I want to
test a specific section of the video, not

924
01:07:12,080 --> 01:07:16,480
the whole thing, alright?
I might also want to transcribe, like

925
01:07:16,480 --> 01:07:20,420
I was saying before, sections of
videos in different languages. This can

926
01:07:20,420 --> 01:07:24,020
be interesting. Sometimes, from second
zero to 30, the person is speaking

927
01:07:24,020 --> 01:07:28,120
French, then I transcribe all the parts
where they spoke French. Then I also

928
01:07:28,120 --> 01:07:31,320
do another transcription, all the parts
where they spoke Portuguese, or any

929
01:07:31,320 --> 01:07:34,560
other language. So, that was a use case
I thought of right now.

930
01:07:35,640 --> 01:07:39,740
You can also pass several things.
So, it's always like this, beginning, end, start,

931
01:07:39,880 --> 01:07:45,880
end. This works perfectly, but if you do
something like this, in this

932
01:07:45,880 --> 01:07:51,160
In this case, when I passed it, it only starts the start,
right? So, it starts from this part and goes

933
01:07:51,160 --> 01:07:54,000
all the way to the end. This might work, alright. For me, it worked

934
01:07:54,000 --> 01:07:58,600
fine. It started at 4 minutes and 30 seconds and went to the end of the video. But

935
01:07:58,600 --> 01:08:02,820
I tested it in a way that I don't think this was expected. It starting from

936
01:08:02,820 --> 01:08:07,320
60, going up to 120, and then I just added a zero,
to see what it would do. It does

937
01:08:07,320 --> 01:08:12,820
exactly what's written here. It starts
transcribing from 60, goes up to two, or

938
01:08:12,820 --> 01:08:17,300
one minute to two minutes, ok, and
continues. But it continues as follows, it

939
01:08:17,300 --> 01:08:20,820
goes back to the beginning of the video and translates to the
end. So, it seems that this wasn't

940
01:08:20,820 --> 01:08:25,120
intended. If you see here, I even wrote.
Attention, this last case seems not to have

941
01:08:25,120 --> 01:08:30,000
It was unexpected. Here it is, the zero
that comes after 120, there, makes the module

942
01:08:30,000 --> 01:08:36,140
go back to the beginning of the transcription
and start over. This made my VLC player run

943
01:08:36,140 --> 01:08:40,920
these subtitles, but it cut the initial
part of the video, for some reason. Even

944
01:08:40,920 --> 01:08:47,060
though it went back to the beginning and
transcribed again, I think VLC reordered the

945
01:08:47,060 --> 01:08:51,360
subtitles there, and what happened was that
it cut the initial part, even though it

946
01:08:51,360 --> 01:08:56,660
did this here. It did this transcription,
but VLC cut it. So, this

947
01:08:56,660 --> 01:09:00,120
is a case I wouldn't use, okay?
So, I would only use this like this, beginning,

948
01:09:00,360 --> 01:09:05,940
end, beginning, end, maybe a beginning,
always after the previous one, but maybe

949
01:09:05,940 --> 01:09:10,360
a beginning and not by end, it goes to the
end. This is also interesting.

950
01:09:10,360 --> 01:09:16,040
In FFmpeg, you can do it with the -ss and -to flags.

951
01:09:16,100 --> 01:09:21,580
You can use just one or the other as well.  If you only use -to, it

952
01:09:21,580 --> 01:09:25,980
will only go up to that point, from the beginning to that point. If you only use -ss,

953
01:09:26,680 --> 01:09:31,720
it will go from that -ss point to the end of the video. Okay.  And this other part here, I

954
01:09:31,720 --> 01:09:36,960
haven't tested, didn't even touch this argument, but it's the `hallucination silence`

955
01:09:36,960 --> 01:09:41,440
`threshold`. It works with `word timestamps` and tries to detect long periods of

956
01:09:41,440 --> 01:09:47,360
silence where the model might have hallucinated, invented some text. If you

957
01:09:47,360 --> 01:09:52,680
pass 1.5, it will ignore silences longer than 1.5 seconds, which generates suspicious text.

958
01:09:52,680 --> 01:09:56,400
Again, I didn't touch this argument, you can test it yourself.

959
01:09:56,400 --> 01:09:59,820
So, I'm reaching the end here of
the things. I think we went through

960
01:09:59,820 --> 01:10:03,960
all the Whisper options here, okay? I
think you won't find any video

961
01:10:03,960 --> 01:10:08,160
that deep about Whisper and at this
point we're only talking about the command

962
01:10:08,160 --> 01:10:12,440
line of Whisper. We'll still have a
next video about Whisper talking

963
01:10:12,440 --> 01:10:16,020
about code, that is, we're gonna do
something to embed

964
01:10:16,020 --> 01:10:19,900
Whisper into our code and, you know,
do a transcription, something

965
01:10:19,900 --> 01:10:23,280
like that. But, instead of doing that
in this video, which I think will take more

966
01:10:23,280 --> 01:10:27,060
than an hour, we'll do that in the next
video together, alright? So that's it,

967
01:10:27,200 --> 01:10:28,500
guys. See you in the next video.
