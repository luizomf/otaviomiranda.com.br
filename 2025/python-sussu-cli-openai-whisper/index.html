<!doctype html>
<html lang="pt-BR">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />

    <title>
      Transcreva Ã¡udio com Python: Sussu CLI + OpenAI Whisper - OtÃ¡vio Miranda
    </title>
    <meta
      name="description"
      content="Aprenda a usar o Sussu, uma ferramenta de linha de comando feita em Python que utiliza o modelo Whisper da OpenAI para transcrever Ã¡udios e vÃ­deos de forma simples e eficiente."
    />

    <link rel="stylesheet" href="../../css/markdown.css" />

    <link rel="icon" type="image/webp" href="../../imgs/favicon-1.webp" />

    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/github-markdown-css/5.5.1/github-markdown-light.min.css"
    />

    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css"
    />

    <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
  </head>
  <body>
    <nav class="breadcrumb-nav">
      <a href="/">&larr; ir para inÃ­cio</a>
    </nav>

    <article class="markdown-body" id="content"></article>

    <script type="text/markdown" id="markdown-source">





# sussu(rro): CLI educacional com OpenAI Whisper

> Ferramenta de linha de comando focada em educaÃ§Ã£o e IA offline. Usa o
> poder do Whisper da OpenAI pra transcrever Ã¡udios e vÃ­deos de forma
> simples.

Ao rodar este projeto, uma das primeiras coisas que vocÃª vai querer fazer
Ã© usar o comando `whisper` para fazer a transcriÃ§Ã£o inicial de algum vÃ­deo
ou Ã¡udio. Essa transcriÃ§Ã£o Ã© um Ã³timo jeito de ver na prÃ¡tica como o
Whisper trabalha e o que esperar dos resultados.

- [RepositÃ³rio oficial do `whisper`](https://github.com/openai/whisper)

Por isso, vamos comeÃ§ar pela **instalaÃ§Ã£o** do projeto, o que vai
disponibilizar os comandos `sussu` e `whisper` no terminal.

## InstalaÃ§Ã£o do `sussu`

Caso tenha dificuldades com o ambiente, recomendo meu tutorial:

- [Ambiente Python Moderno 2025: UV, Ruff, Pyright, pyproject.toml e VS Code](https://www.youtube.com/watch?v=HuAc85cLRx0)

Este projeto usa o `Python 3.11.9` por questÃµes de compatibilidade com o
Whisper. Evite alterar essa versÃ£o caso nÃ£o saiba o que estÃ¡ fazendo,
porque eu **jÃ¡ testei tudo para vocÃª**.

AlÃ©m disso, este projeto tambÃ©m usa o [uv](https://docs.astral.sh/uv/) no
gerenciamento geral (pacotes, versÃ£o do Python, etc).

```sh
uv sync  # Ã© sÃ³ isso mesmo ğŸ˜…
```

`uv sync` Ã© suficiente para:

- Baixar e instalar o `python 3.11.9`
- Criar o ambiente virtual em `.venv`
- Instalar os pacotes necessÃ¡rios
- Buildar o `whisper` e o `sussu`

---

## `ffmpeg`

Ã‰ necessÃ¡rio ter o [`ffmpeg`](https://ffmpeg.org/), que Ã© um pacote de
software open source que contÃ©m uma coleÃ§Ã£o de ferramentas e bibliotecas
para lidar com arquivos multimÃ­dia, principalmente Ã¡udio e vÃ­deo. O
`whisper` trabalha com transcriÃ§Ã£o de arquivos de Ã¡udio, mas o `ffmpeg`
permite que vocÃª nÃ£o tenha que converter seus vÃ­deos em Ã¡udio para fazer a
transcriÃ§Ã£o.

Para instalar o ffmpeg no seu sistema use um dos comandos abaixo. Isso
veio diretamente do
[repositÃ³rio oficial do `whisper`](https://github.com/openai/whisper):

```sh
# no Ubuntu ou Debian
sudo apt update && sudo apt install ffmpeg

# no Arch Linux
sudo pacman -S ffmpeg

# no MacOS com Homebrew (https://brew.sh/)
brew install ffmpeg

# no Windows com Chocolatey (https://chocolatey.org/)
choco install ffmpeg

# no Windows usando Scoop (https://scoop.sh/)
scoop install ffmpeg

# Adicional
# no Windows usando winget (https://winstall.app/apps/Gyan.FFmpeg)
winget install --id=Gyan.FFmpeg  -e
```

**ObservaÃ§Ã£o:** os Ãºnicos comandos que testei da lista acima foram do
MacOS e Ubuntu. Aprovados âœ…!

---

## Rodando pela primeira vez

Para testar se tudo funcionou perfeitamente vocÃª pode tanto **ativar o
ambiente virtual** quanto usar **`uv run`**. Teste com `whisper -h`. Isso
deve mostrar a `help` completa do `whisper`. Exemplos:

```sh
uv run whisper -h
# Ou se estiver com o ambiente virtual ativo
whisper -h
```

**ObservaÃ§Ã£o:** alguns editores como VS Code ou Zed, ativam seu ambiente
virtual automaticamente ao abrir uma nova instÃ¢ncia do terminal se tudo
estiver configurado corretamente, basta sair (`exit`) e abrir novamente o
terminal.

---

## `whisper -h`: entendendo alguns argumentos importantes

Ao digitar `whisper -h` ou `whisper --help`, vocÃª pode se assustar com a
quantidade de argumentos que estÃ£o ali, disponÃ­veis para uso. Claro que
vocÃª nÃ£o precisa saber o que cada um deles faz, na verdade, a maioria dos
argumentos tem valores padrÃ£o que jÃ¡ funcionam perfeitamente. Mas, caso
queira personalizar um pouco o comportamente, vamos analisar alguns deles.

O `whisper` usa o `argparse` do Python para criar essa `help` maravilhosa.
Caso queira aprender mais sobre isso, assista meu vÃ­deo:

- [Python e argparse: Do Zero a uma CLI Profissional (Projeto Real na PrÃ¡tica)](https://www.youtube.com/watch?v=Ad6934NXn4A)

---

### Argumentos essenciais do `whisper`

**`audio`:** este Ã© um argumento posicional que representa o caminho do
vÃ­deo ou Ã¡udio que serÃ¡ transcrito.

Exemplo:

```sh
whisper /caminho/do/arquivo.mp4
```

NÃ£o especifiquei nada alÃ©m de um caminho de vÃ­deo no comando acima, abaixo
detalho as opÃ§Ãµes que mais uso.

---

**`--model MODEL`:** define qual o modelo serÃ¡ utilizado na transcriÃ§Ã£o do
Ã¡udio. Ã‰ opcional, e o valor padrÃ£o Ã© `turbo`. Este model funciona muito
bem, Ã© rÃ¡pido e multilÃ­ngue, mas requer cerca de **6GB de VRAM** para
funcionar.

Talvez vocÃª queira usar outros modelos que usam mais ou menos recursos do
seu hardware, ou que possuem mais ou menos parÃ¢metros (como `base`,
`small`, `medium`, etc).

Abaixo os modelos disponÃ­veis:

- **`tiny`:** 39M, `tiny.en` e `tiny`, VRAM ~1 GB
- **`base`:** 74M, `base.en` e `base`, VRAM ~1 GB
- **`small`:** 244M, `small.en` e `small`, VRAM ~2 GB
- **`medium`:** 769M, `medium.en` e `medium`, VRAM ~5 GB
- **`large`:** 1550M, `large`, `large-v2` e `large-v3`, VRAM ~10 GB
- **`turbo`:** 809M, `turbo`, VRAM ~6 GB

**VRAM** Ã© um tipo especializado de memÃ³ria RAM usada pelas placas de
vÃ­deo (GPUs). Se o seu computador compartilha a RAM com a GPU, como
acontece nos Macs com chip Apple Silicon (M1, M2, M3 e posteriores), vocÃª
conseguirÃ¡ usar os modelos do Whisper **mesmo sem uma placa de vÃ­deo
dedicada**.

Nesse caso, o fator limitador passa a ser a **quantidade total de memÃ³ria
disponÃ­vel no sistema**. Por exemplo: se vocÃª tem apenas 8GB de RAM, o
ideal Ã© testar os modelos `tiny`, `base` ou `small`.

A partir do modelo `medium`, Ã© bem provÃ¡vel que vocÃª perceba uma **queda
absurda no desempenho geral da mÃ¡quina**, jÃ¡ que a memÃ³ria serÃ¡
completamente consumida.

---

**`--device DEVICE`:** se vocÃª tem uma placa de vÃ­deo NVIDIA com driver
CUDA e versÃ£o compatÃ­vel com o PyTorch, vale a pena usar `--device cuda`,
do contrÃ¡rio nem se preocupe em mexer com essa opÃ§Ã£o. PadrÃ£o Ã© `cpu`.

---

**`--output_dir` ou `-o`:** o caminho da pasta onde as transcriÃ§Ãµes serÃ£o
salvas. PadrÃ£o Ã© na raiz do projeto (`.`).

**`--output_format` ou `-f`:** o formato da transcriÃ§Ã£o (ou legenda) que
deseja. OpÃ§Ãµes: `txt`, `vtt`, `srt`, `tsv`, `json` e `all` (todos). O
padrÃ£o Ã© `all` (todos).

---

**`--task`:** vocÃª pode transcrever ou traduzir um Ã¡udio para inglÃªs. O
padrÃ£o Ã© transcrever no idioma que estÃ¡ sendo falado no Ã¡udio. OpÃ§Ãµes
`transcribe` (transcrever) ou `translate` (traduzir para inglÃªs).

---

**`--language`:** o idioma falado no Ã¡udio. SÃ£o muitas opÃ§Ãµes (veja
abaixo). O `whisper` Ã© capaz de detectar o idioma falado no vÃ­deo se essa
opÃ§Ã£o nÃ£o for enviada.

Forma curta (language code):

```python
["af", "am", "ar", "as", "az", "ba", "be", "bg", "bn", "bo", "br", "bs", "ca",
"cs", "cy", "da", "de", "el", "en", "es", "et", "eu", "fa", "fi", "fo", "fr",
"gl", "gu", "ha", "haw", "he", "hi", "hr", "ht", "hu", "hy", "id", "is", "it",
"ja", "jw", "ka", "kk", "km", "kn", "ko", "la", "lb", "ln", "lo", "lt", "lv",
"mg", "mi", "mk", "ml", "mn", "mr", "ms", "mt", "my", "ne", "nl", "nn", "no",
"oc", "pa", "pl", "ps", "pt", "ro", "ru", "sa", "sd", "si", "sk", "sl", "sn",
"so", "sq", "sr", "su", "sv", "sw", "ta", "te", "tg", "th", "tk", "tl", "tr",
"tt", "uk", "ur", "uz", "vi", "yi", "yo", "yue", "", "zh"]
```

Forma longa (language name):

```python
["Afrikaans", "Albanian", "Amharic", "Arabic", "Armenian", "Assamese",
"Azerbaijani", "Bashkir", "Basque", "Belarusian", "Bengali", "Bosnian",
"Breton", "Bulgarian", "Burmese", "Cantonese", "Castilian", "Catalan",
"Chinese", "Croatian", "Czech", "Danish", "Dutch", "English", "Estonian",
"Faroese", "Finnish", "Flemish", "French", "Galician", "Georgian", "German",
"Greek", "Gujarati", "Haitian", "Haitian Creole", "Hausa", "Hawaiian", "Hebrew",
"Hindi", "Hungarian", "Icelandic", "Indonesian", "Italian", "Japanese",
"Javanese", "Kannada", "Kazakh", "Khmer", "Korean", "Lao", "Latin", "Latvian",
"Letzeburgesch", "Lingala", "Lithuanian", "Luxembourgish", "Macedonian",
"Malagasy", "Malay", "Malayalam", "Maltese", "Mandarin", "Maori", "Marathi",
"Moldavian", "Moldovan", "Mongolian", "Myanmar", "Nepali", "Norwegian",
"Nynorsk", "Occitan", "Panjabi", "Pashto", "Persian", "Polish", "Portuguese",
"Punjabi", "Pushto", "Romanian", "Russian", "Sanskrit", "Serbian", "Shona",
"Sindhi", "Sinhala", "Sinhalese", "Slovak", "Slovenian", "Somali", "Spanish",
"Sundanese", "Swahili", "Swedish", "Tagalog", "Tajik", "Tamil", "Tatar",
"Telugu", "Thai", "Tibetan", "Turkish","Turkmen", "Ukrainian", "Urdu", "Uzbek",
"Valencian", "Vietnamese", "Welsh", "Yiddish", "Yoruba"]
```

JÃ¡ coloquei tudo com aspas e em uma lista para facilitar a sua vida. Mesmo
assim, se quiser um dicionÃ¡rio pronto, estÃ¡ em
`whisper.tokenizer.LANGUAGES`.

---

**`--temperature`:** controla a "criatividade" do modelo. Vai de `0.0` a
`1.0`. Quanto mais alto, mais liberdade o modelo tem pra decidir os
prÃ³ximos tokens. Esse parÃ¢metro interage com `--beam_size`, `--patience` e
`--best_of`.

**`--beam_size`:** nÃºmero de hipÃ³teses que o modelo mantÃ©m em paralelo.
Pensa como se ele testasse vÃ¡rios caminhos ao mesmo tempo e no fim
escolhesse o melhor. O padrÃ£o Ã© `5` e **sÃ³ funciona se
`--temperature == 0.0`**.

**`--patience`:** fator de tolerÃ¢ncia que faz o modelo continuar
explorando novas hipÃ³teses mesmo depois de achar uma aceitÃ¡vel. Requer
`--temperature == 0.0` e `--beam_size > 1`.

**`--best_of`:** nÃºmero de amostras diferentes geradas antes de escolher a
melhor. Funciona apenas quando `--temperature > 0.0`.

**Cola rÃ¡pida:**

```
- temperature > 0 â†’ usa sampling
  âœ… best_of 5 (5 amostras)
  ğŸ”´ beam_size (ignorado)
  ğŸ”´ patience (ignorado)

- temperature == 0 â†’ usa beam search
  âœ… --beam_size 5 (5 hipÃ³teses)
  âœ… --patience 2 (2 x 5 = 10 hipÃ³teses)
  ğŸ”´ best_of (ignorado)

- temperature == 0 â†’ greedy
  âœ… --beam_size 1 (1 hipÃ³tese)
  ğŸ”´ --patience (nÃ£o faz diferenÃ§a)
  ğŸ”´ best_of (ignorado)
```

**ObservaÃ§Ã£o sincera:**

Na prÃ¡tica, o modelo vai responder como foi treinado, independente do seu
capricho nas configs. Trocar `temperature`, `beam_size`, `patience` e
afins pode virar desperdÃ­cio de tempo.

**RecomendaÃ§Ã£o direta:** sÃ³ mexa nessas opÃ§Ãµes se:

- o modelo comeÃ§ar a repetir palavras (loop)
- estiver errando demais em blocos grandes

Se for sÃ³ por causa de uma ou duas palavras... aceita e segue. Ou entÃ£o
faz igual eu: **testa tudo por uma semana e conclui que o padrÃ£o jÃ¡ era
bom** ğŸ˜…

---




    </script>

    <script>
      const markdownContent =
        document.getElementById('markdown-source').textContent;
      const contentDiv = document.getElementById('content');
      contentDiv.innerHTML = marked.parse(markdownContent);
    </script>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>
      // Ativa o realce de sintaxe em todos os blocos de cÃ³digo
      hljs.highlightAll();
    </script>
  </body>
</html>
