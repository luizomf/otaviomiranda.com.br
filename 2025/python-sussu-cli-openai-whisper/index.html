<!doctype html>
<html lang="pt-BR">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />

    <title>
      Transcreva √°udio com Python: Sussu CLI + OpenAI Whisper - Ot√°vio Miranda
    </title>
    <meta
      name="description"
      content="Aprenda a usar o Sussu, uma ferramenta de linha de comando feita em Python que utiliza o modelo Whisper da OpenAI para transcrever √°udios e v√≠deos de forma simples e eficiente."
    />

    <link rel="stylesheet" href="../../css/markdown.css" />

    <link rel="icon" type="image/webp" href="../../imgs/favicon-1.webp" />

    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/github-markdown-css/5.5.1/github-markdown-light.min.css"
    />

    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css"
    />

    <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
  </head>
  <body>
    <nav class="breadcrumb-nav">
      <a href="/">&larr; ir para in√≠cio</a>
    </nav>

    <article class="markdown-body" id="content"></article>

    <script type="text/markdown" id="markdown-source">





# sussu(rro): CLI educacional com OpenAI Whisper

> Ferramenta de linha de comando focada em educa√ß√£o e IA offline. Usa o
> poder do Whisper da OpenAI pra transcrever √°udios e v√≠deos de forma
> simples.

Ao rodar este projeto, uma das primeiras coisas que voc√™ vai querer fazer
√© usar o comando `whisper` para fazer a transcri√ß√£o inicial de algum v√≠deo
ou √°udio. Essa transcri√ß√£o √© um √≥timo jeito de ver na pr√°tica como o
Whisper trabalha e o que esperar dos resultados.

- [Reposit√≥rio oficial do `whisper`](https://github.com/openai/whisper)

Por isso, vamos come√ßar pela **instala√ß√£o** do projeto, o que vai
disponibilizar os comandos `sussu` e `whisper` no terminal.

## Instala√ß√£o do `sussu`

Caso tenha dificuldades com o ambiente, recomendo meu tutorial:

- [Ambiente Python Moderno 2025: UV, Ruff, Pyright, pyproject.toml e VS Code](https://www.youtube.com/watch?v=HuAc85cLRx0)

Este projeto usa o `Python 3.11.9` por quest√µes de compatibilidade com o
Whisper. Evite alterar essa vers√£o caso n√£o saiba o que est√° fazendo,
porque eu **j√° testei tudo para voc√™**.

Al√©m disso, este projeto tamb√©m usa o [uv](https://docs.astral.sh/uv/) no
gerenciamento geral (pacotes, vers√£o do Python, etc).

```sh
uv sync  # √© s√≥ isso mesmo üòÖ
```

`uv sync` √© suficiente para:

- Baixar e instalar o `python 3.11.9`
- Criar o ambiente virtual em `.venv`
- Instalar os pacotes necess√°rios
- Buildar o `whisper` e o `sussu`

---

## `ffmpeg`

√â necess√°rio ter o [`ffmpeg`](https://ffmpeg.org/), que √© um pacote de
software open source que cont√©m uma cole√ß√£o de ferramentas e bibliotecas
para lidar com arquivos multim√≠dia, principalmente √°udio e v√≠deo. O
`whisper` trabalha com transcri√ß√£o de arquivos de √°udio, mas o `ffmpeg`
permite que voc√™ n√£o tenha que converter seus v√≠deos em √°udio para fazer a
transcri√ß√£o.

Para instalar o ffmpeg no seu sistema use um dos comandos abaixo. Isso
veio diretamente do
[reposit√≥rio oficial do `whisper`](https://github.com/openai/whisper):

```sh
# no Ubuntu ou Debian
sudo apt update && sudo apt install ffmpeg

# no Arch Linux
sudo pacman -S ffmpeg

# no MacOS com Homebrew (https://brew.sh/)
brew install ffmpeg

# no Windows com Chocolatey (https://chocolatey.org/)
choco install ffmpeg

# no Windows usando Scoop (https://scoop.sh/)
scoop install ffmpeg

# Adicional
# no Windows usando winget (https://winstall.app/apps/Gyan.FFmpeg)
winget install --id=Gyan.FFmpeg  -e
```

**Observa√ß√£o:** os √∫nicos comandos que testei da lista acima foram do
MacOS e Ubuntu. Aprovados ‚úÖ!

---

## Rodando pela primeira vez

Para testar se tudo funcionou perfeitamente voc√™ pode tanto **ativar o
ambiente virtual** quanto usar **`uv run`**. Teste com `whisper -h`. Isso
deve mostrar a `help` completa do `whisper`. Exemplos:

```sh
uv run whisper -h
# Ou se estiver com o ambiente virtual ativo
whisper -h
```

**Observa√ß√£o:** alguns editores como VS Code ou Zed, ativam seu ambiente
virtual automaticamente ao abrir uma nova inst√¢ncia do terminal se tudo
estiver configurado corretamente, basta sair (`exit`) e abrir novamente o
terminal.

---

## `whisper -h`: entendendo alguns argumentos importantes

Ao digitar `whisper -h` ou `whisper --help`, voc√™ pode se assustar com a
quantidade de argumentos que est√£o ali, dispon√≠veis para uso. Claro que
voc√™ n√£o precisa saber o que cada um deles faz, na verdade, a maioria dos
argumentos tem valores padr√£o que j√° funcionam perfeitamente. Mas, caso
queira personalizar um pouco o comportamente, vamos analisar alguns deles.

O `whisper` usa o `argparse` do Python para criar essa `help` maravilhosa.
Caso queira aprender mais sobre isso, assista meu v√≠deo:

- [Python e argparse: Do Zero a uma CLI Profissional (Projeto Real na Pr√°tica)](https://www.youtube.com/watch?v=Ad6934NXn4A)

---

### Argumentos essenciais do `whisper`

**`audio`:** este √© um argumento posicional que representa o caminho do
v√≠deo ou √°udio que ser√° transcrito.

Exemplo:

```sh
whisper /caminho/do/arquivo.mp4
```

N√£o especifiquei nada al√©m de um caminho de v√≠deo no comando acima, abaixo
detalho as op√ß√µes que mais uso.

---

**`--model MODEL`:** define qual o modelo ser√° utilizado na transcri√ß√£o do
√°udio. √â opcional, e o valor padr√£o √© `turbo`. Este model funciona muito
bem, √© r√°pido e multil√≠ngue, mas requer cerca de **6GB de VRAM** para
funcionar.

Talvez voc√™ queira usar outros modelos que usam mais ou menos recursos do
seu hardware, ou que possuem mais ou menos par√¢metros (como `base`,
`small`, `medium`, etc).

Abaixo os modelos dispon√≠veis:

- **`tiny`:** 39M, `tiny.en` e `tiny`, VRAM ~1 GB
- **`base`:** 74M, `base.en` e `base`, VRAM ~1 GB
- **`small`:** 244M, `small.en` e `small`, VRAM ~2 GB
- **`medium`:** 769M, `medium.en` e `medium`, VRAM ~5 GB
- **`large`:** 1550M, `large`, `large-v2` e `large-v3`, VRAM ~10 GB
- **`turbo`:** 809M, `turbo`, VRAM ~6 GB

**VRAM** √© um tipo especializado de mem√≥ria RAM usada pelas placas de
v√≠deo (GPUs). Se o seu computador compartilha a RAM com a GPU, como
acontece nos Macs com chip Apple Silicon (M1, M2, M3 e posteriores), voc√™
conseguir√° usar os modelos do Whisper **mesmo sem uma placa de v√≠deo
dedicada**.

Nesse caso, o fator limitador passa a ser a **quantidade total de mem√≥ria
dispon√≠vel no sistema**. Por exemplo: se voc√™ tem apenas 8GB de RAM, o
ideal √© testar os modelos `tiny`, `base` ou `small`.

A partir do modelo `medium`, √© bem prov√°vel que voc√™ perceba uma **queda
absurda no desempenho geral da m√°quina**, j√° que a mem√≥ria ser√°
completamente consumida.

---

**`--device DEVICE`:** se voc√™ tem uma placa de v√≠deo NVIDIA com driver
CUDA e vers√£o compat√≠vel com o PyTorch, vale a pena usar `--device cuda`,
do contr√°rio nem se preocupe em mexer com essa op√ß√£o. Padr√£o √© `cpu`.

---

**`--output_dir` ou `-o`:** o caminho da pasta onde as transcri√ß√µes ser√£o
salvas. Padr√£o √© na raiz do projeto (`.`).

**`--output_format` ou `-f`:** o formato da transcri√ß√£o (ou legenda) que
deseja. Op√ß√µes: `txt`, `vtt`, `srt`, `tsv`, `json` e `all` (todos). O
padr√£o √© `all` (todos).

---

**`--task`:** voc√™ pode transcrever ou traduzir um √°udio para ingl√™s. O
padr√£o √© transcrever no idioma que est√° sendo falado no √°udio. Op√ß√µes
`transcribe` (transcrever) ou `translate` (traduzir para ingl√™s).

---

**`--language`:** o idioma falado no √°udio. S√£o muitas op√ß√µes (veja
abaixo). O `whisper` √© capaz de detectar o idioma falado no v√≠deo se essa
op√ß√£o n√£o for enviada.

Forma curta (language code):

```python
["af", "am", "ar", "as", "az", "ba", "be", "bg", "bn", "bo", "br", "bs", "ca",
"cs", "cy", "da", "de", "el", "en", "es", "et", "eu", "fa", "fi", "fo", "fr",
"gl", "gu", "ha", "haw", "he", "hi", "hr", "ht", "hu", "hy", "id", "is", "it",
"ja", "jw", "ka", "kk", "km", "kn", "ko", "la", "lb", "ln", "lo", "lt", "lv",
"mg", "mi", "mk", "ml", "mn", "mr", "ms", "mt", "my", "ne", "nl", "nn", "no",
"oc", "pa", "pl", "ps", "pt", "ro", "ru", "sa", "sd", "si", "sk", "sl", "sn",
"so", "sq", "sr", "su", "sv", "sw", "ta", "te", "tg", "th", "tk", "tl", "tr",
"tt", "uk", "ur", "uz", "vi", "yi", "yo", "yue", "", "zh"]
```

Forma longa (language name):

```python
["Afrikaans", "Albanian", "Amharic", "Arabic", "Armenian", "Assamese",
"Azerbaijani", "Bashkir", "Basque", "Belarusian", "Bengali", "Bosnian",
"Breton", "Bulgarian", "Burmese", "Cantonese", "Castilian", "Catalan",
"Chinese", "Croatian", "Czech", "Danish", "Dutch", "English", "Estonian",
"Faroese", "Finnish", "Flemish", "French", "Galician", "Georgian", "German",
"Greek", "Gujarati", "Haitian", "Haitian Creole", "Hausa", "Hawaiian", "Hebrew",
"Hindi", "Hungarian", "Icelandic", "Indonesian", "Italian", "Japanese",
"Javanese", "Kannada", "Kazakh", "Khmer", "Korean", "Lao", "Latin", "Latvian",
"Letzeburgesch", "Lingala", "Lithuanian", "Luxembourgish", "Macedonian",
"Malagasy", "Malay", "Malayalam", "Maltese", "Mandarin", "Maori", "Marathi",
"Moldavian", "Moldovan", "Mongolian", "Myanmar", "Nepali", "Norwegian",
"Nynorsk", "Occitan", "Panjabi", "Pashto", "Persian", "Polish", "Portuguese",
"Punjabi", "Pushto", "Romanian", "Russian", "Sanskrit", "Serbian", "Shona",
"Sindhi", "Sinhala", "Sinhalese", "Slovak", "Slovenian", "Somali", "Spanish",
"Sundanese", "Swahili", "Swedish", "Tagalog", "Tajik", "Tamil", "Tatar",
"Telugu", "Thai", "Tibetan", "Turkish","Turkmen", "Ukrainian", "Urdu", "Uzbek",
"Valencian", "Vietnamese", "Welsh", "Yiddish", "Yoruba"]
```

J√° coloquei tudo com aspas e em uma lista para facilitar a sua vida. Mesmo
assim, se quiser um dicion√°rio pronto, est√° em
`whisper.tokenizer.LANGUAGES`.

---

**`--temperature`:** controla a "criatividade" do modelo. Vai de `0.0` a
`1.0`. Quanto mais alto, mais liberdade o modelo tem pra decidir os
pr√≥ximos tokens. Esse par√¢metro interage com `--beam_size`, `--patience` e
`--best_of`.

**`--beam_size`:** n√∫mero de hip√≥teses que o modelo mant√©m em paralelo.
Pensa como se ele testasse v√°rios caminhos ao mesmo tempo e no fim
escolhesse o melhor. O padr√£o √© `5` e **s√≥ funciona se
`--temperature == 0.0`**.

**`--patience`:** fator de toler√¢ncia que faz o modelo continuar
explorando novas hip√≥teses mesmo depois de achar uma aceit√°vel. Requer
`--temperature == 0.0` e `--beam_size > 1`.

**`--best_of`:** n√∫mero de amostras diferentes geradas antes de escolher a
melhor. Funciona apenas quando `--temperature > 0.0`.

**Cola r√°pida:**

```
- temperature > 0 ‚Üí usa sampling
  ‚úÖ best_of 5 (5 amostras)
  üî¥ beam_size (ignorado)
  üî¥ patience (ignorado)

- temperature == 0 ‚Üí usa beam search
  ‚úÖ --beam_size 5 (5 hip√≥teses)
  ‚úÖ --patience 2 (2 x 5 = 10 hip√≥teses)
  üî¥ best_of (ignorado)

- temperature == 0 ‚Üí greedy
  ‚úÖ --beam_size 1 (1 hip√≥tese)
  üî¥ --patience (n√£o faz diferen√ßa)
  üî¥ best_of (ignorado)
```

**Observa√ß√£o sincera:**

Na pr√°tica, o modelo vai responder como foi treinado, independente do seu
capricho nas configs. Trocar `temperature`, `beam_size`, `patience` e
afins pode virar desperd√≠cio de tempo.

**Recomenda√ß√£o direta:** s√≥ mexa nessas op√ß√µes se:

- o modelo come√ßar a repetir palavras (loop)
- estiver errando demais em blocos grandes

Se for s√≥ por causa de uma ou duas palavras... aceita e segue. Ou ent√£o
faz igual eu: **testa tudo por uma semana e conclui que o padr√£o j√° era
bom** üòÖ

---




    </script>

    <script>
      const markdownContent =
        document.getElementById('markdown-source').textContent;
      const contentDiv = document.getElementById('content');
      contentDiv.innerHTML = marked.parse(markdownContent);
    </script>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>
      // Ativa o realce de sintaxe em todos os blocos de c√≥digo
      hljs.highlightAll();
    </script>
  </body>
</html>
