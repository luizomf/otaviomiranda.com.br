---
title: 'NormalizaÃ§Ã£o Unicode em Python'
description:
  'AlÃ©m da normalizaÃ§Ã£o Unicode e as formas de normalizaÃ§Ã£o NFC, NFD, NFKC e
  NFKD, vocÃª vai aprender tudo o que precisa saber sobre o padrÃ£o Unicode em si
  e Python.'
date: 2020-08-20
author: 'Luiz OtÃ¡vio Miranda'
---

AlÃ©m da **normalizaÃ§Ã£o Unicode** e as formas de normalizaÃ§Ã£o NFC, NFD, NFKC e
NFKD, vocÃª vai aprender tudo o que precisa saber sobre o padrÃ£o Unicode em si e
Python.

Falando sobre a normalizaÃ§Ã£o Unicode em si, que provavelmente Ã© o que te trouxe
aqui: normalizar Ã© o ato de transformar strings
([textos no padrÃ£o unicode](https://pt.wikipedia.org/wiki/Unicode)) para uma
forma normal onde os caracteres sempre terÃ£o a mesma representaÃ§Ã£o binÃ¡ria em
todo o seu programa. Isso facilita a comparaÃ§Ã£o, indexaÃ§Ã£o e ordenaÃ§Ã£o de
strings jÃ¡ que, em um sistema â€œnormalizadoâ€, essas operaÃ§Ãµes sÃ£o Ã© mais
confiÃ¡veis.

Frequentemente vocÃª verÃ¡ emojis no meio do texto com um cÃ³digo na frete. Eu
espero que vocÃª entenda isso ao terminar sua leitura, porque eu nÃ£o costumo
escrever assim, ok? ğŸ¤ (U+1F910).

## Um contexto para iniciarmos

Vamos iniciar uma jornada longa neste momento. Portanto, vou te deixar um
contexto para discutirmos ao longo de todo o artigo. PorÃ©m, nÃ£o se preocupe se
nÃ£o entender nada agora. Prometo que vou explicar tudo o que vocÃª vai ver a
seguir ğŸ™ (U+1F64F).

### Porque precisamos de normalizaÃ§Ã£o?

No padrÃ£o Unicode, caracteres sÃ£o representados por **code points** (cÃ³digos de
identidade do caractere). Mas, alguns desses caracteres sÃ£o representados mais
de uma vez para que o padrÃ£o Unicode mantenha compatibilidade com outros padrÃµes
que vieram antes dele.

Por exemplo, a letra â€œ`Ã¡`â€ (a com acento agudo), representada por â€œ`U+00E1`â€ em
[code point](https://en.wikipedia.org/wiki/Code_point) Unicode, tambÃ©m pode ser
representada por `"U+0061"` (a) + â€œ`U+0301`â€ (acento agudo). Na segunda
representaÃ§Ã£o, o acento Ã© algo que Ã© chamado de
[combining character](https://en.wikipedia.org/wiki/Combining_character), porque
combinado ao â€œ**a**â€œ, forma â€œ**Ã¡**â€œ. No entanto, â€œ`U+00E1`â€ (**Ã¡**) nÃ£o Ã© igual
a â€œ`U+0061`â€ + â€œ`U+0301`â€ (**Ã¡**) do ponto de vista do seu programa em Python,
mesmo que visualmente o caractere final seja exatamente o mesmo (**Ã¡**).

```python
>>> '\u00e1'
'Ã¡'
>>> '\u0061\u0301'
'aÌ'
>>> '\u00e1' == '\u0061\u0301'
False
```

A **normalizaÃ§Ã£o unicode** vai resolver este problema mantendo apenas uma forma
normal dos â€œ**Ã¡**sâ€ apresentados acima. Ou â€œ`U+00E1`â€ ou â€œ`U+0061`â€ +
â€œ`U+0301`â€œ. No entanto, para entender porque precisamos de normalizaÃ§Ã£o unicode
em nosso sistema, precisamos entender o PadrÃ£o Unicode como um todo.

## Unicode â€“ o bÃ¡sico do bÃ¡sico

O Unicode Ã© um padrÃ£o que permite aos computadores representar e manipular texto
de qualquer sistema de escrita existente utilizando cÃ³digos para caracteres
individuais. Cada caractere Ã© mapeado para um cÃ³digo especÃ­fico chamado de
â€œ_code point_â€œ.

Code Points sÃ£o representados por um **U+** seguido de **4 a 6 dÃ­gitos
hexadecimais** (de 0 a 0x10FFFF). Por exemplo: o code point **U+0041**
representa a letra â€œ**A**â€œ; o **U+0042**, a letra â€œ**B**â€œ, o **U+1F40D**, uma
cobra verde â€œğŸâ€, e assim por diante.

Para que um sistema possa representar um **code point** como um caractere
â€œnormalâ€, ele precisa de um sistema de
[codificaÃ§Ã£o de caracteres](https://pt.wikipedia.org/wiki/Codifica%C3%A7%C3%A3o_de_caracteres).
Este sistema de codificaÃ§Ã£o tambÃ©m Ã© provido pelo padrÃ£o Unicode e Ã© responsÃ¡vel
por representar uma sequÃªncia de **code points** (qualquer string no padrÃ£o
Unicode) como um conjunto de **code units** na memÃ³ria do computador, que entÃ£o
sÃ£o mapeados para bytes de 8-bits.

Apesar do padrÃ£o Unicode disponibilizar um conjunto razoavelmente grande de
sistemas de codificaÃ§Ã£o de caracteres, como UTF-7, UTF-8, UTF-EBCDIC, UTF-16 e
UTF32, a codificaÃ§Ã£o mais usada atualmente Ã© a **UTF-8** (UTF sendo
[Unicode Transformation Format](https://pt.wikipedia.org/wiki/UTF-8) e 8 sendo o
nÃºmero de bits por cÃ³digo). No momento da escrita deste post, o site
[W3Techs â€“ Historical trends in the usage statistics of character encodings for websites](https://w3techs.com/technologies/history_overview/character_encoding),
mostra o padrÃ£o UTF-8 sendo usado em **94.7%** dos sites analisados atÃ©
15/04/2020 ğŸ‘€ (U+1F440).

Ã‰ uma boa ideia manter seu editor de cÃ³digos ou IDE no padrÃ£o **UTF-8** para
digitar seus cÃ³digos em Python ğŸ¤·â€â™‚ï¸ (U+1F937).

## Python e Unicode

**Dica ğŸ’¡** (U+1F4A1)**:** Boa parte do trecho a seguir foi baseada na
[documentaÃ§Ã£o oficial do Python](https://docs.python.org/3.9/howto/unicode.html).

As strings (`str`) em Python contÃ©m caracteres Unicode desde a versÃ£o 3.0. Isso
quer dizer que qualquer valor entre aspas simples, duplas ou triplas sÃ£o salvas
em Unicode. De fato, o Python ğŸ suporta atÃ© mesmo identificadores com
caracteres Unicode.

```python
>>> atenÃ§Ã£o = 'Um teste unicode'
>>> atenÃ§Ã£o
'Um teste unicode'
>>>
```

### Usando caracteres unicode

TambÃ©m existem vÃ¡rias maneiras para usar caracteres Unicode dentro do cÃ³digo ğŸ’»
(U+1F4BB).

Por exemplo, vocÃª pode usar o caractere literal (como de costume), mas tambÃ©m
pode usar o nome do caractere Unicode, um hexadecimal ou atÃ© um nÃºmero decimal
que representaria o caractere usando funÃ§Ã£o
[chr](https://docs.python.org/3/library/functions.html#chr):

```python
>>> 'A' # Caractere literal A
'A'
>>> chr(0x41) # Usando a funÃ§Ã£o chr com hexadecimal
'A'
>>> chr(65) # Usando a funÃ§Ã£o chr com decimal
'A'
>>> '\N{Latin Capital Letter A}' # Usando o nome do caractere
'A'
>>> '\u0041' # Usando um hexadecimal 16-bit
'A'
>>> '\U00000041' # Usando um hexadecimal 32-bit
'A'
```

Veja acima, que representei a letra â€œAâ€ de vÃ¡rias maneiras diferentes.

### Obtendo valores Unicode dos caracteres

AlÃ©m do que descrevi anteriormente, vocÃª tambÃ©m pode fazer o inverso, ou seja,
pegar os valores decimal e hexadecimal que representam o caractere desejado.

Para isso, vocÃª pode usar as funÃ§Ãµes
[ord](https://docs.python.org/3/library/functions.html#ord) e
[hex](https://docs.python.org/3/library/functions.html#hex), dependendo do que
deseja (talvez seja necessÃ¡rio combinÃ¡-las).

Por exemplo:

```python
>>> ord('A') # ObtÃ©m o valor decimal que representa A
65
>>> hex(ord('A')) # ObtÃ©m o valor hexadecimal que representa A
'0x41'
```

### Encode (str) e Decode (bytes)

Ã‰ possÃ­vel converter uma string em bytes ou bytes em string usando os mÃ©todos
`encode` da [string](https://docs.python.org/pt-br/3/library/stdtypes.html#str)
ou `decode` de
[bytes](https://docs.python.org/pt-br/3/library/stdtypes.html#bytes). Esses
mÃ©todos recebem dois argumentos. O primeiro argumento especifica a codificaÃ§Ã£o
de caracteres desejada (`utf-8` ou qualquer outra disponÃ­vel em
[Standard Encodings](https://docs.python.org/3/library/codecs.html#standard-encodings)
â€“ use `utf-8` sempre que possÃ­vel ğŸ•µ). O segundo informa como os erros devem ser
tratados (falaremos desse argumento mais adiante neste post).

PorÃ©m, Ã© importante tomar cuidado ao converter uma codificaÃ§Ã£o de caracteres
para outra (exemplo, de **ASCII** para **UTF-8**). Pode nÃ£o ser possÃ­vel mapear
o cÃ³digo de um caractere para outro em determinadas circunstÃ¢ncias.

Veja exemplos a seguir.

#### Encode (str)

Suponha que eu queira converter uma string **UTF-8** para bytes **UTF-8**.

Eu posso fazer isso da seguinte maneira:

```python
>>> meu_nome = 'OtÃ¡vio'
>>> meu_nome_em_bytes = meu_nome.encode('utf-8')
>>> meu_nome_em_bytes
b'Ot\xc3\xa1vio'
>>>
```

Bytes sÃ£o representados por `b'valores'` em Python.

De acordo com o cÃ³digo anterior, tudo ocorreu perfeitamente. Isso porque
converti uma string sabendo que ela tinha caracteres **UTF-8** para bytes em
**UTF-8**. Mantendo a mesma codificaÃ§Ã£o de caractere, nÃ£o terei problemas.

Mas, eu tambÃ©m poderia querer converter minha string **UTF-8** para
[ASCII](https://pt.wikipedia.org/wiki/ASCII) (um outro tipo de codificaÃ§Ã£o de
caracteres). Dependendo do que vocÃª estiver convertendo, nÃ£o terÃ¡ problemas,
porque o UTF-8 foi feito para ser compatÃ­vel com outras codificaÃ§Ãµes de
caracteres existentes. PorÃ©m, assim que um caractere sair do range suportado
pelo ASCII (de 0 a 127 em base 10), terei um erro:

```python
>>> meu_nome_em_bytes = meu_nome.encode('ascii')
Traceback (most recent call last):
...
UnicodeEncodeError: 'ascii' codec can't encode character '\xe1' in position 2: ordinal not in range(128)
```

Veja que no erro Ã© descrito o problema. NÃ£o foi possÃ­vel codificar o caractere
`'\xe1'` na posiÃ§Ã£o 2.

Lembra que te mostrei como exibir o caractere utilizando a funÃ§Ã£o `chr`? EntÃ£o,
o caractere `\xe1` Ã© o mesmo que `chr(0xe1)`, ou â€œ**Ã¡**â€ de â€œ**OtÃ¡vio**â€œ. Esse
caractere nÃ£o faz parte da tabela **ascii**, portanto o erro.

Logo mais veremos o segundo argumento e vocÃª poderÃ¡ selecionar o que acontece
quando um erro assim ocorrer.

#### Decode (bytes)

Se o mÃ©todo `encode` Ã© usado para converter string em bytes, o mÃ©todo `decode` Ã©
usado para fazer o inverso disso, converter bytes em strings.

Por exemplo, suponha que eu tenha apenas os bytes e queira resgatar seu valor
para uma string **UTF-8**.

```python
>>> meu_nome_em_bytes = b'Ot\xc3\xa1vio'
>>> meu_nome_str = meu_nome_em_bytes.decode('utf-8')
>>> meu_nome_str
'OtÃ¡vio'
```

Novamente, aqui ocorreu tudo perfeitamente, porque eu sabia que a codificaÃ§Ã£o
dos bytes era **UTF-8** e eu os decodifiquei adequadamente. EntÃ£o tudo ocorreu
como esperado. Mas, e se eu quisesse decodificar esses bytes em **ASCII**?

Inicialmente, nÃ£o tem como (ğŸ˜’ â€“ U+1F612)!

Para isso vocÃª precisa saber a codificaÃ§Ã£o de caracteres usada na codificaÃ§Ã£o
para decodificar.

**Dica:** [chardet](https://pypi.org/project/chardet/) ajuda a tentar descobrir
a codificaÃ§Ã£o de caracteres usada em bytes que vocÃª nÃ£o saberia de outra forma.
Mas a maneira mais simples continua sendo: â€œpergunte ao dono dos bytes qual a
codificaÃ§Ã£oâ€. **UTF-8** Ã© um bom chute inicial.

#### Erros de codificaÃ§Ã£o em decode (bytes)

Imagine que eu nÃ£o saiba a codificaÃ§Ã£o usada em determinados bytes e tente
chutar **ascii**, por exemplo:

```python
>>> meu_nome_str = meu_nome_em_bytes.decode('ascii') # OtÃ¡vio (em UTF-8)
Traceback (most recent call last):
...
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 2: ordinal not in range(128)
```

Perceba que aqui, alÃ©m de um erro de `UnicodeDecodeError`, eu tambÃ©m tenho um
**code point** incorreto. Se vocÃª observar, `0xc3` aponta para â€œ**Ãƒ**â€ , que nem
existia na minha string anterior.

O motivo disso Ã© simples, â€œ**Ã¡**â€ da minha string anterior usa dois bytes em
**UTF-8** e o codec **ASCII** nÃ£o sabe disso. EntÃ£o ele tenta decodificar byte
por byte e gera esse erro estranho. Se isso passasse sem erros, o resultado
seria um monte de caracteres que nÃ£o fariam sentido algum. Por exemplo, se eu
usasse o codec [latin1](https://pt.wikipedia.org/wiki/ISO/IEC_8859-1) ao invÃ©s
de **ascii**, o resultado seria **â€˜OtÃƒÂ¡vioâ€™**.

Para contornar essa situaÃ§Ã£o, eu preciso saber qual a codificaÃ§Ã£o de caracteres
foi usada para codificar os bytes anteriormente. Sabendo disso, eu deveria
decodificar esses bytes usando a codificaÃ§Ã£o correta antes de fazer qualquer
outra coisa.

Depois de decodificar, eu poderia codificar novamente usando o **codec** que eu
preferir, contando que ele suporte os caracteres que eu estiver usando (usa
sempre **UTF-8**, pelo amor de Deus ğŸ˜¬ â€“ U+1F62C).

Por exemplo, se eu quero codificar de UTF-8 para
[ISO-8859-1 (Latin1)](https://pt.wikipedia.org/wiki/ISO/IEC_8859-1), que Ã© algo
que vejo muito aqui no Brasil, principalmente em sistemas pÃºblicos, poderia
fazer assim:

```python
>>> meu_nome_str = meu_nome_em_bytes.decode('utf8') # OtÃ¡vio
>>> meu_nome_em_bytes_latin = meu_nome_str.encode('latin_1') # OtÃ¡vio
>>> meu_nome_em_bytes_latin
b'Ot\xe1vio' # OtÃ¡vio
```

Basicamente, isso foi uma conversÃ£o de **UTF-8** para **latin_1**. Tenha noÃ§Ã£o
de que sempre que essas conversÃµes ocorrem, uma codificaÃ§Ã£o de caracteres deve
suportar a outra. O Unicode foi criado para ser compatÃ­vel com todas as
codificaÃ§Ãµes existentes. No entanto, apenas no sentido de â€œqualquer codificaÃ§Ã£oâ€
convertido para â€œUnicodeâ€. VocÃª pode ter problemas ao converter no sentido
contrÃ¡rio, de Unicode para â€œqualquer codificaÃ§Ã£oâ€, porque o padrÃ£o Unicode
suporta muito mais caracteres do que qualquer outra codificaÃ§Ã£o de caracteres
que vocÃª quiser utilizar.

No nosso exemplo, tudo funcionou perfeitamente porque todas as letras de
â€œ**OtÃ¡vio**â€ estÃ£o presentes na tabela
[ISO-8859-1 (Latin1)](https://pt.wikipedia.org/wiki/ISO/IEC_8859-1), caso
contrÃ¡rio ocorreriam erros tambÃ©m.

#### Dicas

**Dica nÃºmero 1:** Sempre que possÃ­vel use a codificaÃ§Ã£o de caracteres
**UTF-8**, na grande maioria das vezes isso Ã© possÃ­vel ğŸ˜… (U+1F605).

**Mais dicas:** se vocÃª precisa detectar a codificaÃ§Ã£o de caracteres de algo que
nÃ£o tem a mÃ­nima ideia como foi codificado, use
[chardet.detect](https://pypi.org/project/chardet/). Ele nÃ£o vai acertar em 100%
dos casos, mas jÃ¡ me salvou de muitas enrascadas; Se vocÃª precisa saber quais
codecs de codificaÃ§Ã£o o Python suporta, veja
[Python Specific Encodings](https://docs.python.org/3.9/library/codecs.html#standard-encodings).

#### Erros em encode e decode

Como te contei anteriormente, `encode` e `decode` recebem um argumento com a
codificaÃ§Ã£o desejada e outro especificando como os erros devem ser tratados.
Para o segundo argumento vocÃª pode enviar os seguintes valores:

- `'strict'` â€“ Ã‰ o padrÃ£o. O que levanta uma exceÃ§Ã£o de
  [UnicodeEncodeError](https://docs.python.org/3/library/exceptions.html#UnicodeEncodeError)
  ou
  [UnicodeDecodeError](https://docs.python.org/3/library/exceptions.html#UnicodeDecodeError);
- `'replace'` â€“ Usa o caractere U+FFFD (REPLACEMENT CHARACTER) no lugar do
  caractere que nÃ£o pÃ´de ser convertido;
- `'ignore'` â€“ Simplesmente pula o caractere que nÃ£o pode ser convertido;
- `'backslashreplace'` â€“ que insere uma sequÃªncia `\xNN` no lugar do caractere
  que nÃ£o pode ser convertido;
- `'xmlcharrefreplace'` â€“ que insere uma referÃªncia para um caractere
  [XML](https://en.wikipedia.org/wiki/List_of_XML_and_HTML_character_entity_references)
  (isso sÃ³ funciona com `encode`).

```python
>>> 'OtÃ¡vio'.encode('utf-8')
b'Ot\xc3\xa1vio'
>>> b'Ot\xc3\xa1vio'.decode('ascii', 'ignore') # aqui usei ignore
'Otvio' # e perdi o "Ã¡"
```

## NormalizaÃ§Ã£o Unicode em Python

NÃ³s demos vÃ¡rias voltas atÃ© chegar aqui, mas Ã© importante conhecer o que vocÃª
estÃ¡ fazendo, nÃ£o Ã© mesmo (ğŸ˜ â€“ U+1F60F)?

EntÃ£o, sÃ³ pra recapitular tudo:

- VocÃª conheceu os code points do Unicode;
- TambÃ©m sabe que Unicode foi feito pensando em compatibilidade com padrÃµes jÃ¡
  existente (ascii, latin, etc). Vamos voltar nesse assunto jÃ¡ jÃ¡;
- Viu que UTF-8 Ã© uma das codificaÃ§Ãµes de caracteres do Unicode;
- EstÃ¡ ciente que UTF-8 Ã©, de longe, uma das codificaÃ§Ãµes mais usadas no mundo;
- E deveria estar usando UTF-8 nos seus cÃ³digo (Ã© muito provÃ¡vel que jÃ¡ esteja).

Uma coisa que eu ainda nÃ£o te falei Ã© sobre a normalizaÃ§Ã£o e o porquÃª isso
existe.

Na verdade, todas as voltas foram para fazer vocÃª entender o que Ã© Unicode de
verdade. Se jÃ¡ sabia, melhor ainda!

EntÃ£o agora podemos ter uma conversa mais â€œcomplexaâ€.

### Unicode e outros padrÃµes

Lembra que lÃ¡ no comecinho â˜ (U+261D) te falei que eu poderia escrever a letra
â€œ**Ã¡**â€ de maneiras diferentes em Unicode?

SÃ³ pra te lembrar:

```python
>>> '\u00e1'
'Ã¡'
>>> '\u0061\u0301'
'aÌ'
```

Este pode nÃ£o ser um problema no seu programa caso nÃ£o tenha tido a necessidade
de comparar esses dois â€œ**Ã¡**sâ€. No entanto, em algum momento este problema pode
aparecer e vocÃª vai demorar um tempo considerÃ¡vel atÃ© descobrir que isso estÃ¡
relacionado com a falta de normalizaÃ§Ã£o de caracteres. NÃ³s buscamos recursos de
vÃ¡rias fontes externas ao nosso programa e nÃ£o sabemos qual forma normal
utilizaram no sistema deles, e essa bomba pode explodir na sua mÃ£o.

O Unicode foi criado pensando em compatibilidade, por isso alguns caracteres
aparecem mais de uma vez. Por exemplo, se vocÃª olhar na tabela
[ASCII](https://pt.wikipedia.org/wiki/ASCII), vai ver que a letra â€œ**A**â€ Ã©
representada pelo mesmo hexadecimal que o Unicode (**41** vs **U+0041**). Se
olhar na tabela [ISO/IEC 8859-1](https://pt.wikipedia.org/wiki/ISO/IEC_8859-1),
vai ver que a letra â€œ**Ã¡**â€ Ã© representada exatamente pelo mesmo hexadecimal que
o Unicode (**00E1** vs **U+00E1**). Isso quer dizer que o range de 0 a 127
(base 10) no Unicode Ã© compatÃ­vel com **ASCII**, o range de 0 a 255 (base 10) no
Unicode Ã© compatÃ­vel com **ISO/IEC 8859-1** (ou latin1) e assim por diante. O
Unicode tenta ser compatÃ­vel com todos os padrÃµes existentes.

Isso vai acabar nos levando a um problema, vai vendo ğŸ¤¨ (U+1F928)!

### Caracteres prÃ©-compostos e caracteres combinados

Pelo motivo que te expliquei anteriormente, existem caracteres que sÃ£o chamados
de **prÃ©-compostos**, como: **Ã¡**, **Ã©**, **Ã€**, **Ã** e vÃ¡rios outros. Esses
caracteres prÃ©-compostos existem para manter compatibilidade com padrÃµes que jÃ¡
existiam antes do Unicode.

Por outro lado, o Unicode tambÃ©m dispÃµe de um sistema de combinaÃ§Ã£o para
estender o repositÃ³rio de caractere suportados, e isso Ã© genial (ğŸ¥° â€“ U+1F970)!

Pensa comigo ğŸ¤“ (U+1F913), se eu posso ter um â€œ**a**â€ e um â€œ**acento agudo**â€ em
dois caracteres diferentes, nÃ£o seria inteligente permitir que o **acento
agudo** pudesse ser combinado com esse â€œ**a**â€ ou com qualquer outro caractere
formando um caractere Ãºnico? TambÃ©m acho!

Ã‰ exatamente esse o mecanismo que foi usado no Unicode. Ao invÃ©s de ter um
**code point** Ãºnico para cada caractere do planeta, fizeram um sistema de
combinaÃ§Ã£o de caracteres para formar esses sÃ­mbolos loucos que a gente acaba
usando e nem percebe.

Esses caracteres que podem ser combinados com outros caracteres sÃ£o chamados de
[combining character](https://en.wikipedia.org/wiki/Combining_character) e
existem muitos deles.

Mas, como nem tudo sÃ£o flores (ğŸ¥€ â€“ U+1F940), isso gerou o problema de ter mais
de um caractere representando a mesma coisa. Aquela probleminha que te mostrei
no inÃ­cio, sobre os â€œ**Ã¡**sâ€. Te falei que ia dar problema, nÃ£o falei ğŸ˜
(U+1F601)?

Ã‰ aqui que entra a normalizaÃ§Ã£o e uma outra coisa que Ã© chamada de
**equivalÃªncia canÃ´nica**.

### EquivalÃªncia canÃ´nica

Como os criadores do Unicode sÃ£o bem inteligentes ğŸ§ (U+1F9D0), eles criaram
algo chamado de â€œ**equivalÃªncia canÃ´nica**â€œ. Isso Ã© sÃ³ uma maneira bonita de
falar â€œesses dois caracteres sÃ£o iguaisâ€. EntÃ£o, na equivalÃªncia canÃ´nica,
**U+00E1** (**Ã¡** prÃ©-composto) Ã© igual a **U+0041 + U+0301** (**a** com
**acento agudo** combinados). Isso acontece com todos os caracteres acentuados e
mais outros milhares de caracteres que podem ser combinados em vÃ¡rios idiomas
diferentes.

Sabendo disso, vocÃª poder utilizar mais de uma forma normal em todo o seu
programa: **NFC** e **NFD** (tem mais duas, mas Ã© questÃ£o de compatibilidade,
segura aÃ­ que a gente jÃ¡ fala sobre isso).

### NFC â€“ Normalization Form Canonical Composition

Esse tipo de normalizaÃ§Ã£o Unicode visa **manter os caracteres prÃ©-compostos** no
seu programa (sem a separaÃ§Ã£o de caractere + caractere combinado). Tais
caracteres sÃ£o unidos por equivalÃªncia canÃ´nica.

Lembra dos **Ã¡**s? Aqui eles serÃ£o iguais, porque somente o **U+00E1** (**Ã¡**
prÃ©-composto) serÃ¡ mantido, os caracteres separados serÃ£o convertidos em
prÃ©-compostos. Por exemplo, **U+0061 + U+0301** (**a** com **acento agudo**
combinados) se tornaria sempre **U+00E1** (**Ã¡** prÃ©-composto).

Por exemplo:

```python
>>> import unicodedata
>>> nome = 'Ot\u0061\u0301vio'
>>> nome_normalizado = unicodedata.normalize('NFC', nome)
>>> ['U+' + hex(ord(letra))[2:].zfill(4).upper() for letra in nome_normalizado]
['U+004F', 'U+0074', 'U+00E1', 'U+0076', 'U+0069', 'U+006F']
# O         t         Ã¡         v	      i         o
```

Da pra perceber ali que a letra â€œ**Ã¡**â€ do meu nome, sempre serÃ¡ mantida como
**U+00E1** com esse tipo de normalizaÃ§Ã£o. Mesmo eu dizendo explicitamente que
quero a string `'Ot\u0061\u0301vio'`.

Resumidamente: isso nÃ£o farÃ¡ nada com caracteres prÃ©-compostos, mas combinarÃ¡
caracteres equivalentes que estiverem separados em sua forma prÃ©-composta por
equivalÃªncia canÃ´nica.

### NFD â€“ Normalization Form Canonical Decomposition

Esse tipo de normalizaÃ§Ã£o unicode visa manter os caracteres separados (com a
separaÃ§Ã£o entre caractere e caractere combinado). Os caracteres serÃ£o separados
por equivalÃªncia canÃ´nica.

Aqui os â€œ**Ã¡**sâ€ serÃ£o iguais, porque somente os caracteres **U+0061 + U+0301**
(**a** com **acento agudo** combinados) serÃ£o mantidos. Os â€œ**Ã¡**sâ€
prÃ©-compostos (**U+00E1**) serÃ£o convertidos em **U+0061 + U+0301**.

Por exemplo:

```python
>>> import unicodedata
>>> nome = 'Ot\u00e1vio'
>>> nome_normalizado = unicodedata.normalize('NFD', nome)
>>> ['U+' + hex(ord(letra))[2:].zfill(4).upper() for letra in nome_normalizado]
['U+004F', 'U+0074', 'U+0061', 'U+0301', 'U+0076', 'U+0069', 'U+006F']
# O         t         a         acento    v         i         o
```

Perceba que agora eu consegui manter ambos os caracteres, tanto o â€œ**a**â€ quanto
o â€œ**acento agudo combinado**â€œ. Mesmo especificando que eu queria a string
`'Ot\u00e1vio'`.

Resumidamente: isso nÃ£o farÃ¡ nada com aqueles dois caracteres combinados, porÃ©m
vai separar caracteres prÃ©-compostos para sua forma combinada por equivalÃªncia
canÃ´nica. Basicamente Ã© **U+00E1** (**Ã¡** prÃ©-composto) se transformando em
**U+0061 + U+0301** (**a** com **acento agudo** combinados).

## NFKC e NFKD â€“ Normalization Form Compatibility Composition/Decomposition

Para complicar um pouquinho mais a sua vida na normalizaÃ§Ã£o unicode, tambÃ©m
existem caracteres que **nÃ£o** sÃ£o definidos por **equivalÃªncia canÃ´nica**, mas
por **compatibilidade**.

Por exemplo, em alguns contextos, o sÃ­mbolo **TM** pode ter o mesmo significado
que â„¢ (TRADE MARK SIGN, U+2122). Nesse caso, ambos TM e â„¢ sÃ£o definidos como
caracteres compatÃ­veis, mas que NÃƒO TEM **equivalÃªncia canÃ´nica**.

Isso quer dizer que nem **NFC**, nem **NFD** vÃ£o normalizar esses dois valores.

E sÃ³ pra deixar claro isso pra vocÃª, caso ainda nÃ£o tenha ficado:

- NF â€“ Normalization Form (formato de normalizaÃ§Ã£o);
- C â€“ Composition (composiÃ§Ã£o â€“ une);
- D â€“ Decomposition (decomposiÃ§Ã£o â€“ separa);
- K â€“ Compatibility (separa por compatibilidade).

Agora que vem a pergunta de 1 milhÃ£o de dÃ³lares: qual a forma normal entre TM e
â„¢? Depende! Em qual contexto?

Vou te dar um exemplo: nÃ³s sabemos que seres humanos tem uma preguiÃ§a danada de
digitar as coisas corretamente, certo? Imagine que a minha marca fosse **OMâ„¢** e
eu quisesse que no meu sistema de busca, essa marca fosse encontrada. VocÃª acha
que as pessoas digitariam **OMTM** ou **OMâ„¢**? Eu acho que OMTM (caso nÃ£o
encontrassem antes apenas digitando OM). Mas podemos garantir as duas com a
normalizaÃ§Ã£o.

EntÃ£o nesse caso, eu usaria a compatibilidade para transformar **â„¢** em TM
apenas para realizar uma comparaÃ§Ã£o. Por exemplo, meu texto na base de dados
seria normalizado temporariamente com NFKD e o texto enviado pelo usuÃ¡rio tambÃ©m
seria normalizado para NFKD. Assim eu consigo encontrar **OMTM** ou **OMâ„¢**
independente de como isso foi digitado pelo usuÃ¡rio.

Para fazer a normalizaÃ§Ã£o unicode de **â„¢** para TM, vocÃª vai precisar usar
NF**K**(C ou D):

```python
>>> import unicodedata
>>> nome = 'OMâ„¢'
>>> nome_normalizado = unicodedata.normalize('NFKC', nome)
>>> ['U+' + hex(ord(letra))[2:].zfill(4).upper() for letra in nome_normalizado]
['U+004F', 'U+004D', 'U+0054', 'U+004D']
# O         M         T         M
```

Perceba que â€œ**C**â€ e â€œ**D**â€ aqui vÃ£o fazer o mesmo trabalho descrito
anteriormente, mas o â€œ**K**â€ vai trabalhar na compatibilidade que te falei
antes.

EntÃ£o sÃ³ pra resumir: O **K** significa **compatibility** e vai converter
valores que estariam em apenas um **code point** Unicode em caracteres que
seriam compatÃ­veis (de acordo com as regras deles, que eu nÃ£o sei quais sÃ£o).
Esses caracteres devem se comportar da mesma maneira em pesquisa, comparaÃ§Ã£o,
ordenaÃ§Ã£o e indexaÃ§Ã£o, mas podem mudar o significado e tambÃ©m podem parecer
visualmente diferentes em vÃ¡rios contextos. Como no nosso exemplo, **OMTM** ou
**OMâ„¢** deveria retornar os mesmos valores no meu sistema de pesquisa ou
comparaÃ§Ã£o, mas eles sÃ£o bem diferentes visualmente.

### Um ponto de atenÃ§Ã£o para K

Tenha em mente que a partir do momento que eu separei os valores por
compatibilidade, nÃ£o consigo mais uni-los novamente. Por exemplo, se eu
normalizar com **K** (NFKC ou NFKD) o valor â„¢ (TRADE MARK SIGN, U+2122), vou
obter TM (como vimos). PorÃ©m TM nÃ£o voltarÃ¡ a ser â„¢.

Por este motivo, Ã© super importante que vocÃª **nÃ£o** salve os valores
permanentemente utilizando **K**. VocÃª deve normalizar temporariamente no
momento que precisar realizar alguma comparaÃ§Ã£o e eliminar esse valor apÃ³s
terminar o que estava fazendo. Salve os valores como eles realmente sÃ£o.

## Usando chardet para detectar a codificaÃ§Ã£o de caracteres

Na grande maioria das vezes que nosso sistema gerar algum problema de
codificaÃ§Ã£o de caracteres, esse problema virÃ¡ de algum recurso externo.
Portanto, para simular isso, suponha que eu tenha um arquivo em **ISO-8859-1**
(latin1). Qualquer editor de textos decente vai te permitir criar o mesmo texto
com a mesma codificaÃ§Ã£o de caracteres. Por exemplo, o
[Visual Studio Code](https://code.visualstudio.com/).

!\[Exemplo de codificaÃ§Ã£o no VS Code\](./imgs/python-1.png)

NÃ³s sabemos qual a codificaÃ§Ã£o de caracteres foi usada neste arquivo (latin1),
mas finja que nÃ£o. Vamos carregar esse arquivo pelo Python usando â€œ`rb`â€ (read
bytes) e solicitar ao [chardet](https://pypi.org/project/chardet/) para detectar
a codificaÃ§Ã£o de caracteres.

**Nota:** vocÃª precisa instalar o chardet com â€œ`pip install chardet`â€œ.

```python
>>> import chardet
>>> with open('text.txt', 'rb') as file:
...     raw_content = file.read()
...     encoding = chardet.detect(raw_content)
...     encoding["encoding"]
...
'ISO-8859-9'
```

Ele detectou como â€˜ISO-8859-9â€™, isso seria **latin5** e nÃ£o **latin1**, mas
lembra que te falei que ele nÃ£o iria acertar 100% das vezes, certo? Bom, vamos
tentar converter esse arquivo de ISO-8859-9 (mesmo nÃ£o sendo a codificaÃ§Ã£o exata
do arquivo) para UTF-8 e ver o que ocorre.

Vamos abrir o arquivo novamente, decodificar com a codificaÃ§Ã£o que o **chardet**
quiser (no caso ISO-8859-9, latin5), depois vamos abrir um novo arquivo com
`'wb'` e salvar como UTF-8. Veja:

```python
>>> with open('text.txt', 'rb') as file:
...     # Vamos ler apenas bytes do arquivo
...     raw_content = file.read()
...     # Agora a gente decodifica
...     content_string = raw_content.decode(encoding["encoding"])
>>>
# Perfeito, agora vamos tentar pegar o conteÃºdo
# da content_string e salvar em outro arquivo
# porÃ©m, agora vamos codificar em UTF-8
>>> with open('text2.txt', 'wb') as file:
...     file.write(content_string.encode('utf8'))
...
192
>>>
```

Perfeito, sem erros! Agora vamos ver como estÃ¡ o nosso arquivo â€œ**text2.txt**â€
(o novo arquivo gerado). SerÃ¡ que os caracteres se mantiveram?

!\[UTF-8\](./imgs/python-2.png)

Perfeito! Viu como a aproximaÃ§Ã£o que o **chardet** encontrou me ajudou muito?
Mesmo que ele nÃ£o tenha detectado com 100% de certeza qual a codificaÃ§Ã£o de
caracteres usada no arquivo, ele me passou uma que provavelmente iria funcionar.

Se vocÃª fosse tentar decodificar direto com UTF-8, isso ocorreria:

`UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe7 in position 4: invalid continuation byte`

E se ignorasse os erros, seu texto ficaria assim:

`Ateno Exceo Impresso Concesso Presuno`  
`Voc Pur Croch Metr`  
`Plstico Grfico Espcie Clebre`  
`quelas s`  
`Acar ACAR CABEA CAROO'`

Eu acho que deu pra vocÃª entender, nÃ£o Ã©?

## FunÃ§Ãµes interessantes com normalizaÃ§Ã£o unicode

VocÃª, como programador(a), jÃ¡ pode ter imaginado milhares de coisas
interessantes que pode fazer com a normalizaÃ§Ã£o unicode, nÃ£o Ã© mesmo? Se nÃ£o,
vou te dar algumas ideias:

### Obtendo code points Unicode

Suponha que eu queira obter uma lista com todos os code points de uma frase.
Veja que legal:

```python
from typing import List
from unicodedata import normalize


def get_unicode_code_points(string: str) -> List[str]:
    string_normalized = normalize('NFD', string)
    code_points: List[str] = [
        'U+' + hex(ord(letter))[2:].zfill(4).upper()
        for letter in string_normalized
    ]
    return code_points


if __name__ == "__main__":
    text = 'Python ğŸâ„¢'
    code_points = get_unicode_code_points(text)
    print(code_points)

    """
    ['U+0050', 'U+0079', 'U+0074', 'U+0068',
    'U+006F', 'U+006E', 'U+0020', 'U+1F40D',
    'U+2122']
    """
```

### Obtendo caracteres de code points

Mas, e o inverso? Dado um code point, como converto em caractere? VocÃª jÃ¡ viu
isso ao longo do texto todo, mas aqui vai.

```python
from typing import List


def get_char_from_code_point(code_points: List[str]) -> str:
    chars = [chr(int(c.replace('U+', '0x'), 16)) for c in code_points]
    return ''.join(chars)


if __name__ == "__main__":
    code_points = [
        'U+0050', 'U+0079', 'U+0074', 'U+0068',
        'U+006F', 'U+006E', 'U+0020', 'U+1F40D',
        'U+2122'
    ]

    print(get_char_from_code_point(code_points))
    # Python ğŸâ„¢
```

### Removendo caracteres fora da tabela ASCII

Em alguns casos, pode ser interessante manter apenas caracteres compatÃ­veis com
a tabela â€œASCIIâ€. AlÃ©m disso, nÃ³s tambÃ©m podemos converter caracteres que seriam
compatÃ­veis se nÃ£o fossem prÃ©-compostos. Por exemplo, â€˜**Ã¡**â€˜, â€˜**Ã£**â€˜, **Ã ** e
**Ã¢** se tornariam simplesmente â€˜**a**â€˜ e assim por diante para todos os
caracteres. PorÃ©m, caracteres como ğŸ e ğŸ˜€ nÃ£o estariam presentes, porque nÃ£o
existem na tabela ASCII e tambÃ©m nÃ£o existem compatÃ­veis.

Vamos ver como farÃ­amos isso:

```python
import unicodedata


def non_ascii_to_ascii(string: str) -> str:
    ascii_only = unicodedata.normalize('NFKD', string)\
        .encode('ascii', 'ignore')\
        .decode('ascii')
    return ascii_only


if __name__ == "__main__":
    string = 'AtenÃ§Ã£o ğŸ ğŸ˜€'
    print(non_ascii_to_ascii(string))  # Atencao
```

Perceba que caracteres como â€œ**Ã§**â€ e â€œ**Ã£**â€ de â€œ**AtenÃ§Ã£o**â€ foram mantidos
sem acento (porque existem na tabela ASCII), porÃ©m ğŸ e ğŸ˜€ foram ignorados.

### Removendo acentos das palavras

Na funÃ§Ã£o anterior, a gente meio que removeu os acentos, porÃ©m tambÃ©m removemos
outras coisas que nÃ£o querÃ­amos. Mas, suponha que eu queira remover apenas o
**combining character** mantendo o restante (o combining character seria o
acento propriamente dito).

Isso me retornaria palavras sem acento.

Eu posso fazer isso, como o
[Luciano Ramalho](https://twitter.com/ramalhoorg?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor)
descreve em seu livro [Python Fluente](https://amzn.to/34HdIPs).

```python
import unicodedata

def remove_accents(string: str) -> str:
    normalized = unicodedata.normalize('NFKD', string)
    return ''.join([c for c in normalized if not unicodedata.combining(c)])
```

PorÃ©m, eu tambÃ©m posso fazer isso com expressÃµes regulares. O que funcionar
melhor pra vocÃª:

```python
import unicodedata
import re


def remove_accents_regex(string: str) -> str:
    regex = re.compile(r'[\u0300-\u036F]', flags=re.DOTALL)
    normalized = unicodedata.normalize('NFKD', string)
    return regex.sub('', normalized)


if __name__ == "__main__":
    string = 'AtenÃ§Ã£o ğŸ ğŸ˜€'
    print(remove_accents_regex(string))  # Atencao ğŸ ğŸ˜€
```

Por falar nisso, eu tenho um detalhe sobre expressÃµes regulares pra te informar:
eu tenho um curso inteiro e gratuito na
[Udemy](https://www.udemy.com/course/expressoes-regulares-com-python-3-curso-gratuito/)
e no
[Youtube](https://www.youtube.com/watch?v=wBI0yv2FG6U&list=PLbIBj8vQhvm1VnTa2Np5vDzCxVtyaYLMr)
sobre isso.

Portanto, nÃ£o hÃ¡ motivos para entrarmos em detalhes sobre
[regex](https://pt.wikipedia.org/wiki/Express%C3%A3o_regular) aqui.

## Mais sobre Unicode e NormalizaÃ§Ã£o Unicode

Eu sei que esse assunto talvez passe despercebido para vÃ¡rios desenvolvedores e
desenvolvedoras mundo a fora e nÃ£o Ã© culpa deles (ou nossa, tambÃ©m passei por
isso). Em nosso meio, a maioria dos cursos, faculdades e livros que vocÃª lÃª para
aprender a programar, infelizmente nÃ£o tratam desse assunto, ou, se tratam, Ã© de
maneira superficial. PorÃ©m, como vocÃª pÃ´de ver, eu fiz questÃ£o de deixar todos
os links de onde removi todas as informaÃ§Ãµes que escrevi aqui. Assim, Ã©
extremamente necessÃ¡rio que vocÃª leia esses links tambÃ©m.

AlÃ©m disso, ainda faltaram algumas coisas que nÃ£o consegui falar neste post. Por
exemplo, casefold e tratamento de arquivos, foram coisas que nÃ£o mencionei aqui,
mas que sÃ£o mencionadas no
[Unicode HOWTO oficial do Python](https://docs.python.org/3/howto/unicode.html).
EntÃ£o, dÃ¡ um jeito de ler esse artigo tambÃ©m.

EntÃ£o Ã© isso, te deixo aqui com um pouco mais de serviÃ§o pela frente.

Te espero no prÃ³ximo post.
